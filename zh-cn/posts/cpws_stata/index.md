# è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—

+++
Title = ' æ ‡é¢˜å'
Slug = 'é“¾æ¥å'
date = 2024-10-29T18:52:19+08:00
Draft = true
Math= false
OutdatedInfoWarning  = false
Lightgallery = true
FeaturedImage="/img/pythonGIS.zh-cn-20250524111143536.webp"
Tags = ["æ ‡ç­¾","ass" ]
Categories= ["Movies","test"]
Summary=' æ€»ç»“'
+++

{{< music url="/music/ä¸‰å¶(ä¸»é¢˜æ›²) - RADWIMPS-MengLuoRJ.m4s" name="ä¸‰è‘‰ã®ãƒ†ãƒ¼ãƒ" artist=" å›ã®å" cover="/images/ä¸‰è‘‰ã®ãƒ†ãƒ¼ãƒ.jpg" >}} 

{{< admonition type=note  title="This is a tip" open=false >}}
ç±»å‹ï¼štip note bug abstract info success question  warning  failure danger example quote
{{< /admonition >}}


{{< youtube FO8Ngl57HRQ>}}
{{< bilibili BV1Qu4y1j7GS>}}
{{< music netease song 1833805540>}}

# è£åˆ¤æ–‡ä¹¦æ¸…æ´—æ‰‹å†Œ

ä½œè€…ï¼šæ»‘ç¿”é—ª

- è¿™ä¸ªæ–‡æ¡£æ—¢æ˜¯ç›®å‰å·¥ä½œçš„æ±‡æŠ¥ï¼Œä¹Ÿæ˜¯ä¸€äº›ç»éªŒçš„æ€»ç»“ã€‚
- æˆ–è®¸èƒ½æ–¹ä¾¿åé¢çš„åŒå­¦æ›´å¿«ä¸Šæ‰‹è£åˆ¤æ–‡ä¹¦çš„æ¸…æ´—å·¥ä½œã€‚
- å¸Œæœ›æœªæ¥çš„åŒå­¦èƒ½é‡è§†å·¥ç¨‹æŠ€å·§çš„ä¼˜åŒ–æ€»ç»“ä¸æ–‡æ¡£åˆ†äº«ã€‚

**æ€è€ƒå¦‚ä½•ä¼˜åŒ–å·¥ç¨‹ï¼Œä¼˜åŒ–å›¢é˜Ÿåä½œï¼Œè®©ç§¯ç´¯å’Œè¾“å‡ºæˆä¸ºä¸€ç§èƒ½åŠ›**ã€‚

ä¸–ç•Œä¸ä½†éœ€è¦è¾“å…¥ï¼Œä¹Ÿéœ€è¦è¾“å‡ºï¼ŒçŸ¥è¯†çš„åˆ†äº«ä¸ä¼ é€’æ‰æ˜¯äººç±»æ–‡æ˜ä¸æ–­è¿›æ­¥çš„çº½å¸¦ã€‚

å…³äº AI å·¥å…·çš„æ¨èï¼š

[é€šä¹‰åƒé—®](https://tongyi.aliyun.com/)ï¼š**ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›é•¿**ï¼Œèƒ½è®©å®ƒæ”¹å–„é•¿ä»£ç ã€‚

[GPT](https://chat.openai.com/auth/login) ï¼š**æ›´äº†è§£ stata çš„å‡½æ•°**ã€‚åœ¨æˆ‘å¤„ç†å­—ç¬¦ä¸²å˜é‡æ—¶ï¼Œé€šä¹‰åƒé—®ç»å¸¸ç”¨å¤æ‚çš„å‘½ä»¤å¤„ç†ï¼Œé€šå¸¸è¿˜æŠ¥é”™ï¼Œè€Œ GPT çŸ¥é“å¾ˆå¤š stata çš„ä¸å¸¸ç”¨å‡½æ•°ï¼Œä¸€è¡Œä»£ç å°±å¯ä»¥è½»æ¾æå®šã€‚

[deepseek](https://chat.deepseek.com/) :å›½å†…æ–°ç§€ã€‚è¯­è¨€è¡¨è¾¾å¾ˆä¸é”™ã€‚èƒ½å¾ˆå¿«ç†è§£ä½¿ç”¨è€…çš„æ„æ€ã€‚è™½ç„¶ GPT çš„ä»£ç èƒ½åŠ›æ›´åŠ ä¼˜ç§€ï¼Œä½† deepseek æ›´èƒ½å¬æ‡‚è¦æ±‚ï¼Œå»ºè®® deepseek ç”Ÿäº§åŸºç¡€ä»£ç ï¼Œgpt è¿›è¡Œä¼˜åŒ–ã€‚

## å…³äºè£åˆ¤æ–‡ä¹¦çš„ç½‘ç«™

ä¸å»ºè®®å»è£åˆ¤æ–‡ä¹¦ç½‘æŸ¥è¯¢æ¡ˆä¾‹ï¼Œååº”å¡é¡¿ï¼ŒéªŒè¯é¢‘ç¹

æ¨è[åŒ—å¤§æ³•å®](https://home.pkulaw.com/)å’Œ[å¨ç§‘å…ˆè¡Œ](å¨ç§‘å…ˆè¡Œ)ï¼ˆè‡ªå¸¦å¯è§†åŒ–å’Œä¾¿æ·æŸ¥è¯¢ï¼‰

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #7d637a; background-color: #f6edf5; border-color: #f1e4f0;">
&#x1F4AC
    <b> å¤‡æ³¨ï¼šè£åˆ¤æ–‡ä¹¦çš„æ¸…æ´—éœ€è¦äº¤æ›¿å¤„ç†ä¸­æ–‡å’Œæ•°å­—ã€‚æœ¬æ–‡æ¡£ä¼šå¤šæä¸€äº› python å’Œ stata çš„å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼ã€‚
</b></div>

AI è™½ç„¶æŒæ¡äº†å¾ˆå¤šå‡½æ•°ï¼Œä½†å®ƒä¸çŸ¥é“æ€ä¹ˆç»„åˆã€‚

è°‹ç¯‡å¸ƒå±€çš„ç®—æ³•ç†è§£æ‰æ˜¯æˆ‘ä»¬çš„æœ€å¤§ä¼˜åŠ¿ã€‚

æé—®æ—¶è¦æ›¿ AI ç†é¡ºä»£ç çš„ç®—æ³•ã€‚

æœ‰äº† AI åâ€”â€”çŸ¥é“ä»€ä¹ˆåŒ…èƒ½å®ç°ä»€ä¹ˆåŠŸèƒ½ï¼Œåœ¨ç®—æ³•ä¸‹æœ‰ä»€ä¹ˆä½œç”¨ï¼Œæ¯”æŒæ¡ç»†èŠ‚æ›´é‡è¦ã€‚

## è£åˆ¤æ–‡ä¹¦èµ„æº

**åŸå§‹æ–‡æœ¬æ•°æ®**ï¼šä¸€å † txt æ–‡ä»¶ã€‚æ¯ä¸€è¡Œæ˜¯ä¸€æ¡è£åˆ¤æ–‡ä¹¦ä¿¡æ¯ï¼ŒTxt æ–‡ä»¶ä»¥ â€œä¸‡â€ä½œä¸ºæ–‡ä»¶åï¼Œå› æ­¤ä¸‹é¢ä½¿ç”¨æ•´æ•°éƒ¨åˆ†çš„æ•°å­—ä»£è¡¨æ¯ä¸ª txt æ–‡ä»¶ã€‚

é…å¥—çš„æ–‡æ¡£æ˜¯å‹ç¼©åŒ…ã€æ–‡ä¹¦å­—å…¸ã€å˜é‡å«ä¹‰è¡¨æ ¼ã€‚
â€‹
![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113205914854.webp)

**æç¤º**ğŸ˜ï¼šæ¯ä¸ª txt æ–‡ä»¶éƒ½å¾ˆå¤§ï¼Œä¸€èˆ¬è½¯ä»¶æ‰“ä¸å¼€ã€‚ä¸ªäººæ¨èä½¿ç”¨ [EmEditor](https://www.emeditor.com/) è½¯ä»¶æŸ¥çœ‹ã€‚ç§¯æä½¿ç”¨ [EmEditor](https://www.emeditor.com/) è½¯ä»¶æŸ¥çœ‹ä¾‹å­ï¼Œèƒ½è®©æˆ‘ä»¬æ›´å¥½åœ°äº†è§£ä¹±ç æƒ…å†µå’Œè£åˆ¤æ–‡ä¹¦çš„ä¹¦å†™è§„åˆ™ï¼Œä»¥ä¾¿æ”¹è¿›æˆ‘ä»¬çš„æ–‡æœ¬å¤„ç†ä»£ç ã€‚


- **Dta æ•°æ®**ï¼šå‰è¾ˆä»¬æ¸…æ´—å¥½çš„ stata æ–‡ä»¶ã€‚æ‹†åˆ†ä¸ºäº† ans 1â€”ans 83ã€‚æ ¼å¼åˆ· stata çš„ `.dta`æ ¼å¼

![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210027701.webp)

ä¸€å¥—å®Œæ•´çš„æ¸…æ´—æµç¨‹å°±æ˜¯ï¼š

ä»åŸå§‹çš„ txt ä¸­ç­›é€‰æå–æˆ‘ä»¬éœ€è¦çš„ä¿¡æ¯ï¼Œæœ€ç»ˆæˆä¸ºèƒ½å¯¼å…¥ stata ä½¿ç”¨çš„æ•°æ®ã€‚

## txt æ–‡ä»¶çš„å¤„ç†

Txt çš„è£åˆ¤æ–‡ä¹¦å¤§éƒ¨åˆ†å·²ç»å¤„ç†ä¸ºäº† jsonæ ¼å¼ï¼Œpython è¯»å–å…¶å®è¾ƒä¸ºæ–¹ä¾¿ã€‚

å…¶ä¸­ï¼Œå¤§éƒ¨åˆ†è£åˆ¤æ–‡ä¹¦ txt æ–‡ä»¶æ˜¯ä¸‹æ–¹çš„ s ç³»åˆ—å‘½åç»“æ„ï¼š

![é‡è¦çš„ä¿¡æ¯éƒ½åœ¨ç¬¬ä¸‰åˆ—ï¼Œs*ç³»åˆ—åŒ…å«çš„æ•°æ®ç»“æ„å½“ä¸­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210107895.webp)

é‡è¦çš„ä¿¡æ¯éƒ½åœ¨ç¬¬ä¸‰åˆ—ï¼Œs*ç³»åˆ—åŒ…å«çš„æ•°æ®ç»“æ„å½“ä¸­

ä½†è·‘é€šäº†ä¸€åˆ°ä¸‰ä»½ txt æ–‡ä»¶ä¸ç­‰äºèƒ½è·‘é€šæ‰€æœ‰ã€‚

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #a94442; background-color: #f2dede; border-color: #ebccd1;">
&#x26D4<b> Txt çš„è£åˆ¤æ–‡ä¹¦è¿˜å­˜åœ¨ä¸€äº›å…¶ä»–æ ¼å¼ï¼š

- æœ‰äº› txt æ–‡ä»¶æ ¼å¼ä¸º DocInfoVo å¼€å¤´çš„ã€‚
- æœ‰äº› txt æ–‡ä»¶æ ¼å¼ä¸º title å¼€å¤´çš„ã€‚
- æœ‰äº› txt æ–‡ä»¶æ ¼å¼å¤¹æ‚ç€ HTML è¯­è¨€ã€‚
- æœ‰äº› txt æ–‡ä»¶æ ¼å¼æ˜¯çº¯ç²¹çš„ä¹±ç ã€‚
- æœ‰äº›æ ¼å¼ä¸­ s ç³»åˆ—å’Œ DocInfoVo æ ¼å¼æ··åœ¨ä¸€èµ·ï¼Œå¯¼è‡´å¾ªç¯ä»£ç è¢«æ‰“æ–­ã€‚
</b></div>

å› æ­¤å¤„ç†æ—¶å¾—è€ƒè™‘**ä»£ç çš„å…¼å®¹æ€§å’Œå…¨é¢æ€§**ï¼Œåˆ†å¼€å†™å¤„ç†ä»£ç ï¼Œç„¶åå¯¹è¿™ç§ txt æ–‡ä»¶å¤„ç†ä¸¤æ¬¡ã€‚

![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210203690.webp)

![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210207504.webp)
### ä¸€äº›å»ºè®®

ç”±äºå¾ªç¯å®¹æ˜“è¢«æ‰“æ–­ï¼Œç”¨ txt è¿›è¡Œå¤„ç†å¤„ç†æ—¶ï¼Œå»ºè®®å‚è€ƒå¦‚ä¸‹é€»è¾‘ï¼š

1ã€å°†ä¹±ç çš„ txtï¼ˆé™„åœ¨åæ–‡ï¼‰å’Œ s ç³»åˆ—çš„ txt åˆ†å¼€å¤„ç†

2ã€åŸå§‹æ–‡ä»¶ä¸º txtï¼Œæ¸…æ´—å¤„ç†åç”Ÿæˆæ–°çš„ cleaned_txtã€‚

3ã€ç”±äºæ¸…æ´—å¾ªç¯å‘½ä»¤å®¹æ˜“è¢«æ‰“æ–­ï¼Œæœ€å¥½å¤šå†™ä¸€ä¸ªå‘½ä»¤ï¼šå¯¹æ¯” txt å’Œ cleaned_txt çš„æ–‡ä»¶åã€‚

4ã€å¤šå†™ä¸€ä¸ªæ­¥éª¤ï¼šå°†å¤„ç†å®Œæˆçš„ txt æ–‡æ¡£ç§»åŠ¨åˆ°å¦ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œä¿®æ”¹æŠ¥é”™ä»£ç åç»§ç»­å¾ªç¯ã€‚

> Python é»˜è®¤è¯»å–æ–‡ä»¶çš„é¡ºåºæ˜¯ txt 1 txt 10 text 111... è€Œä¸æ˜¯ 1ã€2ã€3...

### ä¹±ç æ–‡æ¡£

ä¸åªæ˜¯ s ç³»åˆ—æ ¼å¼çš„ txt æ–‡ä»¶æœ‰ï¼š

![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210248250.webp)
## Python ä¾‹å­ ï¼šæå–æ¡ˆä»¶å·åºåˆ—

### æ€è·¯

python è¯»å– txt çš„ json æ ¼å¼ã€‚

æå–ç¬¬ä¸‰åˆ— `CourtInfo` æ•°æ®ã€‚

![ä¾‹å­](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210332387.webp)

s 23 å˜é‡æ‰€æœ‰æ¡ˆä»¶å·ï¼Œä»¥æ­¤è¿½è¸ª**ä¸€å®¡äºŒå®¡å†å®¡ç»ˆå®¡**ã€‚

ä¾‹å¦‚ `ï¼ˆ2016ï¼‰æ²ª0104æ°‘åˆ33043å·` ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªè£åˆ¤æ–‡ä¹¦æ¡ˆä»¶çš„èº«ä»½è¯å·ã€‚

æ­£åˆ™è¡¨è¾¾å¼ç­›é€‰ï¼šæŒ‰ç…§ `ï¼ˆå¹´ä»½ï¼‰åˆ\ç»ˆ\å† å·` çš„è§„åˆ™æå–æ»¡è¶³è¦æ±‚çš„å­—ç¬¦ä¸²

```python
Â pattern = r"[ï¼ˆ(]\d{4}[)ï¼‰][^å·]*?(?:åˆ|ç»ˆ|å†)[^å·]*?å·"
```

æå–åå»é™¤é‡ã€æ’é™¤åŒ…å«â€œ`['X', 'x', 'ï¼¸', 'æ‰§', 'ä¸åŠ¨äº§', '-', 'ç¬¬']`â€çš„æ¡ˆä»¶å·ã€‚

> å¦‚æœç»§ç»­ä½¿ç”¨è¿™ç§æ–¹æ³•æå–ï¼Œæ’é™¤æ—¶æœ€å¥½åŠ å…¥ **è¾–**ã€‚
> 
> è¿™ç§æ¡ˆå·ä¹Ÿæœ‰è¾–åˆã€è¾–å†ã€è¾–ç»ˆï¼Œä½†åªè¡¨ç¤ºæ³•é™¢åœ¨ç®¡ç†æƒä¸Šçš„åˆ’åˆ†ã€‚
> 
> ï¼ˆæˆ–è®¸ä»¥åè¿˜æœ‰äººèƒ½æƒ³åˆ°ç”¨è¿™ä¸ªç‰¹å¾åšç ”ç©¶ï¼ŸğŸ‘ï¼‰

```python
Â exclusions = ['X', 'x', 'ï¼¸', 'æ‰§', 'ä¸åŠ¨äº§', '-', 'ç¬¬']
```

æ’åºï¼Œä¼˜å…ˆæŒ‰ç…§æ¡ˆå·ä¸­çš„ (å¹´ä»½) è¿›è¡Œæ’åºã€‚ åˆ é™¤æ¡ˆå·æ•°é‡å¤§äºç­‰äº 5 çš„æ‰€æœ‰è¡Œã€‚

ä¸€ä¸ªæ¡ˆå­æœ€å¤šæ˜¯å¦‚ä¸‹çš„é¡ºåºï¼š

åˆå®¡ï¼Œä¸æœç”³è¯·ä¸Šè¯‰ï¼Œå†å®¡ï¼Œç»ˆå®¡ï¼Œä¸æœå†è¿›è¡Œä¸Šè¯‰ï¼Œç»ˆå®¡ã€‚

å¯ä»¥ç›´æ¥ä»åˆå®¡è·³åˆ°ç»ˆå®¡ï¼Œç»ˆå®¡åè¿˜æœ‰æœ€åä¸€æ¬¡å†å®¡æœºä¼šï¼Œå†å®¡ç»“æŸåä¸€èˆ¬å°±ä¸ä¼šæ›´æ”¹äº†ã€‚

> ä¸ªäººçš„æ’åºé€»è¾‘ï¼šï¼ˆæ­¤å¤„æœ€å¤§çš„æ¼æ´æ˜¯æˆ‘æƒ³ä¸åˆ°æ€ä¹ˆå®Œç¾è¯†åˆ«â€œå†â€æ¡ˆå·çš„ä½ç½®ï¼‰
> 
> ç”Ÿæˆ m 1â€”â€”m 4 å››ä¸ªå˜é‡ å¸¦æœ‰åˆçš„æ¡ˆå·å½’ç±»åˆ° m 1ï¼Œå¸¦æœ‰ç»ˆçš„æ¡ˆå·å½’ç±»åˆ° m 3ã€‚
> 
> å¸¦æœ‰å†çš„æƒ…å†µï¼š
> 
> å¦‚æœåªæœ‰åˆå’Œå†ï¼Œå†å½’ç±»åˆ° m 2
> 
> å¦‚æœåªå†ï¼Œæˆ‘é»˜è®¤æ”¾åœ¨ m 4
> 
> å¦‚æœåŒæ—¶æœ‰ç»ˆã€å†ï¼Œæ ¹æ®ç›¸å¯¹ä½ç½®åˆ¤æ–­å†æ˜¯ m 2 è¿˜æ˜¯ m 4ã€‚
> 
> åé¢å‘ç°ï¼šã€è¾–ç»ˆã€‘ä¼šå¹²æ‰°é¡ºåºï¼Œæ‰€ä»¥å…ˆå°†åŒ…å«è¾–çš„æ¡ˆå·æ¸…é™¤ï¼Œç„¶åå’Œè¢«å…¶æŒ¤åˆ° m 4 çš„ã€ç»ˆã€‘æ¡ˆå·äº¤æ¢é¡ºåºã€‚
> 
> è¿™å°±æ˜¯å¤§æ•°æ®æœ€éº»çƒ¦çš„ç‚¹ï¼š
> 
> **å¦‚æœå‰é¢å·¥ä½œæ²¡æœ‰å®Œç¾åšå¥½ï¼Œåé¢è¿”å·¥æˆæœ¬ä¼šè¶Šæ¥è¶Šé«˜ã€‚ æœ€ç³Ÿç³•çš„æ˜¯â€”â€”å¤§æ•°æ®æ€»æ˜¯å¾ˆéš¾ç©·å°½æ‰€æœ‰ä¹±ç å¯èƒ½ã€‚**

### å•ä¸ª txt çš„å¤„ç†

ä»¥ä¸‹ä»£ç åªé’ˆå¯¹ **s ç³»åˆ—**å‘½åçš„æ–‡æ¡£ã€‚

```python
Â import pandas as pd  
Â from collections import OrderedDict  
Â import json  
Â import re  
Â import os  
Â â€‹  
Â # è¯»å–æ•°æ®  
Â data = pd.read_csv("F:/æ¡Œé¢/è£åˆ¤æ–‡ä¹¦æ•°æ®åº“/test/test.txt", delimiter='\t')  
Â â€‹  
Â # æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®  
Â print(data.head(3))  
Â â€‹  
Â # å‡è®¾ CourtInfo åˆ—ä¸­åŒ…å« JSON å­—ç¬¦ä¸²  
Â json_data = data['CourtInfo']  
Â â€‹  
Â # è§£æ JSON æ•°æ®  
Â def parse_json(json_str):  
Â  Â  Â try:  
Â  Â  Â  Â  Â return json.loads(json_str)  
Â  Â  Â except json.JSONDecodeError:  
Â  Â  Â  Â  Â return None  
Â â€‹  
Â # åº”ç”¨è§£æå‡½æ•°  
Â parsed_prac_c = json_data.apply(parse_json)  
Â â€‹  
Â â€‹  
Â â€‹  
Â # å°† JSON å­—å…¸è½¬æ¢ä¸º DataFrameï¼Œå¹¶å¤„ç†ç¼ºå¤±å€¼  
Â json_df = pd.json_normalize(parsed_prac_c.dropna(how='all'))  
Â â€‹  
Â # æŸ¥çœ‹è½¬æ¢åçš„ DataFrame  
Â print(json_df.head(20))  
Â â€‹  
Â # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼  
Â pattern = r"[ï¼ˆ(]\d{4}[ï¼‰)]\s*.*?å·"  
Â â€‹  
Â # ä½¿ç”¨findallæå–æ‰€æœ‰åŒ¹é…é¡¹  
Â matches = json_df['s23'].apply(lambda x: re.findall(pattern, x) if isinstance(x, str) else [])  
Â â€‹  
Â # æŸ¥çœ‹è½¬æ¢åçš„ DataFrame  
Â print(matches.head(3))  
Â print(matches)  
Â â€‹  
Â â€‹  
Â # åˆ›å»ºæ–°çš„ DataFrameï¼Œå°†æ›´æ–°åçš„ 's23' åˆ—ä¿å­˜  
Â matches_df = pd.DataFrame({  
Â  Â  Â 's23': matches  
Â })  
Â â€‹  
Â # æŸ¥çœ‹è½¬æ¢åçš„ DataFrame  
Â print(matches_df.head(10))  
Â # print(matches_df)  
Â â€‹  
Â # é€‰æ‹©è¦åˆå¹¶çš„åˆ—  
Â selected_data_columns = []   
Â selected_json_df_columns = []  
Â â€‹  
Â # åˆå¹¶é€‰æ‹©çš„åˆ—  
Â combined_df = pd.concat([data[selected_data_columns], json_df[selected_json_df_columns],matches_df], axis=1)  
Â â€‹  
Â def clean_text(text):  
Â  Â  Â if isinstance(text, list):  
Â  Â  Â  Â  Â text = ', '.join(text) Â # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…ƒç´ ä¹‹é—´ç”¨é€—å·å’Œç©ºæ ¼åˆ†éš”  
Â  Â  Â if isinstance(text, str):  
Â  Â  Â  Â  Â if text == "[]":  
Â  Â  Â  Â  Â  Â  Â return ''  
Â  Â  Â  Â  Â cleaned_text = text.replace('[', '').replace(']', '').strip()  
Â  Â  Â  Â  Â return cleaned_text  
Â  Â  Â return ''  
Â â€‹  
Â # åˆ‡å‰²å¹¶ç”Ÿæˆåˆ—å  
Â def split_and_rename(content):  
Â  Â  Â content = clean_text(content)  
Â  Â  Â split_content = content.split(', ')  
Â  Â  Â result = {f'm{i+1}': split_content[i].strip() for i in range(len(split_content)) if split_content[i].strip()}  
Â  Â  Â return result  
Â â€‹  
Â # åº”ç”¨å‡½æ•°  
Â expanded_df = combined_df['s23'].apply(split_and_rename).apply(pd.Series)  
Â â€‹  
Â # æŸ¥çœ‹ç»“æœ  
Â # print("Expanded DataFrame:")  
Â â€‹  
Â # æå–å¹´ä»½çš„å‡½æ•°  
Â def extract_year(text):  
Â  Â  Â match = re.search(r'[ï¼ˆ(](\d{4})[ï¼‰)]', text)  
Â  Â  Â return int(match.group(1)) if match else float('inf')  
Â â€‹  
Â # æ¸…æ´—å’Œæ’åºå‡½æ•°  
Â def clean_and_sort(row):  
Â  Â  Â # è¿‡æ»¤æ‰éå­—ç¬¦ä¸²ç±»å‹çš„å€¼  
Â  Â  Â filtered = [item for item in row if isinstance(item, str) and not any(keyword in item for keyword in exclusions)]  
Â  Â  Â # å»é™¤é‡å¤é¡¹å¹¶ä¿æŒé¡ºåº  
Â  Â  Â unique_items = list(OrderedDict.fromkeys(filtered))  
Â  Â  Â # æŒ‰å¹´ä»½æ’åº  
Â  Â  Â sorted_items = sorted(unique_items, key=lambda x: extract_year(x))  
Â  Â  Â return sorted_items  
Â â€‹  
Â # éœ€è¦æ’é™¤çš„å…³é”®å­—åˆ—è¡¨  
Â exclusions = ['X', 'x', 'ï¼¸', 'æ‰§', 'ä¸åŠ¨äº§', '-', 'ç¬¬']  
Â â€‹  
Â # ç­›é€‰æ‰€æœ‰ m* åˆ—  
Â m_columns = [col for col in expanded_df.columns if col.startswith('m')]  
Â â€‹  
Â # å¯¹æ¯ä¸€è¡Œåº”ç”¨æ¸…æ´—å’Œæ’åºå‡½æ•°  
Â def process_row(row):  
Â  Â  Â # è·å– m* åˆ—çš„å†…å®¹  
Â  Â  Â row_values = [row[col] for col in m_columns]  
Â  Â  Â # æ‰å¹³åŒ–åˆ—è¡¨  
Â  Â  Â flattened = [item for sublist in row_values for item in (sublist if isinstance(sublist, list) else [sublist])]  
Â  Â  Â # å¤„ç†å¹¶æ’åº  
Â  Â  Â sorted_items = clean_and_sort(flattened)  
Â  Â  Â return sorted_items  
Â â€‹  
Â # åº”ç”¨åˆ°æ•°æ®æ¡†çš„æ¯ä¸€è¡Œ  
Â sorted_results = expanded_df.apply(process_row, axis=1)  
Â â€‹  
Â # åˆ›å»ºæ–°çš„æ•°æ®æ¡†  
Â max_length = max(sorted_results.apply(len))  
Â sorted_df = pd.DataFrame(sorted_results.tolist(), columns=[f'm_sorted_{i+1}' for i in range(max_length)])  
Â â€‹  
Â # æ‰“å°ç»“æœ  
Â print(sorted_df.head(10))  
Â â€‹  
Â # åˆå¹¶åˆ°åŸæ•°æ®  
Â combined_df2 = pd.concat([combined_df, sorted_df], axis=1)  
Â â€‹  
Â # æŸ¥çœ‹ç»“æœ  
Â print(combined_df.head(10))  
Â # æ£€æŸ¥ 'm_sorted_5' åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¹¶åˆ é™¤éç©ºè¡Œ  
Â if 'm_sorted_5' in combined_df2.columns:  
Â  Â  Â combined_df2_cleaned = combined_df2[combined_df2['m_sorted_5'].isna() | (combined_df2['m_sorted_5'] == '')]  
Â else:  
Â  Â  Â # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œåˆ™ä¿ç•™åŸæ•°æ®æ¡†  
Â  Â  Â combined_df2_cleaned = combined_df2.copy()  
Â â€‹  
Â # æ‰“å°ç»“æœ  
Â print(combined_df2_cleaned.head(10))  
Â â€‹  
Â # è·å–æ‰€æœ‰ m_sorted_* åˆ—çš„åç§°  
Â m_sorted_columns = [col for col in combined_df2_cleaned.columns if col.startswith('m_sorted_')]  
Â â€‹  
Â # è¿‡æ»¤å‡ºç´¢å¼•å¤§äºç­‰äº 5 çš„åˆ—  
Â columns_to_drop = [col for col in m_sorted_columns if int(col.split('_')[2]) >= 5]  
Â â€‹  
Â # åˆ é™¤æŒ‡å®šçš„åˆ—  
Â combined_df2_cleaned = combined_df2_cleaned.drop(columns=columns_to_drop, errors='ignore')  
Â â€‹  
Â # æ‰“å°ç»“æœä»¥æ£€æŸ¥æ¸…ç†åçš„æ•°æ®æ¡†  
Â print(combined_df2_cleaned.head())  
Â â€‹  
Â combined_df2.to_csv('F:/æ¡Œé¢/combined_df.txt', sep='\t', index=False, quoting=1, quotechar='"')
```

### æ‰¹é‡å¤„ç†

ä¸ªäººé€‰æ‹©çš„æ‰¹é‡å¤„ç†æ€è·¯æ˜¯å¾ªç¯å¤„ç†ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢çš„æ‰€æœ‰æ–‡æ¡£ã€‚

å¾ªç¯å®¹æ˜“è¢«æ‰“æ–­ï¼Œå»ºè®®å†™ä¸€ä¸ªæŠ¥é”™è®°å½•ï¼Œå’Œå¤„ç†å®Œä¸€ä¸ªæ–‡æ¡£ä¾¿ç§»åŠ¨çš„å‘½ä»¤ã€‚

```python
Â import pandas as pd  
Â from collections import OrderedDict  
Â import json  
Â import re  
Â import os  
Â â€‹  
Â # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„  
Â # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„  
Â input_folder_path = "C:/Users/PC/Desktop/hzp_project/äºŒå®¡å’Œä¸€å®¡/æ•°æ®/"  
Â output_folder_path = "C:/Users/PC/Desktop/hzp_project/äºŒå®¡å’Œä¸€å®¡/æ¸…æ´—ç»“æœ/"  
Â â€‹  
Â # å¦‚æœè¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ  
Â if not os.path.exists(output_folder_path):  
Â  Â  Â os.makedirs(output_folder_path)  
Â â€‹  
Â # éå†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ .txt æ–‡ä»¶  
Â for filename in os.listdir(input_folder_path):  
Â  Â  Â if filename.endswith('.txt'):  
Â  Â  Â  Â  Â input_file_path = os.path.join(input_folder_path, filename)  
Â  Â  Â  Â  Â output_file_path = os.path.join(output_folder_path, filename)  
Â â€‹  
Â  Â  Â  Â  Â # è¯»å–æ•°æ®  
Â  Â  Â  Â  Â data = pd.read_csv(input_file_path, delimiter='\t')  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°å‰3è¡Œæ•°æ®ä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(f"Processing file: {filename}")  
Â  Â  Â  Â  Â print(data.head(3))  
Â â€‹  
Â  Â  Â  Â  Â # è§£æJSONå­—æ®µ  
Â  Â  Â  Â  Â json_data = data['CourtInfo']  
Â â€‹  
Â  Â  Â  Â  Â def parse_json(json_str):  
Â  Â  Â  Â  Â  Â  Â try:  
Â  Â  Â  Â  Â  Â  Â  Â  Â return json.loads(json_str)  
Â  Â  Â  Â  Â  Â  Â except json.JSONDecodeError:  
Â  Â  Â  Â  Â  Â  Â  Â  Â return None  
Â â€‹  
Â  Â  Â  Â  Â parsed_prac_c = json_data.apply(parse_json)  
Â â€‹  
Â  Â  Â  Â  Â # å°†è§£æåçš„JSONè½¬æ¢ä¸ºDataFrame  
Â  Â  Â  Â  Â json_df = pd.json_normalize(parsed_prac_c.dropna(how='all'))  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°å‰20è¡Œä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(json_df.head(20))  
Â â€‹  
Â  Â  Â  Â  Â # å®šä¹‰åŒ¹é…æ¨¡å¼  
Â  Â  Â  Â  Â pattern = r"[ï¼ˆ(]\d{4}[ï¼‰)]\s*.*?å·"  
Â â€‹  
Â  Â  Â  Â  Â # æŸ¥æ‰¾åŒ¹é…é¡¹  
Â  Â  Â  Â  Â matches = json_df['s23'].apply(lambda x: re.findall(pattern, x) if isinstance(x, str) else [])  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°å‰3è¡ŒåŒ¹é…ç»“æœä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(matches.head(3))  
Â  Â  Â  Â  Â print(matches)  
Â â€‹  
Â  Â  Â  Â  Â # å°†åŒ¹é…ç»“æœè½¬æ¢ä¸ºDataFrame  
Â  Â  Â  Â  Â matches_df = pd.DataFrame({'s23': matches})  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°å‰10è¡ŒåŒ¹é…ç»“æœDataFrameä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(matches_df.head(10))  
Â â€‹  
Â  Â  Â  Â  Â # é€‰æ‹©è¦ä¿ç•™çš„æ•°æ®åˆ—  
Â  Â  Â  Â  Â selected_data_columns = []   
Â  Â  Â  Â  Â selected_json_df_columns = []  
Â â€‹  
Â  Â  Â  Â  Â # åˆå¹¶åŸå§‹æ•°æ®ã€è§£æåçš„JSONæ•°æ®ä»¥åŠåŒ¹é…ç»“æœ  
Â  Â  Â  Â  Â combined_df = pd.concat([data[selected_data_columns], json_df[selected_json_df_columns], matches_df], axis=1)  
Â â€‹  
Â  Â  Â  Â  Â # æ¸…æ´—æ–‡æœ¬å‡½æ•°  
Â  Â  Â  Â  Â def clean_text(text):  
Â  Â  Â  Â  Â  Â  Â if isinstance(text, list):  
Â  Â  Â  Â  Â  Â  Â  Â  Â text = ', '.join(text)   
Â  Â  Â  Â  Â  Â  Â if isinstance(text, str):  
Â  Â  Â  Â  Â  Â  Â  Â  Â if text == "[]":  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â return ''  
Â  Â  Â  Â  Â  Â  Â  Â  Â cleaned_text = text.replace('[', '').replace(']', '').strip()  
Â  Â  Â  Â  Â  Â  Â  Â  Â return cleaned_text  
Â  Â  Â  Â  Â  Â  Â return ''  
Â â€‹  
Â  Â  Â  Â  Â # åˆ†å‰²å¹¶é‡å‘½åå‡½æ•°  
Â  Â  Â  Â  Â def split_and_rename(content):  
Â  Â  Â  Â  Â  Â  Â content = clean_text(content)  
Â  Â  Â  Â  Â  Â  Â split_content = content.split(', ')  
Â  Â  Â  Â  Â  Â  Â result = {f'm{i+1}': split_content[i].strip() for i in range(len(split_content)) if split_content[i].strip()}  
Â  Â  Â  Â  Â  Â  Â return result  
Â â€‹  
Â  Â  Â  Â  Â # å±•å¼€åŒ¹é…ç»“æœ  
Â  Â  Â  Â  Â expanded_df = combined_df['s23'].apply(split_and_rename).apply(pd.Series)  
Â â€‹  
Â  Â  Â  Â  Â # æå–å¹´ä»½å‡½æ•°  
Â  Â  Â  Â  Â def extract_year(text):  
Â  Â  Â  Â  Â  Â  Â match = re.search(r'[ï¼ˆ(](\d{4})[ï¼‰)]', text)  
Â  Â  Â  Â  Â  Â  Â return int(match.group(1)) if match else float('inf')  
Â â€‹  
Â  Â  Â  Â  Â # æ¸…æ´—å¹¶æ’åºå‡½æ•°  
Â  Â  Â  Â  Â def clean_and_sort(row):  
Â  Â  Â  Â  Â  Â  Â filtered = [item for item in row if isinstance(item, str) and not any(keyword in item for keyword in exclusions)]  
Â  Â  Â  Â  Â  Â  Â unique_items = list(OrderedDict.fromkeys(filtered))  
Â  Â  Â  Â  Â  Â  Â sorted_items = sorted(unique_items, key=lambda x: extract_year(x))  
Â  Â  Â  Â  Â  Â  Â return sorted_items  
Â â€‹  
Â  Â  Â  Â  Â # æ’é™¤å…³é”®è¯åˆ—è¡¨  
Â  Â  Â  Â  Â exclusions = ['X', 'x', 'ï¼¸', 'æ‰§', 'ä¸åŠ¨äº§', '-', 'ç¬¬']  
Â â€‹  
Â  Â  Â  Â  Â # è·å–æ‰€æœ‰må¼€å¤´çš„åˆ—å  
Â  Â  Â  Â  Â m_columns = [col for col in expanded_df.columns if col.startswith('m')]  
Â â€‹  
Â  Â  Â  Â  Â # å¤„ç†æ¯ä¸€è¡Œ  
Â  Â  Â  Â  Â def process_row(row):  
Â  Â  Â  Â  Â  Â  Â row_values = [row[col] for col in m_columns]  
Â  Â  Â  Â  Â  Â  Â flattened = [item for sublist in row_values for item in (sublist if isinstance(sublist, list) else [sublist])]  
Â  Â  Â  Â  Â  Â  Â sorted_items = clean_and_sort(flattened)  
Â  Â  Â  Â  Â  Â  Â return sorted_items  
Â â€‹  
Â  Â  Â  Â  Â sorted_results = expanded_df.apply(process_row, axis=1)  
Â â€‹  
Â  Â  Â  Â  Â # è®¡ç®—æœ€å¤§é•¿åº¦ï¼Œå¹¶åˆ›å»ºæ–°çš„æ’åºåçš„DataFrame  
Â  Â  Â  Â  Â max_length = max(sorted_results.apply(len))  
Â  Â  Â  Â  Â sorted_df = pd.DataFrame(sorted_results.tolist(), columns=[f'm_sorted_{i+1}' for i in range(max_length)])  
Â â€‹  
Â  Â  Â  Â  Â # å†æ¬¡åˆå¹¶æ•°æ®  
Â  Â  Â  Â  Â combined_df2 = pd.concat([combined_df, sorted_df], axis=1)  
Â â€‹  
Â  Â  Â  Â  Â # è¿‡æ»¤æ¡ä»¶  
Â  Â  Â  Â  Â if 'm_sorted_5' in combined_df2.columns:  
Â  Â  Â  Â  Â  Â  Â combined_df2_cleaned = combined_df2[combined_df2['m_sorted_5'].isna() | (combined_df2['m_sorted_5'] == '')]  
Â  Â  Â  Â  Â else:  
Â  Â  Â  Â  Â  Â  Â combined_df2_cleaned = combined_df2.copy()  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°æ¸…ç†åçš„å‰10è¡Œæ•°æ®ä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(combined_df2_cleaned.head(10))  
Â â€‹  
Â  Â  Â  Â  Â # è·å–æ‰€æœ‰m_sortedå¼€å¤´çš„åˆ—å  
Â  Â  Â  Â  Â m_sorted_columns = [col for col in combined_df2_cleaned.columns if col.startswith('m_sorted_')]  
Â â€‹  
Â  Â  Â  Â  Â # è¦åˆ é™¤çš„åˆ—å  
Â  Â  Â  Â  Â columns_to_drop = [col for col in m_sorted_columns if int(col.split('_')[2]) >= 5]  
Â â€‹  
Â  Â  Â  Â  Â # åˆ é™¤ä¸éœ€è¦çš„åˆ—  
Â  Â  Â  Â  Â combined_df2_cleaned = combined_df2_cleaned.drop(columns=columns_to_drop, errors='ignore')  
Â â€‹  
Â  Â  Â  Â  Â # æ‰“å°æœ€ç»ˆå‰å‡ è¡Œæ•°æ®ä»¥æ£€æŸ¥  
Â  Â  Â  Â  Â print(combined_df2_cleaned.head())  
Â â€‹  
Â  Â  Â  Â  Â # ä¿å­˜åˆ°æ–‡ä»¶  
Â  Â  Â  Â  Â combined_df2_cleaned.to_csv(output_file_path, sep='\t', index=False, quoting=1, quotechar='"')  
Â â€‹  
Â print("Processing complete.")
```

## Python ä¾‹å­ ï¼šæå–æ³•æ¡å’Œç§»åŠ¨æ–‡ä»¶å¤¹

æ³•æ¡æ•°é‡å¯ä»¥è¡¡é‡æ¡ˆä»¶å¤æ‚åº¦ï¼Œæ³•æ¡æœ¬èº«ä¹Ÿè¡¡é‡äº†æ¡ˆä»¶ç±»å‹ã€‚

Txt æ–‡æ¡£çš„ s 47 ç³»åˆ—å¯¹åº”ç€æ–‡ä¹¦çš„æ³•æ¡ï¼Œé‡Œé¢æ˜¯ç¬¬äºŒå±‚ json æ ¼å¼ï¼š

```python
Â for clause in data:  
Â  Â  Â fgmc = clause.get('fgmc', '')  
Â  Â  Â tkx = clause.get('tkx', '')  
Â  Â  Â match = re.search(r'(ç¬¬.*?æ¡)', tkx)
```

`['s23', 's25', 's26', 's27']` çš„ä¸­æ–‡å­—ç¬¦åŠ æ€»å°±æ˜¯è£åˆ¤æ–‡ä¹¦çš„æ­£æ–‡å­—æ•°ã€‚

> "s7": "æ¡ˆä»¶å·","s23": "è¯‰è®¼è®°å½•","s24": "è¯‰æ§è¾©","s25": "äº‹å®","s26": "ç†ç”±","s27": "åˆ¤å†³ç»“æœ"ã€‚

### å¤šä¸ª txt

```python
Â import pandas as pd  
Â import json  
Â import re  
Â import os  
Â import glob  
Â import logging  
Â â€‹  
Â # é…ç½®æ—¥å¿—è®°å½•  
Â logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')  
Â â€‹  
Â # æ˜ç¡®æŒ‡å®šè¾“å…¥æ–‡ä»¶å¤¹å’Œè¾“å‡ºæ–‡ä»¶å¤¹çš„è·¯å¾„  
Â input_folder_path = "C:/Users/PC/Desktop/hzp_project/äºŒå®¡å’Œä¸€å®¡/æ•°æ®/"  
Â output_folder_path = "C:/Users/PC/Desktop/hzp_project/äºŒå®¡å’Œä¸€å®¡/æ¸…æ´—ç»“æœ/"  
Â log_folder_path = "C:/Users/PC/Desktop/hzp_project/äºŒå®¡å’Œä¸€å®¡/æ—¥å¿—/"  
Â â€‹  
Â # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å’Œæ—¥å¿—æ–‡ä»¶å¤¹å­˜åœ¨  
Â os.makedirs(output_folder_path, exist_ok=True)  
Â os.makedirs(log_folder_path, exist_ok=True)  
Â â€‹  
Â # è§£æ JSON æ•°æ®  
Â def parse_json(json_str):  
Â  Â  Â try:  
Â  Â  Â  Â  Â if isinstance(json_str, str):  
Â  Â  Â  Â  Â  Â  Â return json.loads(json_str)  
Â  Â  Â  Â  Â elif isinstance(json_str, list):  
Â  Â  Â  Â  Â  Â  Â return json_str  
Â  Â  Â  Â  Â else:  
Â  Â  Â  Â  Â  Â  Â logging.warning(f"æœªçŸ¥æ•°æ®ç±»å‹: {type(json_str)}")  
Â  Â  Â  Â  Â  Â  Â return None  
Â  Â  Â except json.JSONDecodeError:  
Â  Â  Â  Â  Â logging.warning(f"JSON è§£æå¤±è´¥: {json_str}")  
Â  Â  Â  Â  Â return None  
Â â€‹  
Â # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°  
Â def count_chinese_chars(text):  
Â  Â  Â if pd.isnull(text):  
Â  Â  Â  Â  Â return 0  
Â  Â  Â return len(re.findall(r'[\u4e00-\u9fff\u0030-\u0039\u3000-\u303f\uff00-\uffef]', str(text)))  
Â â€‹  
Â # æå–å’Œæ ¼å¼åŒ–æ¡æ¬¾  
Â def extract_clauses(s47_data):  
Â  Â  Â formatted_clauses = []  
Â  Â  Â for item in s47_data.dropna():  
Â  Â  Â  Â  Â data = parse_json(item)  
Â  Â  Â  Â  Â if data is not None:  
Â  Â  Â  Â  Â  Â  Â clauses = set() Â # ä½¿ç”¨é›†åˆå»é‡  
Â  Â  Â  Â  Â  Â  Â for clause in data:  
Â  Â  Â  Â  Â  Â  Â  Â  Â fgmc = clause.get('fgmc', '')  
Â  Â  Â  Â  Â  Â  Â  Â  Â tkx = clause.get('tkx', '')  
Â  Â  Â  Â  Â  Â  Â  Â  Â match = re.search(r'(ç¬¬.*?æ¡)', tkx)  
Â  Â  Â  Â  Â  Â  Â  Â  Â if match:  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â clauses.add(f"{fgmc}{match.group(1)}")  
Â  Â  Â  Â  Â  Â  Â formatted_clauses.append(list(clauses)) Â # ä¿å­˜æ¯è¡Œæå–çš„æ¡æ¬¾  
Â  Â  Â  Â  Â else:  
Â  Â  Â  Â  Â  Â  Â formatted_clauses.append([]) Â # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™æ·»åŠ ç©ºåˆ—è¡¨  
Â  Â  Â return formatted_clauses  
Â â€‹  
Â # å¤„ç†å•ä¸ªæ–‡ä»¶  
Â def process_file(input_file, output_folder, log_folder, total_rows):  
Â  Â  Â # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„åˆ—è¡¨æ¥å­˜å‚¨é”™è¯¯ä¿¡æ¯  
Â  Â  Â error_logs = []  
Â  Â  Â   
Â  Â  Â def bad_line_handler(bad_line):  
Â  Â  Â  Â  Â error_logs.append((input_file, bad_line.line_num, bad_line.line))  
Â  Â  Â   
Â  Â  Â # è¯»å–æ•°æ®  
Â  Â  Â try:  
Â  Â  Â  Â  Â data = pd.read_csv(input_file, delimiter='\t', on_bad_lines=bad_line_handler, engine='python')  
Â  Â  Â  Â  Â logging.info(f"Read {len(data)} rows from file: {input_file}")  
Â  Â  Â except Exception as e:  
Â  Â  Â  Â  Â logging.error(f"Error reading file: {input_file}")  
Â  Â  Â  Â  Â logging.error(f"Error message: {e}")  
Â  Â  Â  Â  Â return  
Â  Â  Â   
Â  Â  Â # è¿‡æ»¤æ‰ CourtInfo åˆ—ä»¥ "DocInfoVo" å¼€å¤´æˆ–æ— æ³•è§£æä¸º JSON çš„è¡Œ  
Â  Â  Â data = data[data['CourtInfo'].apply(lambda x: x.startswith('{"s1":') if pd.notnull(x) else False)]  
Â  Â  Â data = data[data['CourtInfo'].apply(lambda x: isinstance(parse_json(x), dict))]  
Â â€‹  
Â  Â  Â logging.info(f"Filtered out invalid rows from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # æå– JSON æ•°æ®  
Â  Â  Â json_data = data['CourtInfo']  
Â  Â  Â   
Â  Â  Â # åº”ç”¨è§£æå‡½æ•°  
Â  Â  Â parsed_prac_c = json_data.apply(parse_json)  
Â  Â  Â logging.info(f"Parsed JSON data from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # å°† JSON å­—å…¸è½¬æ¢ä¸º DataFrameï¼Œå¹¶å¤„ç†ç¼ºå¤±å€¼  
Â  Â  Â json_df = pd.json_normalize(parsed_prac_c.dropna(how='all'))  
Â  Â  Â logging.info(f"Normalized JSON data from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # é€‰å–ç‰¹å®šåˆ—  
Â  Â  Â selected_columns = json_df[['s7', 's23', 's25', 's26', 's27', 's47']].copy() Â # ä½¿ç”¨ .copy() åˆ›å»ºå‰¯æœ¬  
Â  Â  Â   
Â  Â  Â # è®¡ç®— s26 åˆ—ä¸­â€œæ¡â€å­—çš„å‡ºç°æ¬¡æ•°  
Â  Â  Â selected_columns['count_tiao'] = selected_columns['s26'].apply(lambda x: x.count('æ¡') if pd.notnull(x) else 0)  
Â  Â  Â logging.info(f"Calculated 'count_tiao' from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # è®¡ç®— s23, s25, s26, s27 åˆ—çš„æ€»å­—æ•°  
Â  Â  Â selected_columns['total_chars'] = selected_columns[['s23', 's25', 's26', 's27']].apply(  
Â  Â  Â  Â  Â lambda row: sum(count_chinese_chars(x) for x in row), axis=1  
Â  Â   )  
Â  Â  Â logging.info(f"Calculated 'total_chars' from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # æå–å’Œæ ¼å¼åŒ–æ¡æ¬¾  
Â  Â  Â unique_clauses = extract_clauses(selected_columns['s47'])  
Â  Â  Â selected_columns['unique_clauses'] = unique_clauses Â # ä½¿ç”¨ .loc è¿›è¡Œå®‰å…¨èµ‹å€¼  
Â  Â  Â selected_columns['clause_count'] = [len(clauses) for clauses in unique_clauses] Â # æ¯è¡Œçš„æ¡æ¬¾æ•°é‡  
Â  Â  Â logging.info(f"Extracted and formatted clauses from file: {input_file}")  
Â  Â  Â   
Â  Â  Â # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å  
Â  Â  Â base_name = os.path.basename(input_file)  
Â  Â  Â output_file = os.path.join(output_folder, f"cleaned_{base_name}")  
Â  Â  Â   
Â  Â  Â # å°†ç»“æœä¿å­˜ä¸º txt æ–‡ä»¶  
Â  Â  Â selected_columns[['s7', 'count_tiao', 'total_chars', 'unique_clauses', 'clause_count']].to_csv(output_file, sep='\t', index=False, quoting=1, quotechar='"')  
Â  Â  Â logging.info(f"Processed and saved to: {output_file}")  
Â  Â  Â   
Â  Â  Â # æ±‡æŠ¥è¿›åº¦  
Â  Â  Â processed_rows = len(data)  
Â  Â  Â logging.info(f"å·²å®Œæˆå¤„ç†æ–‡ä»¶: {base_name}, æ¡ç›®æ•°: {processed_rows}/{total_rows}")  
Â â€‹  
Â  Â  Â # ä¿å­˜é”™è¯¯æ—¥å¿—  
Â  Â  Â if error_logs:  
Â  Â  Â  Â  Â log_file = os.path.join(log_folder, f"error_log_{base_name}.txt")  
Â  Â  Â  Â  Â with open(log_file, 'w', encoding='utf-8') as f:  
Â  Â  Â  Â  Â  Â  Â for log in error_logs:  
Â  Â  Â  Â  Â  Â  Â  Â  Â f.write(f"File: {log[0]}, Line: {log[1]}, Content: {log[2]}\n")  
Â  Â  Â  Â  Â logging.info(f"Error logs saved to: {log_file}")  
Â â€‹  
Â # éå†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶  
Â all_files = glob.glob(os.path.join(input_folder_path, "*.txt"))  
Â total_files = len(all_files)  
Â processed_files = 0  
Â â€‹  
Â if not all_files:  
Â  Â  Â logging.warning("No files found in the input folder.")  
Â else:  
Â  Â  Â for input_file in all_files:  
Â  Â  Â  Â  Â processed_files += 1  
Â  Â  Â  Â  Â # æ±‡æŠ¥å½“å‰å¤„ç†çš„æ–‡ä»¶è¿›åº¦  
Â  Â  Â  Â  Â logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {input_file} ({processed_files}/{total_files})")  
Â  Â  Â  Â  Â process_file(input_file, output_folder_path, log_folder_path, total_files)  
```
Â â€‹
### ç§»åŠ¨ txt

å› ä¸ºä»£ç å¾ªç¯éå¸¸å®¹æ˜“è¢«æ‰“æ–­ï¼Œæˆ‘å»ºè®®è®¾ç½®**ä¸‰ä¸ªæ–‡ä»¶å¤¹**ï¼š

åŸå§‹æ–‡ä»¶å¤¹å­˜æ”¾åŸå§‹æ•°æ®ï¼Œ

ç»“æœæ–‡ä»¶å¤¹å­˜æ”¾å¤„ç†å®Œæˆçš„æ•°æ®ï¼Œç»Ÿä¸€æ”¹ä¸º `cleaned_*` çš„ txt åå­—ï¼Œ

æ¸…æ´—å®Œæˆçš„æ–‡ä»¶å¤¹ï¼šæŠŠæ¸…æ´—å®Œæˆçš„åŸå§‹æ•°æ®æ”¾åˆ°è¿™é‡Œï¼Œä¾¿äºå¾ªç¯æ‰“æ–­åç»§ç»­å¼€å§‹ï¼Œ

å› æ­¤æœ¬ä»£ç çš„é€»è¾‘å°±æ˜¯å¯¹ç…§**æ¸…æ´—å®Œæˆçš„æ–‡ä»¶å¤¹**å’Œ**åŸå§‹æ–‡ä»¶å¤¹**ï¼Œå°†åå­—æœ‰å¯¹åº”çš„ txt æ–‡æ¡£è¿›è¡Œç§»åŠ¨ã€‚

```python
Â import os  
Â import shutil  
Â â€‹  
Â # å®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„  
Â source_folder = r'F:/æ¡Œé¢/æµ‹è¯•/æ¸…æ´—ç»“æœ'  
Â data_folder = r'F:/æ¡Œé¢/æµ‹è¯•/æ•°æ®'  
Â cleaned_folder = r'F:/æ¡Œé¢/æµ‹è¯•/å®Œæˆæå–'  
Â â€‹  
Â # è·å–æºæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰.txtæ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰ï¼Œå¹¶å»é™¤å‰ç¼€ "cleaned_"ï¼ŒåŒæ—¶å»é™¤æ‰€æœ‰å¤šä½™ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå°å†™  
Â def standardize_filename(filename):  
Â  Â  Â return ''.join(filename.split()).lower()  
Â â€‹  
Â # è·å–æºæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶å  
Â source_files = [f for f in os.listdir(source_folder) if f.startswith('cleaned_') and f.endswith('.txt')]  
Â standardized_source_files = {standardize_filename(f[8:]): f for f in source_files}  
Â â€‹  
Â # è·å–æ•°æ®æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶å  
Â data_files = [f for f in os.listdir(data_folder) if f.endswith('.txt')]  
Â standardized_data_files = set(standardize_filename(f) for f in data_files)  
Â â€‹  
Â # æ‰¾å‡ºæ•°æ®æ–‡ä»¶å¤¹ä¸­ä¸æºæ–‡ä»¶å¤¹ä¸­ç›¸åŒçš„æ–‡ä»¶  
Â common_files = standardized_source_files.keys() & standardized_data_files  
Â â€‹  
Â # ç§»åŠ¨ç›¸åŒçš„æ–‡ä»¶åˆ°æ¸…æ´—å®Œæˆæ–‡ä»¶å¤¹  
Â for filename in common_files:  
Â  Â  Â original_filename = standardized_source_files[filename]  
Â  Â  Â source_path = os.path.join(data_folder, original_filename.replace('cleaned_', ''))  
Â  Â  Â destination_path = os.path.join(cleaned_folder, original_filename.replace('cleaned_', ''))  
Â  Â  Â   
Â  Â  Â # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨  
Â  Â  Â os.makedirs(os.path.dirname(destination_path), exist_ok=True)  
Â  Â  Â   
Â  Â  Â shutil.move(source_path, destination_path)  
Â  Â  Â print(f"Moved: {source_path} -> {destination_path}")  
Â â€‹  
Â print("Process completed.")
```


## ä¾‹å­ ï¼š è£åˆ¤æ–‡ä¹¦ç´¢å¼•é‡æ–°æ’åº


**åˆå®¡æ¡ˆå·æœ€æ—©çš„æ’åœ¨å‰ï¼ˆm_sorted_1ï¼‰**

**ç»ˆå®¡æ¡ˆå·æœ€æ™šçš„æ’åœ¨åï¼ˆm_sorted_3ï¼‰**

**å†å®¡æ¡ˆä»¶æŒ‰ç…§â€œç»ˆå®¡å‰æœ€æ—©ã€ç»ˆå®¡åæœ€æ™šâ€å®‰æ’ï¼ˆm_sorted_2, m_sorted_4ï¼‰**

**å…¶ä½™æ¡ˆå·æŒ‰æ—¶é—´é¡ºåºè¡¥å……ç©ºä½ï¼ˆm_sorted_5ï¼‰**



```python
import pandas as pd  
from datetime import datetime  
  
  
def parse_date(year_str, month_str):  
    """å°†å¹´ä»½ã€æœˆä»½å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡ï¼Œè‹¥è½¬æ¢å¤±è´¥è¿”å› None"""    try:  
        year = int(year_str)  
        month = int(month_str) if month_str and month_str.strip() != "" else 1  
        return datetime(year, month, 1)  
    except Exception:  
        return None  
  
  
def reorder_row(row):  
    # è¦ä¸€èµ·ç§»åŠ¨çš„é¢å¤–å­—æ®µåˆ—è¡¨ï¼ˆé™¤äº† m_sorted_i ä¹‹å¤–çš„å…¶å®ƒå…³è”å˜é‡ï¼‰  
    extra_fields = ['court', 'court_name', 'jud_year', 'jud_month', 'win', 'judge_am', 'judge_fin']  
    entries = []  
  
    # éå†åŸæœ‰ 5 ç»„æ•°æ®ï¼Œæ„é€ æ¯ç»„æ¡ç›®ï¼ˆåªæœ‰å½“ m*_judge_am å’Œ m*_jud_year å‡æœ‰å€¼æ—¶è®¤ä¸ºè¯¥ç»„æœ‰æ•ˆï¼‰  
    for i in range(1, 6):  
        judge_am = row.get(f"m{i}_judge_am")  
        jud_year = row.get(f"m{i}_jud_year")  
        if pd.isna(judge_am) or pd.isna(jud_year) or judge_am == "" or jud_year == "":  
            continue  
        case_str = row.get(f"m_sorted_{i}")  
        if not case_str or pd.isna(case_str):  
            continue  
        jud_month = row.get(f"m{i}_jud_month")  
        dt = parse_date(jud_year, jud_month)  
  
        # åˆ¤æ–­æ¡ˆå·ç±»å‹  
        if "åˆ" in case_str:  
            typ = "chu"  
        elif "ç»ˆ" in case_str:  
            typ = "zhong"  
        elif "å†" in case_str:  
            typ = "zai"  
        else:  
            typ = "other"  
  
        # æ”¶é›†å…¶å®ƒç›¸å…³å­—æ®µ  
        fields_data = {}  
        for field in extra_fields:  
            col_name = f"m{i}_{field}"  
            fields_data[field] = row.get(col_name, "")  
  
        entry = {  
            "orig_index": i,  # åŸæ¥çš„ç»„å·  
            "case": case_str,  # æ¡ˆå·å­—ç¬¦ä¸²  
            "date": dt,  # è½¬æ¢åçš„æ—¥æœŸï¼ˆå¯èƒ½ä¸º Noneï¼‰  
            "type": typ,  # æ¡ˆå·ç±»å‹ï¼šchu / zhong / zai / other  
            "fields": fields_data  
        }  
        entries.append(entry)  
  
    # è‹¥æ²¡æœ‰æœ‰æ•ˆæ¡ç›®ï¼Œåˆ™è¿”å›æ‰€æœ‰æ–°ç»„ä¸ºç©º  
    if not entries:  
        return pd.Series({f"m_sorted_{i}": "" for i in range(1, 6)})  
  
    # å®šä½â€œåˆâ€ï¼šä»æ‰€æœ‰åŒ…å«â€œåˆâ€çš„æ¡ç›®ä¸­å–æ—¥æœŸæœ€æ—©çš„  
    chu_entries = [e for e in entries if e["type"] == "chu" and e["date"] is not None]  
    initial_entry = min(chu_entries, key=lambda e: e["date"]) if chu_entries else None  
  
    # å®šä½â€œç»ˆâ€ï¼šä»æ‰€æœ‰åŒ…å«â€œç»ˆâ€çš„æ¡ç›®ä¸­å–æ—¥æœŸæœ€å¤§çš„  
    zhong_entries = [e for e in entries if e["type"] == "zhong" and e["date"] is not None]  
    terminal_entry = max(zhong_entries, key=lambda e: e["date"]) if zhong_entries else None  
  
    # è·å–æ‰€æœ‰â€œå†â€çš„æ¡ç›®  
    zai_entries = [e for e in entries if e["type"] == "zai" and e["date"] is not None]  
  
    # åˆå§‹åŒ–æ–°æ’åºä½ç½®å­—å…¸ï¼Œè¡¨ç¤ºæœ€ç»ˆæ”¾ç½®åœ¨ m_sorted_1ï½m_sorted_5 ä¸­çš„æ¡ç›®  
    new_positions = {1: None, 2: None, 3: None, 4: None, 5: None}  
  
    # m_sorted_1 å§‹ç»ˆæ”¾â€œåˆâ€æ¡ˆ  
    if initial_entry:  
        new_positions[1] = initial_entry  
  
    if terminal_entry:  
        # å¦‚æœå­˜åœ¨ç»ˆæ¡ˆï¼Œåˆ™ m_sorted_3 å›ºå®šæ”¾ç»ˆæ¡ˆ  
        new_positions[3] = terminal_entry  
  
        # åˆ†ç»„ï¼šç»ˆæ¡ˆä¹‹å‰çš„â€œå†â€  
        re_before = [e for e in zai_entries if e["date"] < terminal_entry["date"]]  
        # åˆ†ç»„ï¼šç»ˆæ¡ˆä¹‹åçš„â€œå†â€  
        re_after = [e for e in zai_entries if e["date"] > terminal_entry["date"]]  
  
        # æ—¥æœŸæ—©äºç»ˆæ¡ˆçš„â€œå†â€ä¸­å–æœ€æ—©çš„æ”¾å…¥ m_sorted_2        if re_before:  
            new_positions[2] = min(re_before, key=lambda e: e["date"])  
  
        # æ—¥æœŸæ™šäºç»ˆæ¡ˆçš„â€œå†â€ä¸­å–æœ€æ™šçš„æ”¾å…¥ m_sorted_4        if re_after:  
            new_positions[4] = max(re_after, key=lambda e: e["date"])  
    else:  
        # å¦‚æœä¸å­˜åœ¨ç»ˆæ¡ˆï¼Œå³åªæœ‰â€œåˆâ€ä¸â€œå†â€  
        zai_sorted = sorted(zai_entries, key=lambda e: e["date"])  
        if len(zai_sorted) == 1:  
            new_positions[2] = zai_sorted[0]  
        elif len(zai_sorted) >= 2:  
            # æœ€æ—©çš„æ”¾å…¥ m_sorted_2ï¼Œæœ€æ™šçš„æ”¾å…¥ m_sorted_4            new_positions[2] = zai_sorted[0]  
            new_positions[4] = zai_sorted[-1]  
  
    # å°†å‰©ä½™æœªåˆ†é…çš„æ¡ç›®ï¼ˆåŒ…æ‹¬ç±»å‹ä¸º other æˆ–æœªè¢«é€‰ä¸­çš„å…¶ä»–ï¼‰æŒ‰æ—¥æœŸæ’åºåå¡«å…¥å‰©ä½™ç©ºä½ï¼ˆä¾‹å¦‚ m_sorted_5ï¼‰  
    assigned_indices = {e["orig_index"] for e in new_positions.values() if e is not None}  
    remaining = [e for e in entries if e["orig_index"] not in assigned_indices]  
    remaining_sorted = sorted(remaining, key=lambda e: e["date"] if e["date"] is not None else datetime.max)  
    for pos in [2, 5]:  
        if new_positions[pos] is None and remaining_sorted:  
            new_positions[pos] = remaining_sorted.pop(0)  
  
    # æ„é€ è¿”å›çš„ Seriesï¼šæŒ‰æ–°çš„ç»„å· 1ï½5 å†™å…¥ m_sorted_i ä¸å¯¹åº”çš„å…³è”å­—æ®µ  
    result = {}  
    for new_group in range(1, 6):  
        if new_positions[new_group]:  
            entry = new_positions[new_group]  
            result[f"m_sorted_{new_group}"] = entry["case"]  
            for field in extra_fields:  
                result[f"m{new_group}_{field}"] = entry["fields"].get(field, "")  
        else:  
            result[f"m_sorted_{new_group}"] = ""  
            for field in extra_fields:  
                result[f"m{new_group}_{field}"] = ""  
  
    return pd.Series(result)  
  
  
# è¯»å–æ•°æ®ï¼ˆæ³¨æ„è·¯å¾„ä¸­ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²é˜²æ­¢åæ–œæ è½¬ä¹‰é—®é¢˜ï¼‰  
df = pd.read_csv(r"C:\Users\PC\Desktop\æ¡ˆå·æ’åº.txt", sep=",", dtype=str)  
  
# å¯¹æ¯ä¸€è¡Œè°ƒç”¨ reorder_rowï¼Œå¾—åˆ°é‡æ–°æ’åºåçš„ m* ç³»åˆ—æ•°æ®  
df_new = df.apply(reorder_row, axis=1)  
  
# å°†é‡æ–°æ’åºåçš„å˜é‡æ›´æ–°å›åŸ DataFrameï¼ˆåŒ…æ‹¬ m_sorted_1ï½m_sorted_5 åŠå„ç»„çš„å…³è”å­—æ®µï¼‰  
for col in [f"m_sorted_{i}" for i in range(1, 6)]:  
    df[col] = df_new[col]  
  
for new_group in range(1, 6):  
    for field in ['court', 'court_name', 'jud_year', 'jud_month', 'win', 'judge_am', 'judge_fin']:  
        col_name = f"m{new_group}_{field}"  
        df[col_name] = df_new[col_name]  
  
# ä¿å­˜ç»“æœåˆ°æ–°æ–‡ä»¶  
df.to_csv(r"C:\Users\PC\Desktop\ç»“æœ_æ¡ˆå·æ’åº.txt", index=False)
```


## dta æ•°æ®çš„åŒ¹é…

### å…³äºæ¡ˆå·


è£åˆ¤æ–‡ä¹¦çš„æ¡ˆå·åˆ†ä¸ºè£å®šæ–‡ä¹¦ã€åˆ¤å†³æ–‡ä¹¦ã€å…¶ä»–æ–‡ä¹¦ï¼ˆç›‘ç£ã€æ‰§è¡Œã€ä¿è¯....ï¼‰

è£å®šæ–‡ä¹¦æ˜¯å®¡åˆ¤ç¨‹åºä¸Šçš„é—®é¢˜ï¼Œä¾‹å¦‚è¿™ä¸ªé—®é¢˜ä¸è¯¥xxæ³•é™¢ç®¡ï¼Œç§»äº¤åˆ°xxæ³•é™¢å»ã€‚

åˆ¤å†³æ–‡ä¹¦æ˜¯å®ä½“é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯åˆ¤å†³çš„æœ€ç»ˆç»“æœã€‚

æ³¨æ„ï¼åˆ¤å†³æ–‡ä¹¦å’Œè£åˆ¤æ–‡ä¹¦æ˜¯å¯èƒ½å…±äº«ä¸€ä¸ªæ¡ˆå·çš„ï¼

åŒæ—¶åœ¨è¿™é‡Œåˆ†äº«ä¸€äº›ä¸ªäººçœ‹åˆ°çš„å¥‡è‘©è£åˆ¤æ–‡ä¹¦æ¡ˆå·ï¼Œæœ‰ä¸€äº›å¥‡æ€ªçš„ç¬¦å·å¯èƒ½æ˜¯åˆ«äººç”¨è¿‡äº†ç„¶åå†åŠ ç¬¦å·ä½œä¸ºåŒºåˆ†ã€‚

```
2018ï¼ˆï¼‰çš–02æ°‘ç»ˆ1098å·  
2016æ¹˜06æ°‘ç»ˆ2392å·  
ï¼ˆï¼ˆ2017ï¼‰è±«0481æ°‘å†10å· 
ä¸€ï¼ˆ2017ï¼‰ç²¤0605æ°‘åˆ10499å· 
(2015)èŠ™æ°‘åˆå­—ç¬¬7479ã€7601å· 
ï¼ˆ2015ï¼‰èŠ™æ°‘åˆå­—ç¬¬7478.7603å· 
ï¼ˆ2019ï¼‰ç²¤0305æ°‘åˆ1382å·-1429å· 
ï¼ˆ2019ï¼‰é²1724æ°‘åˆ195å·- 018ï¼‰
è‹0509æ°‘åˆ6820å· 
ï½Œï½ï½š2016ï½Œï½ï½šèµ£1030æ°‘åˆ220å·  
ä¸€äº¿äºŒåƒä¸‰ç™¾å››åäº”ä¸‡ï¼ˆ2016ï¼‰ç²¤0605æ°‘åˆ14359å· 
ï¼ˆ2018ï¼‰åœŸåœ°æ‰¿åŒ…ç»è¥æƒè½¬è®©æµ™0523æ°‘åˆ2386å· 
2016é—½0525æ°‘åˆ3798å·2016é—½0525æ°‘åˆ3798å· 
ï¼ˆç©ºä¸€è¡Œè¡Œè·15ç£…ï¼‰ï¼ˆ2014ï¼‰æ»¨æ¸¯æ°‘åˆå­—ç¬¬307å·
```

ç”±äºä¸­è‹±æ–‡æ‹¬å·çš„é—®é¢˜ï¼Œå»ºè®®å¯ä»¥å»é™¤æ‹¬å·ï¼Œä½†æ˜¯ä¸å»ºè®®é‡‡ç”¨åªä¿ç•™ä¸­æ–‡å’Œæ•°å­—çš„ç­›é€‰æ–¹å¼ï¼Œå¾ˆå¤šæ—¶å€™çŸ­æ¨ªçº¿ç¥ç§˜å­—ç¬¦ä¹Ÿæœ‰æ ‡è¯†ä½œç”¨ã€‚

ä¸ªäººåŒ¹é…çš„ç­›é€‰å¦‚ä¸‹ï¼š

```
sort case_wid
drop if case_wid ==""

foreach var of varlist case_wid {
	* å»é™¤ç©ºæ ¼å˜é‡ã€æ‹¬å·ã€å…¶ä»–ç¬¦å·
	replace `var' = ustrregexra(`var',"[^\p{L}\p{N}]", "")
		}
sort case_wid

gen case_length = strlen(case_wid)

summarize case_length

drop if  case_length > 66

drop case_length
```
### æ ‡è¯†çš„å»é‡

Txt æå–å®Œæˆåï¼Œå°±æ˜¯è½¬åŒ–ä¸º `.dta` åŒ¹é…åˆ° `ans*.dta` ä¸­ã€‚

**æ¡ˆå·**æ˜¯ç†è®ºä¸Šçš„å”¯ä¸€æ ‡è¯†ï¼Œä½†å®é™…ä¸Šå¯èƒ½å‡ºç°é‡å¤çš„æƒ…å†µã€‚

è¿™æ—¶å€™å»ºè®®ä½¿ç”¨ä»¥ä¸‹é€»è¾‘ï¼Œå…ˆå†éæ‰€æœ‰å˜é‡ï¼Œç”Ÿæˆéç©ºæ•°å€¼æ€»æ•°ï¼Œå»é‡ï¼Œåœ¨æ¡ˆå·çš„é‡å¤è¡Œé—´ä¿ç•™éç©ºæ•°å€¼æœ€å¤šçš„ä¸€è¡Œï¼š

```python
Â * æ ‡è®°é‡å¤çš„ case_widï¼Œå¹¶è®¡ç®—æ¯ç»„çš„æ•°é‡  
Â  Â   bysort case_wid: gen dup_count = _N  
Â  Â   replace dup_count = . if _n > 1 // å¯é€‰ï¼šåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®¡æ•°  
Â  Â   * åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è¦æ£€æŸ¥çš„å˜é‡çš„å±€éƒ¨å®  
Â  Â   local varlist var1 var2  
Â  Â   * ä½¿ç”¨å¾ªç¯åˆ›å»ºæ ‡å¿—å˜é‡  
Â  Â   foreach var of local varlist {  
Â  Â  Â  Â   gen flag_`var' = cond(missing(`var'), 0, 1)  
Â  Â   }  
Â  Â   * è®¡ç®—æ¯è¡Œä¸­éç©ºå€¼çš„æ•°é‡  
Â  Â   egen non_missing = rowtotal(flag*)  
Â  Â   * å¯¹æ¯ä¸ª case_wid æŒ‰éç©ºå˜é‡æ•°é‡é™åºæ’åºï¼Œå¹¶æ ‡è®°ä¿ç•™çš„è§‚æµ‹å€¼  
Â  Â   bysort case_wid (non_missing): gen keep = (_n == _N)  
Â  Â   * æ¸…ç†ä¸´æ—¶å˜é‡  
Â  Â   capture drop flag*  
Â  Â   * åˆ é™¤é‡å¤å€¼ï¼Œä¿ç•™éç©ºå˜é‡æœ€å¤šçš„è¡Œ  
Â  Â   drop if !keep  
Â  Â   * æ¸…ç†è¾…åŠ©å˜é‡  
Â  Â   drop dup_count non_missing keep  
Â  Â   * ä¿å­˜å¤„ç†åçš„æ–‡ä»¶ï¼Œè¦†ç›–åŸæœ‰æ–‡ä»¶  
Â  Â   save "`input_file'", replace  
Â }
```

### ç´¢å¼•å€¼çš„å»é‡

```stata
* å¯¹ç¬¬äºŒä¸ªå˜é‡ï¼Œè‹¥ä¸ç¬¬ä¸€ä¸ªå˜é‡ç›¸åŒåˆ™æ¸…ç©º
replace m_sorted_2 = "" if m_sorted_2 == m_sorted_1

* å¯¹ç¬¬ä¸‰ä¸ªå˜é‡ï¼Œè‹¥ä¸å‰é¢ä»»ä¸€å˜é‡ç›¸åŒåˆ™æ¸…ç©º
replace m_sorted_3 = "" if m_sorted_3 == m_sorted_2 | m_sorted_3 == m_sorted_1

* å¯¹ç¬¬å››ä¸ªå˜é‡ï¼Œè‹¥ä¸å‰é¢ä»»ä¸€å˜é‡ç›¸åŒåˆ™æ¸…ç©º
replace m_sorted_4 = "" if m_sorted_4 == m_sorted_3 | m_sorted_4 == m_sorted_2 | m_sorted_4 == m_sorted_1
* å¯¹ç¬¬äº”ä¸ªå˜é‡ï¼Œè‹¥ä¸å‰é¢ä»»ä¸€å˜é‡ç›¸åŒåˆ™æ¸…ç©º
replace m_sorted_5 = "" if  m_sorted_5 == m_sorted_4 | m_sorted_5 == m_sorted_3 | m_sorted_5 == m_sorted_2 | m_sorted_5 == m_sorted_1
```

### æ–‡æœ¬åŒ¹é…æ ‡è¯†çš„æ ¼å¼è½¬åŒ–

`ans*.dta` ç³»åˆ—çš„æ–‡æœ¬æ ¼å¼éƒ½æ˜¯ `strL`ï¼Œå«ä¹‰ä¸ºé•¿å­—ç¬¦ä¸²ã€‚Stata çš„å­—ç¬¦ä¸²åŒ¹é…åªèƒ½é€‰æ‹©æœ‰å…·ä½“é•¿åº¦çš„å­—ç¬¦ä¸²æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å­—ç¬¦æˆªå–æ¥æ”¹å˜å˜é‡æ ¼å¼ã€‚

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
&#x1F50A<b> é‡è¦ï¼šæ³¨æ„ï¼Œè‹±æ–‡å­—ç¬¦çš„å ä½ç¬¦å’Œä¸­æ–‡å­—ç¬¦çš„å ä½ç¬¦æ˜¯ä¸åŒçš„ã€‚</b>
</div>

> ä¾‹å¦‚â€œæ°‘åˆ 730 å·â€ï¼Œè™½ç„¶åªæ˜¯å…­ä¸ªå­—ç¬¦ï¼Œä½†å¯èƒ½å®é™…ä¸Šå äº† 15 ä¸ªä½ç½®ã€‚
> 
> å¦‚æœæˆ‘ä»¬åªæˆªå– 6 ä¸ªé•¿åº¦ï¼Œå˜é‡å°±ä¼šå˜æˆä¹±ç ã€‚
> 
> ä¸ªäººè§‚å¯Ÿåˆ°çš„æ¡ˆå·å ä½ç¬¦æœ€é•¿å¤§æ¦‚æ˜¯ 39 ä¸ªä½ç½®ï¼Œä¸ªäººä¸ºäº†ç¨³å¥ç›´æ¥é€‰æ‹©çš„æˆªå– 88 ä¸ªå­—ç¬¦ç¬¦å·ã€‚
> 
> è¿™ä¹Ÿæ„å‘³ç€ï¼š æˆ‘ä»¬éœ€è¦åœ¨æ¸…ç†æ‰æ–‡æœ¬ä¸­çš„å¥‡æ€ªç¬¦å·ä»¥åå†è½¬åŒ–ã€‚
> 
> ä¾‹å¦‚æœ‰äº›æ³•é™¢åå­—æ˜¯"`å¹¿è¥¿å£®æ—è‡ªæ²»åŒºæŸ³å·å¸‚é±¼å³°åŒºäººæ°‘æ³•é™¢`"

```stata
Â # æˆªå–å­—ç¬¦  
Â gen judge_fin2 = substr(judge_fin, 1, 45)  
Â gen court_name2 = substr(court_name, 1, 88)  
Â drop judge_fin court_name  
Â rename judge_fin2  judge_fin  
Â rename court_name2 court_name
Â 
Â 
Â # å¤§äºç‰¹å®šå­—ç¬¦æ•°é‡ï¼Œä¾‹å¦‚66ï¼Œçš„æ¡ˆå·æ¸…æ´—ä¸ºç©ºå€¼

foreach var of varlist s7 m_sorted_1 m_sorted_2 m_sorted_3 m_sorted_4 {
	
	gen case_length = strlen(`var')

	summarize case_length

	replace `var' =  "" if  case_length > 66

	drop case_length		 
		
		}
```

### åŒ¹é…å‘½ä»¤çš„é€‰æ‹©

**å¤šï¼ˆåŸºç¡€è¡¨æ ¼ï¼‰å¯¹ä¸€æ—¶**ï¼š`merge` å‘½ä»¤ç›´æ¥ä½¿ç”¨ã€‚

**ä¸€ï¼ˆåŸºç¡€è¡¨æ ¼ï¼‰å¯¾å¤šæ—¶**ï¼šä¾‹å¦‚ txt æœ‰ä¸€äº›é‡å¤è®°å½•ï¼Œä½†æ˜¯ä¸åŒè¡Œçš„ç©ºç¼ºå€¼ä¸åŒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `merge` çš„è¿½åŠ åŒ¹é… `update nogen force`ï¼Œåªåœ¨è¿™ä¸€è¡Œæœ‰ç©ºç¼ºæ—¶ï¼Œå°†ç©ºç¼ºå€¼çš„å˜é‡åŒ¹é…ä¸Šå»ï¼›éç©ºç¼ºçš„éƒ¨åˆ†åˆ™ä¸æ›´æ”¹ã€‚

```stata
Â clear   
Â forvalues j = 1/83 {  
Â  Â   use "D:\æ•°æ®åˆé›†\åŸå§‹æ•°æ®\ans`j'.dta", clear  
Â  Â   merge 1:m case_wid_str using "C:\Users\PC\Desktop\hzp_project_re\ä¸€å®¡äºŒå®¡æ•°æ®åŒ¹é…\æ’åº\é¢æ¿æ•°æ®\all.dta", update nogen force  
Â  Â   drop if case_wid == ""  
Â duplicates drop case_wid_str , force  
Â  Â   save "C:\Users\PC\Desktop\hzp_project_re\ä¸€å®¡äºŒå®¡æ•°æ®åŒ¹é…\æ’åº\é¢æ¿æ•°æ®\ans`j'.dta", replace  
Â }
```

å¤šå¯¹å¤šï¼ˆåŸºç¡€è¡¨æ ¼ï¼‰åŒ¹é…æ—¶ï¼šæ­¤æ—¶ä½¿ç”¨ `merge` ä¸å†åˆé€‚ï¼Œæ¨è `joinby`

è¯¦ç»†å¯å‚è€ƒ [Stataå‘½ä»¤ï¼šjoinby VS merge m:må¸¸è§é—®é¢˜](https://mp.weixin.qq.com/s/bTFFvihL221zM2MYaGZcuA)

### å¾ªç¯åŒ¹é…å‘½ä»¤

å½“æˆ‘ä»¬å†™å¾ªç¯åŒ¹é…å‘½ä»¤æ—¶ï¼Œæ•°å­—å¯èƒ½æ˜¯é—´æ–­ç‚¹ï¼Œä¾‹å¦‚ data 1ã€data 3ã€data 5ã€data 7...

å¯ä»¥é€‰æ‹©ä½¿ç”¨ python ç›´æ¥æ›´æ”¹æ–‡ä»¶åå­—æŒ‰é¡ºåºæ’åºï¼Œä½†å®¹æ˜“ç ´åå‘½åå«ä¹‰ã€‚

è¿™é‡Œæä¾›ä¸ªäººå†™çš„ stata è‡ªç„¶æ•°å¾ªç¯å‘½ä»¤ï¼š

> å½“ç„¶ï¼Œå…¶å®å¯ä»¥é€šè¿‡æ‰«ææ–‡ä»¶å¤¹ä¸‹é¢çš„æ–‡ä»¶åè¯†åˆ«æ‰€æœ‰æ–‡ä»¶ï¼Œç„¶åå¾ªç¯ï¼Œä½†æ˜¯å¦‚æœæ˜¯å­˜åœ¨ä¸€äº›æ–‡ä»¶éœ€è¦æ‰‹åŠ¨æ’é™¤å¯ä»¥å‚è€ƒä¸‹è¿™ä¸ªä»£ç ã€‚

```stata
Â * å®šä¹‰æ’é™¤åˆ—è¡¨  
Â global exclude_list 22 50 54 58  105  
Â â€‹  
Â * åˆå§‹åŒ– valid_files å˜é‡  
Â global valid_files  
Â â€‹  
Â * å¾ªç¯éå†æ–‡ä»¶ç¼–å·èŒƒå›´ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­  
Â forvalues i = 1/119 {  
Â  Â   local is_excluded 0  
Â  Â   foreach excl in $exclude_list {  
Â  Â  Â  Â   if `i' == `excl' {  
Â  Â  Â  Â  Â  Â   local is_excluded 1  
Â  Â  Â  Â  Â  Â   break  
Â  Â  Â  Â   }  
Â  Â   }  
Â  Â   if !`is_excluded' {  
Â  Â  Â  Â   global valid_files $valid_files `i'  
Â  Â   }  
Â }  
Â â€‹  
Â * æ˜¾ç¤ºæœ‰æ•ˆçš„æ–‡ä»¶ç¼–å·  
Â display "Valid file numbers: $valid_files"  
Â â€‹  
Â â€‹  
Â * å¾ªç¯å¤„ç†æœ‰æ•ˆçš„æ–‡ä»¶ç¼–å·  
Â foreach i of global valid_files {  
Â  Â   *å†™å…¥è‡ªå·±çš„å¾ªç¯åŒ¹é…ä»£ç   
Â }
```

ä½†æ˜¯å¦‚æœé‡åˆ°çš„å®Œå…¨ä¹±ç åå­—çš„æ–‡ä»¶æ€ä¹ˆåŠï¼Ÿåœ¨æ–‡ä»¶åæ²¡æœ‰ç©ºæ ¼æˆ–è€…ç‰¹æ®Šç¬¦å·çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è¯»å–æ–‡ä»¶åå­˜å…¥ `local` å†åŠ å…¥å¾ªç¯ã€‚è¿™ä¸ªä¹Ÿæ˜¯æ›´åŠ æ³›ç”¨çš„ä»£ç ã€‚

ä¾‹å¦‚ä»¥ä¸‹ä¹±ç ï¼š

![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-1769873803723.webp)
- æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ stata çš„ fs åŒ…è¯»å–æ‰€æœ‰åå­—
- ç„¶åä½¿ç”¨ local ç›´æ¥å‚¨å­˜åå­—è¿›è¡Œå¾ªç¯
- å¾ªç¯è¿‡ç¨‹ä¸­å¯ä»¥åµŒå¥—ä¸€å±‚å¾ªç¯ï¼Œä½¿å¾—æ–°æ–‡ä»¶æŒ‰ç…§é¡ºåºå‘½å

```sql
* ==============================
* é…ç½®è·¯å¾„
* ==============================
local path "F:\æ•°æ®åˆé›†\åˆ‘äº‹æ¡ˆä»¶"
local outpath "F:\æ•°æ®åˆé›†\åˆ‘äº‹æ¡ˆä»¶ç®€æ´"

cd "`path'"


fs "*.csv", all

* ==============================
* æ‰‹åŠ¨ç”Ÿæˆ CSV æ–‡ä»¶åˆ—è¡¨åˆ° local
* ==============================
local files " 080hfjsviunvohos.csv 749baig9gvbblcn.csv     97sytdv97goushd.csv 08cdhas0hvihikd.csv 7839yhcudaboce.csv      98ya08chiahnicn.csv 0s87d0hvcinowbv.csv 788tgd9cayh0c-9.csv 9a7gtdc97g08a0a.csv 1368bcohucg970.csv 797fshonlsnlfu98.csv 9f79sghuvjksbklk.csv 236797rbvlsbvoo.csv 79d7goadbco9hki.csv 9s7fhosivnlsnlnffr.csv 2489ndsujbovbo.csv 79s7dhnofnvlbns.csv a97dyc8udv8+hds9.csv 28638bvbsa7f93.csv 80sdhv08uisnvoh.csv i7sdhvu9hbosdv8.csv 2937hvoushcv8h.csv 89hdoshvoinsnnlj.csv oa7gc7agbkd78ad.csv 4689578yhkjsbd.csv 89s0fjisn7vgiskub.csv oc8ahhc89ha98g.csv 4790r8hvobosh7.csv 8y08h0chaicnil8o.csv os8dvyh089shdvi.csv 4792794bvoshc8.csv 94573hvsoidhv9y.csv s8oyvf08s09vu08.csv 479hu5nnv89007.csv 9473hgnvov9h7s.csv      u8ca0hc80ha00a.csv 4927bvogf92h011.csv 97atcg97ga0hc08.csv ughdso8h98989.csv 739g4foh8hv991.csv 97sgvc98dshv08.csv "


* ==============================
* å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶
* ==============================
* åˆå§‹åŒ–è®¡æ•°å™¨
local i = 1

* å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶
foreach f of local files {
    di "Processing file `i': `f'"
    * å¯¼å…¥ CSV
    import delimited "`f'", clear varnames(1) encoding(utf8)
    * æ•°æ®æ¸…æ´—
    * å¤„ç†éƒ¨åˆ†
    * ä¿å­˜ä¸ºæ•°å­—é¡ºåºçš„ DTA æ–‡ä»¶
    save "`outpath'/`i'.dta", replace
    * è®¡æ•°å™¨ +1
    local ++i
}
```


### åŒ¹é…é€Ÿåº¦çš„ä¼˜åŒ–

æ­£å¸¸åŒ¹é…ï¼Œåº”è¯¥æ˜¯å°† 119 ä¸ª txt è½¬åŒ–çš„ dta åŒ¹é…ï¼ŒåµŒå¥—å¾ªç¯åŒ¹é…åˆ° ans 1-ans 83 ä¸Šå»ã€‚

å¦‚æœç›´æ¥åµŒå¥—å¾ªç¯åŒ¹é…ï¼Œå¯èƒ½éœ€è¦ 4 åˆ° 5 å¤©ã€‚

åŒ¹é…è€—æ—¶è¿‡é•¿çš„åŸå› åœ¨äºæ¯ä¸€æ¬¡åµŒå¥—å¾ªç¯çš„æ–‡ä»¶è¯»å–æ—¶é—´å¤ªé•¿äº†ï¼Œè€Œä¸”ä¸åŒæ–‡ä»¶ä¹‹é—´æœ‰é‡å¤å€¼ã€‚

å»ºè®®å…ˆå°† 119 ä¸ª dta æ–‡ä»¶ä½¿ç”¨ `append` å‘½ä»¤çºµå‘åŒ¹é…åˆ°ä¸€èµ·ï¼Œç„¶åå¾ªç¯åŒ¹é…åˆ° ans 1 åˆ° ans 83 ä¸Šå»ï¼Œè¿™æ ·æœ€å¤šåªéœ€è¦ 2 å¤©å³å¯å®Œæˆå…¨éƒ¨åŒ¹é…ã€‚
â€‹
<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #7d637a; background-color: #f6edf5; border-color: #f1e4f0;">
&#x1F4AC<b> å¤‡æ³¨ï¼šåŠå…¬å®¤ç”µè„‘æ˜¯ stata MP ç‰ˆæœ¬ï¼Œè¿™ä¸ªç‰ˆæœ¬çš„ç‰¹æ€§æ˜¯â€œè®¡ç®—é€Ÿåº¦æ ¹æ®ç”µè„‘çš„æ ¸æ¥åˆ†é…èµ„æºâ€ã€‚ç›®å‰ cpu æœ‰è¶³è¶³ 16 æ ¸ï¼Œä¸ªäººå¤„ç†è¿‡çš„æœ€å¤§é‡å³°å€¼æ˜¯ 60 gã€‚åŸºæœ¬æ²¡æœ‰é—®é¢˜ï¼Œåªéœ€è¦è€å¿ƒç­‰å¾…å¤„ç†è¿‡ç¨‹ä¸­çš„å¡é¡¿ã€‚</b>
</div>

## dta æ•°æ®çš„æ¸…æ´—

### å¸¸ç”¨æ–‡æœ¬å¤„ç†å‡½æ•°

#### å»é™¤ç©ºæ ¼

ä¾‹å¦‚è£åˆ¤é•¿ï¼ˆæ³•å®˜ï¼‰å˜é‡ï¼Œä¼šå‡ºç°ä»¥ä¸‹æƒ…å†µ `å››å· å¤§å­¦`ï¼Œ `å››å·å¤§å­¦`ï¼Œ`å›› å· å¤§ å­¦`ã€‚

ä¸åŒç½‘ç«™çš„ç©ºæ ¼ç¬¦å·å¹¶ä¸ç›¸åŒï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

Â * ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸å¯è§å­—ç¬¦ã€ç©ºç™½å’Œæ ‡ç‚¹ç¬¦å·  
```stata
Â replace judge_fin = ustrregexra(æ³•å®˜åå­—å˜é‡, "[^\p{L}\p{N}]+", "")
```

#### ç­›é€‰ç‰¹å®šå­—ç¬¦

å¦‚æœå˜é‡ä¸­å«æœ‰ç‰¹å®šå­—ç¬¦åˆ™ä¿ç•™

```stata
Â keep if strpos(var, "æ°‘åˆ") > 0
```

å¦‚æœæœ‰ä¸¤ä¸ªå˜é‡ï¼Œä¾‹å¦‚ä¸€ä¸ªæ˜¯è§„èŒƒçš„çœä»½å˜é‡ aï¼ˆå››å·çœï¼Œå†…è’™å¤è‡ªæ²»åŒºï¼‰ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯ä¸è§„èŒƒçš„çœå˜é‡ bï¼ˆå››å·ï¼Œå†…è’™å¤ï¼‰ã€‚å¦‚æœè¦åˆ¤æ–­ a å˜é‡æ˜¯å¦åŒ…å«äº† b å˜é‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ï¼š

```
gen localed = strpos(a, b) > 0
```
#### å»é™¤ç‰¹æ®Šå­—ç¬¦

Ans 1 åˆ° ans 83 çš„æ–‡æœ¬å˜é‡å¤¹æ‚ç€å„ç§éä¸­æ–‡ç¬¦å·ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°å»é™¤ï¼š

ä»¥ä¸‹æ˜¯ä¸ªäººç­›é€‰å‡ºçš„ä¹±ç ç¬¦å·ã€‚

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #7d637a; background-color: #f6edf5; border-color: #f1e4f0;">
&#x1F4AC<b> å¤‡æ³¨ï¼šä¸ºä»€ä¹ˆè¦åšæŒæ’é™¤æ³•ï¼Œè€Œä¸é€‰æ‹©åªä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Ÿ å› ä¸ºä¸ªäººå°è¯•æ—¶ï¼Œå‘ç°éƒ¨åˆ†ä¸­æ–‡å˜é‡ä¼šå› æ­¤å˜æˆä¹±ç ã€‚</b>
</div>

```stata
# å®šä¹‰éœ€è¦ç§»é™¤çš„ç¬¦å·åˆ—è¡¨ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰  
local symbols "â…©â…©â…© ï¼¸ ? ; ï¼›ï¼Œ ã€Š ã€‹ ï¼ ï¼‹ ï¼ˆ ï¼‰ ( ) & # 1 2 3 4 5 6 7 8 9 0 & A-Z a-z ï¼›ï¼Ÿ ï¼Š ï¸° ++  . Â  XX XXX Â  ______ Â  ` - +  BRR H imestimes jzwjzwnjzwnjzwjzwnjzwnjzwnjzwjzwjzwjzwnjzwnjzwnjzwjzwnjzwnj jzwnjzwjzwjzwnjzwnjzwnjzwnjzwnjzwjzwjzwjzwnjzwjzwnjzwjzwj spemsp sp emsp ï¼›ï¼› *  XX XXX ï¼¸ ï¼¸ï¼¸ï¼¸"  
â€‹  
* éå†ç¬¦å·åˆ—è¡¨å¹¶ç§»é™¤æ¯ä¸ªç¬¦å·åŠç©ºæ ¼  
foreach s of local symbols {  
 Â   replace judge_fin = subinstr(judge_fin, "`s'", "", .)  
}
```

#### è®ºæ–‡æ ‡ç­¾

æœ‰äº›æ—¶å€™ï¼Œstata æ•°æ®åªæœ‰å˜é‡åï¼ŒåŒæ—¶å¦å¤–æœ‰ä¸€ä¸ª excel è¡¨æ ¼å‚¨å­˜ label å€¼ã€‚
å¦‚ä½•è¯»å– excel ç„¶åè‡ªåŠ¨æ‰¹é‡ç”Ÿæˆ `labal vars name` å‘¢ï¼Ÿ

```sql
* å¯¼å…¥ Excel å¯¹ç…§è¡¨
clear
import excel "åŸºæœ¬æƒ…å†µä»£ç å¯¹ç…§è¡¨.xlsx", ///
    sheet("æ”¯å‡º1ç¼–ç å¯¹ç…§") 

rename B varname 
rename A varlabel

* ç”Ÿæˆå‘½ä»¤ï¼ˆæ³¨æ„ compound quotesï¼‰
gen cmd = "label variable " + varname + " `" + `"""' + varlabel + `"""' + "'"

outsheet cmd using "å˜é‡æ ‡ç­¾å‘½ä»¤.txt", noquote replace
```

è¿™æ ·å°±ä¼šç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ï¼Œå‚¨å­˜æ‰€æœ‰ stata å¯¹åº”çš„æ ‡ç­¾åŒ–å‘½ä»¤ã€‚

> ä¸€åˆ‡ä»£ç åœ¨è‡ªåŠ¨åŒ–å’Œé€æ˜åº¦éƒ½æœ‰ä¸ªå–èˆï¼Œä¸ªäººæ¯”è¾ƒåå¥½è¿™ç§æœ‰ä¸­é—´äº§ç‰©çš„æµç¨‹ï¼Œä¾¿äºæ£€æŸ¥å’Œä¿®æ­£ã€‚
### æ­£åˆ™è¡¨è¾¾å¼æå–

#### å­—ç¬¦ç­›é€‰

ä»¥è£åˆ¤é•¿æ–‡æœ¬å˜é‡ä¸ºä¾‹å­ï¼Œ

æ˜æ˜å˜é‡åº”è¯¥æ˜¯ä¸€ä¸ªåå­—ï¼Œç»“æœå‡ºç°äº†ä¸€ä¸ªæ®µè½

> å¼ ä¸‰äººæ°‘å§”æ‰˜ã€å¼ ä¸‰äº 2014 å¹´è¿›è¡Œå®¡åˆ¤ã€å¼ ä¸‰äººæ°‘å®¡åˆ¤å‘˜è¿›è¡Œè£å†³ã€å¼ ä¸‰ä»£ç†è£åˆ¤é•¿è¿›è¡Œåº­å®¡ã€å¼ ä¸‰æå››ç†Ÿè®°å‘˜è¿›è¡Œå¼€åº­......

![å†ä¸¾ä¾‹å­â€”â€”æ³•é™¢å˜é‡](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113210847789.webp)
æ¢è€Œè¨€ä¹‹ï¼Œæ­£ç¡®çš„æ–‡æœ¬å­—ç¬¦åº”è¯¥æ˜¯ç‰¹å®šè¯æ±‡çš„å‰é¢å‡ ä¸ªå­—ã€‚

å…ˆä½¿ç”¨å­—ç¬¦é•¿åº¦ç­›é€‰ä¾‹å­ï¼ŒæŸ¥çœ‹æ–‡æœ¬æ ¼å¼ï¼š

```stata
Â *åˆ—å‡ºå­—ç¬¦é•¿åº¦å¤§äº14çš„å˜é‡è¡Œ  
Â list var if ustrlen(judge_fin) >= 14
```

ç„¶åæ ¹æ®æƒ…å†µ**åªä¿ç•™ç‰¹å®šå­—ç¬¦**ã€‚

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #a94442; background-color: #f2dede; border-color: #ebccd1;">
&#x26D4<b> è­¦å‘Šï¼šæ³¨æ„é¡ºåºï¼</b>
</div>

> ä¾‹å¦‚æœ‰ä»¥ä¸‹ç»„åˆ
> 
> å¼ ä¸‰**äº** 2014 å¹´è¿›è¡Œåº­å®¡
> 
> å¼ ä¸‰**ä»£ç†**å®¡åˆ¤é•¿**äº** 2014 å¹´è¿›è¡Œåº­å®¡
> 
> åœ¨è¯¥å˜é‡åŒ…å«"åº­å®¡"ä¸¤ä¸ªå­—æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å…ˆéƒ½ä¿ç•™â€œäºâ€ä»¥å‰çš„å­—ç¬¦ï¼Œå†ä¿ç•™"ä»£ç†"ä»¥å‰çš„å­—ç¬¦ã€‚
> 
> æ¡ä»¶ç­›é€‰æœ€å¥½æ‰¾ä¸¤ä¸ªå­—çš„ç‰¹å¾è¯ï¼Œè¿™æ ·å«â€œäºåº­å®¡â€çš„æ³•å®˜åå­—å‡ ä¹ä¸å¯èƒ½å‡ºç°ã€‚

ä¸ªäººä½¿ç”¨çš„é¡ºåºï¼Œä»…ä¾›å‚è€ƒï¼š

```stata
Â replace judge_fin = ustrregexra(judge_fin, "äººæ°‘é™ªå®¡å‘˜.*", "") if  strpos(judge_fin, "äººæ°‘é™ªå®¡å‘˜") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "å®¡åˆ¤å‘˜.*", "") if  strpos(judge_fin, "å®¡åˆ¤å‘˜") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "ç‹¬ä»».*", "") if  strpos(judge_fin, "ç‹¬ä»»") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "é™ªå®¡å‘˜.*", "") if  strpos(judge_fin, "é™ªå®¡å‘˜") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "ä»£ç†.*", "") if  strpos(judge_fin, "ä»£ç†") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "ä¹¦è®°å‘˜.*", "") if  strpos(judge_fin, "ä¹¦è®°å‘˜") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "åŠ©ç†.*", "") if  strpos(judge_fin, "åŠ©ç†") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "å®¡åˆ¤é•¿.*", "") if  strpos(judge_fin, "å®¡åˆ¤é•¿") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "äººæ°‘.*", "") if  strpos(judge_fin, "äººæ°‘") > 0  
Â replace judge_fin = ustrregexra(judge_fin, "äºŒ.*", "") if  strpos(judge_fin, "å¹´") > 0   
Â replace judge_fin = ustrregexra(judge_fin, "é€‚ç”¨.*", "") if  strpos(judge_fin, "é€‚ç”¨") > 0   
Â replace judge_fin = ustrregexra(judge_fin, "äº.*", "") if  strpos(judge_fin, "å¼€åº­") > 0   
Â replace judge_fin = ustrregexra(judge_fin, "å¯¹.*", "") if  strpos(judge_fin, "å®¡ç†") > 0 
```

ç”±äºçˆ¬è™«æœ‰æ—¶å€™é‡ä¸Šç½‘ç«™åŠ å¯†ï¼Œçˆ¬è™«ç»“æœæ˜¯ `é™ˆ*`,ï¼Œå®Œæˆä»¥ä¸Šæ¸…æ´—ååªå‰©ä¸‹ `é™ˆ`ï¼Œåˆ é™¤å•ä¸ªå­—ç¬¦è¡Œçš„å‡½æ•°å‘½ä»¤æ˜¯ï¼š

```stata
Â *åˆ é™¤åªæœ‰ä¸€ä¸ªå­—çš„ï¼Œä¾‹å¦‚æœ¬æ¥æ˜¯*é™ˆ  
Â drop if ustrregexm(judge_fin, "^[\p{Han}]$")
```

#### æ•°å­—æå–

ä¾‹å¦‚æ•°æ®æ˜¾ç¤º `2.34435å…ƒ`ï¼ˆéœ€è¦æ˜¯å­—ç¬¦ä¸²å˜é‡ï¼‰ï¼Œæˆ‘ä»¬åªæƒ³å•ç‹¬æå–æ•°å­—ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚

```stata
Â gen num_var = .  
Â replace num_var = real(regexs(1)) if regexm(str_var, "([0-9]+\.[0-9]+|[0-9]+)")
```

é¡ºä¾¿ä¸€æï¼Œå¦‚æœæ²¡æœ‰ `real(regexs(1))` è¿™ä¸ªå‚æ•°åˆ™è¡¨ç¤ºåˆ¤æ–­ï¼Œåªä¼šè¿”å› 0 å’Œ 1ï¼Œä»£è¡¨ä¸ç¬¦åˆæˆ–è€…ç¬¦åˆæ ¼å¼ã€‚

```stata
Â replace num_var =  regexm(str_var, "([0-9]+\.[0-9]+|[0-9]+)")
```

## dta æ—¶é—´ä¿®æ­£

åœ¨å¸ˆé—¨çš„æ‹›æŠ•æ ‡ä¸­ï¼Œå¾ˆå¤šæ—¶é—´æ ¼å¼æ˜¯æ··ä¹±çš„ã€‚

ä¾‹å¦‚æ ‡å‡†çš„ä¸¤ç§ï¼š
`2019-12-31 09:00:00`ã€
` 2019/12/30 09:30`

ä½†æ˜¯ä¹Ÿä¼šå‡ºç° ` 20220516 09:30:00 `ã€` 2022051609:30:00 `ã€` 2022051609:30 `

ä¾‹å¦‚ç¼ºå°‘ç§’ã€æœˆä»½ä½ç½®å†™æˆ 4 è€Œä¸æ˜¯ 04ã€‚

![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250123143246038.webp)

ä¸‹é¢çš„ä»£ç é›†ä¸­å¤„ç†äº†è¿™å¥—æ—¶é—´ç¼–ç ã€‚

ä¸€äº›ä¸­æ–‡ç¬¦å·è½¬è‹±æ–‡çš„é¢„å¤„ç†ã€‚

åœ¨æ—¥æœŸå’Œæ—¶é—´ä¹‹é—´æ²¡æœ‰ç©ºæ ¼æ—¶ï¼ˆä¾‹å¦‚ `2022051609:30:00`ï¼‰ï¼Œå¦‚æœèŒƒå›´åœ¨å¯¹åº”å¹´æœˆå†…ï¼ˆæ¯”å¦‚ 0-31ï¼‰åˆ™ä¼˜å…ˆå–ä¸¤ä½æ•°ï¼Œè¶…è¿‡èŒƒå›´äº†åˆ™å–å‰é¢çš„ä¸ªä½æ•°ã€‚

æœ€åå…¼å®¹å„ç§æ ¼å¼çš„æ—¶é—´æå–ã€‚

```
/*-----------------------------------------
  æ—¥æœŸæå–è½¬æ¢æ–¹æ¡ˆï¼ˆç´§å‡‘æ ¼å¼å¢å¼ºç‰ˆï¼‰
  ç‰ˆæœ¬ï¼š2.6
  æœ€åæ›´æ–°ï¼š2024-06-27
-----------------------------------------*/
version 17
clear all
set more off

use "F:\æ¡Œé¢\test.dta", clear
keep time

*===============================
*        é¢„å¤„ç†é˜¶æ®µï¼ˆå¢å¼ºï¼‰
*===============================
// åŸºç¡€æ¸…ç†
replace time = ustrtrim(time)
replace time = subinstr(time, char(92), "-", .)
replace time = subinstr(time, "ï¼š", ":", .) 
replace time = subinstr(time, " ", "", .)

*===============================
*      æ–°å¢ç´§å‡‘æ ¼å¼è¯†åˆ«æ¨¡å—
*===============================
// è¯†åˆ«YYYYMMDD-æ ¼å¼ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
gen str8 compact_date = ""
replace compact_date = substr(time, 1, 8) ///
    if ustrregexm(time, "^\d{8}-")  // ä¸¥æ ¼åŒ¹é…8ä½æ•°å­—+è¿å­—ç¬¦æ ¼å¼

// åˆ†ç¦»ç´§å‡‘æ ¼å¼æ•°æ®
preserve
keep if compact_date != ""
gen date_stata = date(compact_date, "YMD")
format date_stata %tdCCYY-NN-DD
tempfile compact
save `compact'
restore

// å¤„ç†éç´§å‡‘æ ¼å¼æ•°æ®
keep if compact_date == ""
drop compact_date

*===============================
*      åŸæœ‰å¤„ç†æµç¨‹ï¼ˆä¼˜åŒ–ï¼‰
*===============================
// ç»Ÿä¸€åˆ†éš”ç¬¦å¤„ç†
foreach s in "." "_" "/" {
    replace time = subinstr(time, "`s'", "-", .)
}

// æ—¥æœŸéƒ¨åˆ†æå–
gen str50 date_part = ustrregexs(1) if ustrregexm(time, "(\d{4}-\d{1,2}-\d{2,})[^\d]")
replace date_part = time if missing(date_part)

// æ™ºèƒ½æ—¥æœŸå¤„ç†
gen str4 year = ustrregexs(1) if ustrregexm(date_part, "^(\d{4})-\d+")
gen str2 month = ustrregexs(1) if ustrregexm(date_part, "-(\d{1,2})-")
gen str5 raw_day = ustrregexs(1) if ustrregexm(date_part, "-(\d+)$")

replace month = cond(strlen(month) == 1, "0" + month, month)
replace month = "12" if real(month) > 12

gen str2 day = substr(raw_day,1,2)
replace day = substr(day,1,1) if real(day) > 31
replace day = "31" if real(day) > 31
replace day = "0" + day if real(day) < 10 & strlen(day) == 1

replace date_part = year + "-" + month + "-" + day
drop year month raw_day day

gen date_stata = date(date_part, "YMD")
replace date_stata = date(ustrregexra(time, "[^0-9]", ""), "YMD") if missing(date_stata)

*===============================
*      åˆå¹¶å¤„ç†ç»“æœ
*===============================
append using `compact'

*===============================
*      åå¤„ç†ä¸éªŒè¯
*===============================
format date_stata %tdCCYY-NN-DD


```



### æé†’

å»ºè®®å…ˆå°† stata æ•°æ®å¯¼å‡ºä¸º csv æ ¼å¼ã€‚

æœ€å¤§çš„é—®é¢˜æ˜¯**ä¹±ç **ã€‚


![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250113211041112.webp)
csv ä¹±ç äº†ã€‚å› ä¸ºæå–çš„ txt ä¸æ˜¯ utf 8 æ ¼å¼ã€‚

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
&#x1F628<b> æ³¨æ„ï¼šä¸€å®šè¦ä¿è¯CSV æ–‡ä»¶é»˜è®¤æ ¼å¼åˆ· UTF-8 ï¼Œç„¶åè¿›è¡Œå¤„ç†ã€‚</b>
</div>

## Python ä¾‹å­ ï¼š æå–ä¸Šè¯‰æ³•å®˜è¿‡å»å’Œç°åœ¨çš„ç‰¹å¾å‡å€¼

ä¾‹å­ 3 æ˜¯é€è¡ŒåŒ¹é…ï¼Œä½†æ˜¯è¿™æ ·å¤„ç†å®åœ¨æ˜¯å¤ªæ…¢äº†ã€‚

ç°åœ¨æˆ‘æƒ³è¦è¿™æ ·å¤„ç†ï¼š

æå–æ³•å®˜ç»å†ä¸Šè¯‰å‰çš„ 20 ä¸ªæ¡ˆå­ï¼Œç„¶åæ±‚ç½šæ¬¾å‡å€¼ï¼›åŒæ—¶æå–æ³•å®˜ç»å†ä¸Šè¯‰åçš„ 20 ä¸ªæ¡ˆå­ï¼Œç„¶åæ±‚ç½šæ¬¾çš„å‡å€¼ã€‚

ä¸ªäººè®¾è®¡çš„ç®—æ³•é€»è¾‘å¦‚ä¸‹ï¼š

- æŒ‰ç…§æ³•é™¢-æ³•å®˜çš„ç»„åˆå¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ã€‚
- æ¯ä¸ªæ•°æ®éƒ½æœ‰åˆå®¡åˆ°ç»ˆå®¡çš„æ—¶é—´ï¼Œå½¢æˆæ—¶é—´æˆ³ï¼Œå¹¶ä¸”æ¯ä¸€è¡Œåº”è¯¥è¯†åˆ«ä¸€ç³»åˆ—æ—¶é—´ä¸­çš„æœ€æ—©æ—¶é—´å’Œæœ€æ™šæ—¶é—´ã€‚
- å½“æœ€æ—©æ—¶é—´ä¸ç­‰äºæœ€æ™šæ—¶é—´æ—¶ï¼Œè¿™æ¡æ•°æ®å°±æ˜¯ä¸»å¹²æ•°æ®ã€‚
- åœ¨æ¯ä¸ªç»„åˆ«å†…ï¼Œåªä¿ç•™æ»¡è¶³æ¡ä»¶çš„æ ·æœ¬ã€‚
- åœ¨ä¸»å¹²æ•°æ®å‘¨å›´ï¼Œå½“åœ¨ä¸»å¹²æ•°æ®æœ€æ—©æ•°æ®ä¹‹å‰è¿˜æœ‰ 10 ä¸ªæ¡ˆå­ï¼Œåœ¨ä¸»å¹²æ•°æ®ä¹‹åä¹Ÿè¿˜æœ‰ 10 ä¸ªæ¡ˆå­ï¼Œæœ€ç»ˆåªä¿ç•™è¿™ 21 æ¡æ•°æ®ã€‚
- å¦‚æœä¸€ä¸ªç»„åˆ«åŒæ—¶æœ‰å¤šä¸ªæ¡ˆå­ï¼Œåˆ™éƒ½ä¿ç•™æ»¡è¶³è¦æ±‚çš„æ ·æœ¬ã€‚
- ç»™æ»¡è¶³å‘¨å›´å­˜åœ¨å¯¹åº”æ•°é‡æ¡ˆå­çš„ä¸»å¹²æ•°æ®ç”Ÿæˆå˜é‡ aï¼Œæ ‡è¯†ä¸º 1ã€‚
- ç”Ÿæˆä¸Šè¯‰å‰å˜é‡å’Œä¸Šè¯‰åå˜é‡ï¼Œæ±‚æ•°é‡å¯¹åº”æ¡ˆå­çš„å‡å€¼ã€‚

{{< admonition type=note  title="åˆ†è§£ä»£ç " open=false >}}
ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ aiï¼Œå®é™…ä¸Šè¿™ä¸ªç®—æ³•æˆ‘æ˜¯æ‹†æˆäº†ä¸¤æ­¥ã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯ç­›é€‰æ»¡è¶³è¦æ±‚çš„æ‰€æœ‰æ ·æœ¬ï¼›ç¬¬äºŒéƒ¨æ˜¯åœ¨æ»¡è¶³è¦æ±‚çš„ä¸»å¹²æ•°æ®ï¼ˆa=1ï¼‰å¤„è®¡ç®—å‰åæ¡ˆä»¶çš„å‡å€¼ã€‚æœ€å‘¨å†æ•´åˆä¸¤éƒ¨åˆ†çš„ä»£ç ã€‚
{{< /admonition >}}

æˆ‘æœ€ç»ˆå¤šè®¾ç½®äº†ä¸€ä¸ªå‚æ•°ï¼Œå…³äºä¸»å¹²æ¡ˆä»¶å‘¨å›´éœ€è¦æœ‰å¤šå°‘æ¡ˆå­æ‰ä¿ç•™ã€‚

```python
import pandas as pd
from tqdm import tqdm
import numpy as np

# å‚æ•°é…ç½®
input_path = r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æœ€ç»ˆé¢æ¿æ•°æ®\å»æç«¯å€¼äº¤é€šæ°‘äº‹ä¸Šè¯‰æ ·æœ¬.csv'
output_path = r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æœ€ç»ˆé¢æ¿æ•°æ®\å»æç«¯å€¼äº¤é€šæ°‘äº‹ä¸Šè¯‰æ ·æœ¬_è®ºæ–‡å»0_45_æ°‘è¯‰æ ·æœ¬.csv'
before_n = 45  # ä¿ç•™ä¹‹å‰è®°å½•çš„æ¡æ•°
after_n = 45   # ä¿ç•™ä¹‹åè®°å½•çš„æ¡æ•°

# æ—¶é—´å­—æ®µé…ç½®
time_columns = [
    'jud_year', 'jud_month',
    'm1_jud_year', 'm1_jud_month',
    'm2_jud_year', 'm2_jud_month',
    'm3_jud_year', 'm3_jud_month',
    'm4_jud_year', 'm4_jud_month',
]

def extract_timestamps(row):
    """æå–æ¯è¡Œçš„æ—¶é—´æˆ³ï¼ˆæœ€æ—©å’Œæœ€æ™šæ—¶é—´ï¼‰"""
    timestamps = []
    for i in range(0, len(time_columns), 2):
        year = row[time_columns[i]]
        month = row[time_columns[i + 1]]
        if pd.notna(year) and pd.notna(month):
            try:
                timestamps.append((int(year), int(month)))
            except:
                continue
    return min(timestamps, default=None), max(timestamps, default=None)

def is_main_case(row):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»å¹²æ•°æ®"""
    conditions = [
        (pd.notna(row['m1_judge_am']) & pd.notna(row['m2_judge_am'])),
        (pd.notna(row['m1_judge_am']) & pd.notna(row['m3_judge_am'])),
        (pd.notna(row['m1_judge_am']) & pd.notna(row['m4_judge_am'])),
        (pd.notna(row['m2_judge_am']) & pd.notna(row['m3_judge_am'])),
        (pd.notna(row['m2_judge_am']) & pd.notna(row['m4_judge_am'])),
        (pd.notna(row['m3_judge_am']) & pd.notna(row['m4_judge_am'])),
    ]
    return any(conditions)

def filter_valid_judge(data):
    """è¿‡æ»¤æœ‰æ•ˆé‡‘é¢ï¼ˆé0éç©ºï¼‰"""
    return data[(data['judge_am'] != 0) & (data['judge_am'].notna())]

def calculate_means(group, main_case_idx):
    """
    è®¡ç®—ä¸»å¹²æ•°æ®çš„ judge_sum, bf_judge_sum å’Œ af_judge_sumï¼ˆå‡æ’é™¤0å€¼å’Œç©ºå€¼ï¼‰
    """
    min_time, max_time = extract_timestamps(group.loc[main_case_idx])

    if min_time is None or max_time is None:
        return group

    # è®¡ç®—judge_sum
    judge_data = group[group.index != main_case_idx]
    valid_judge = filter_valid_judge(judge_data)
    group.loc[main_case_idx, 'judge_sum'] = valid_judge['judge_am'].mean() if not valid_judge.empty else np.nan

    # è®¡ç®—bf_judge_sum
    bf_cond = (
        (group['jud_year'] < min_time[0]) |
        ((group['jud_year'] == min_time[0]) & (group['jud_month'] < min_time[1]))
    ) & (group.index != main_case_idx)
    valid_bf = filter_valid_judge(group[bf_cond])
    group.loc[main_case_idx, 'bf_judge_sum'] = valid_bf['judge_am'].mean() if not valid_bf.empty else np.nan

    # è®¡ç®—af_judge_sum
    af_cond = (
        (group['jud_year'] > max_time[0]) |
        ((group['jud_year'] == max_time[0]) & (group['jud_month'] > max_time[1]))
    ) & (group.index != main_case_idx)
    valid_af = filter_valid_judge(group[af_cond])
    group.loc[main_case_idx, 'af_judge_sum'] = valid_af['judge_am'].mean() if not valid_af.empty else np.nan

    return group

def process_group(group):
    """å¤„ç†å•ä¸ªåˆ†ç»„å¹¶ç”Ÿæˆç‰¹å¾"""
    # åˆ›å»ºæ–°åˆ—çš„ DataFrame
    new_columns = {}
    for i in range(1, before_n + 1):
        new_columns[f'bf_{i}_judge_am'] = np.nan
        new_columns[f'bf_{i}_count_tiao'] = np.nan
        new_columns[f'bf_{i}_total_chars'] = np.nan
        new_columns[f'bf_{i}_clause_count'] = np.nan
    for j in range(1, after_n + 1):
        new_columns[f'af_{j}_judge_am'] = np.nan
        new_columns[f'af_{j}_count_tiao'] = np.nan
        new_columns[f'af_{j}_total_chars'] = np.nan
        new_columns[f'af_{j}_clause_count'] = np.nan
    new_columns.update({
        'a': 0,
        'judge_sum': np.nan,
        'bf_judge_sum': np.nan,
        'af_judge_sum': np.nan
    })

    # å°†æ–°åˆ—ä¸€æ¬¡æ€§æ·»åŠ åˆ° group ä¸­
    new_columns_df = pd.DataFrame(new_columns, index=group.index)
    group = pd.concat([group, new_columns_df], axis=1)

    # ç­›é€‰ä¸»å¹²æ•°æ®
    main_cases = group[group.apply(is_main_case, axis=1)]
    if main_cases.empty:
        return group

    for idx, row in main_cases.iterrows():
        min_time, max_time = extract_timestamps(row)
        if not min_time or not max_time:
            continue

        group.loc[idx, 'a'] = 1
        group = calculate_means(group, idx)

        # è·å–æœ‰æ•ˆä¸Šä¸‹æ–‡æ•°æ®
        before_data = filter_valid_judge(group[
            (group['jud_year'] < min_time[0]) |
            ((group['jud_year'] == min_time[0]) & (group['jud_month'] < min_time[1]))
        ]).sort_values(['jud_year', 'jud_month'], ascending=[True, True])

        after_data = filter_valid_judge(group[
            (group['jud_year'] > max_time[0]) |
            ((group['jud_year'] == max_time[0]) & (group['jud_month'] > max_time[1]))
        ]).sort_values(['jud_year', 'jud_month'], ascending=[True, True])

        # å¡«å……å‰å‘ç‰¹å¾
        for i in range(1, min(before_n + 1, len(before_data) + 1)):
            row_data = before_data.iloc[-i]
            group.loc[idx, f'bf_{i}_judge_am'] = row_data['judge_am']
            group.loc[idx, f'bf_{i}_count_tiao'] = row_data['count_tiao']
            group.loc[idx, f'bf_{i}_total_chars'] = row_data['total_chars']
            group.loc[idx, f'bf_{i}_clause_count'] = row_data['clause_count']

        # å¡«å……åå‘ç‰¹å¾
        for j in range(1, min(after_n + 1, len(after_data) + 1)):
            row_data = after_data.iloc[j - 1]
            group.loc[idx, f'af_{j}_judge_am'] = row_data['judge_am']
            group.loc[idx, f'af_{j}_count_tiao'] = row_data['count_tiao']
            group.loc[idx, f'af_{j}_total_chars'] = row_data['total_chars']
            group.loc[idx, f'af_{j}_clause_count'] = row_data['clause_count']

    return group

if __name__ == "__main__":
    try:
        with open(input_path, 'r', encoding='utf-8', errors='replace') as file:
            df = pd.read_csv(file)
        print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼š{len(df):,}æ¡")
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}")
        exit()

    final_dfs = []
    grouped = df.groupby(['court_name', 'judge_fin'])

    with tqdm(total=len(grouped), desc="å¤„ç†è¿›åº¦") as pbar:
        for (court, judge), group in grouped:
            final_dfs.append(process_group(group))
            pbar.update(1)

    final_df = pd.concat(final_dfs)
    final_df.to_csv(output_path, index=False, encoding='utf-8')

    # ç»“æœéªŒè¯
    main_data = final_df[final_df['a'] == 1]
    print(f"\nç”Ÿæˆä¸»å¹²æ•°æ®ï¼š{len(main_data):,}æ¡")
    print("å­—æ®µéªŒè¯ï¼ˆåº”æ— 0å€¼å’Œç©ºå€¼ï¼‰ï¼š")
    print(main_data[['judge_sum', 'bf_judge_sum', 'af_judge_sum']].describe())
```

> ä»¥ä¸Šä»£ç å¦‚æœä¸æ˜¯åœ¨ jupyter ç¯å¢ƒä¸­è¿è¡Œè€Œæ˜¯åœ¨ pycharm ç¯å¢ƒä¸­è¿è¡Œï¼Œéœ€è¦è°ƒæ•´ä¸‹è¾“å…¥å‚æ•°çš„ä½ç½®ã€‚

## ä¸€äº›å¯ä»¥å¿«é€Ÿå¾—åˆ°çš„è£åˆ¤æ–‡ä¹¦å˜é‡

- å·¥ä½œç»éªŒï¼šç¬¬ä¸€æ¬¡å‡ºç°çš„æ—¶é—´å’Œæœ€åä¸€æ¬¡å‡ºç°çš„æ—¶é—´æ®µ
- å…±åŒæ¡ˆä»¶ï¼šåœ¨æ ·æœ¬ä¸­ä¸€å…±å¤„ç†äº†å¤šå°‘æ¡ˆä»¶
- ç´¯è®¡æ¡ˆä»¶ï¼šè¿™æ˜¯ç›®å‰å¤„ç†çš„ç¬¬å‡ èµ·æ¡ˆä»¶ã€‚ç”±äºæ ·æœ¬æ˜¯ä»¥æœˆä¸ºå•ä½ï¼Œç´¯è®¡è®¡æ•°åº”å½“æ˜¯1ã€1ã€3ã€3ã€5ã€6......

```stata

* ç”Ÿæˆå”¯ä¸€è¡Œå· id
gen id = _n

* ç¡®ä¿æ•°æ®æŒ‰æ³•é™¢ã€æ³•å®˜ã€å¹´ä»½ã€æœˆä»½ã€æ¡ˆä»¶é¡ºåºæ’åº
sort court_name judge_fin jud_year jud_month id

* åˆ›å»ºæ¡ˆä»¶è®¡æ•°å˜é‡ï¼ˆæ¯ä¸€è¡Œé»˜è®¤è®¡æ•°ä¸º 1ï¼‰
gen case_count = 1

* æŒ‰æ³•é™¢å’Œæ³•å®˜åˆ†ç»„ï¼Œå¹¶å¯¹æ¡ˆä»¶è®¡æ•°è¿›è¡Œç´¯è®¡
bysort court_name judge_fin (jud_year jud_month): gen cumulative_count = sum(case_count)

* ä¿®æ­£ä¸ºåŒæœˆç´¯è®¡æ•°ç›¸åŒ
bysort court_name judge_fin jud_year jud_month (cumulative_count): replace cumulative_count = cumulative_count[_N]

* è½¬æ¢å¹´ä»½å’Œæœˆä»½ä¸ºå•ä¸€æ—¶é—´ç‚¹ï¼ˆä»¥æœˆä»½ä¸ºå•ä½ï¼‰
gen time = jud_year * 12 + jud_month

* è®¡ç®—é¦–æ¬¡æ—¶é—´ç‚¹ï¼ˆæ¯ä¸ªæ³•å®˜åœ¨æŸæ³•é™¢å¤„ç†ç¬¬ä¸€ä¸ªæ¡ˆä»¶çš„æ—¶é—´ï¼‰
bysort court_name judge_fin (jud_year jud_month): gen first_time = time[1]

* è®¡ç®—å·¥ä½œç»éªŒï¼ˆå•ä½ï¼šæœˆä»½ï¼‰
gen work_experience_months = time - first_time + 1

* åˆ é™¤ä¸éœ€è¦çš„ä¸´æ—¶å˜é‡
drop id case_count time first_time

egen combo_count = count(court_name + judge_fin), by(court_name judge_fin)
```

## Python ä¾‹å­ ï¼šæ¡ˆä»¶ç§¯å‹é‡

åœ¨è£åˆ¤æ–‡ä¹¦ä¸­æœ‰ä¸¤ä¸ªæ—¶é—´ï¼Œæ¡ˆä»¶åˆ¤å†³æ—¶é—´( jud å¼€å¤´)å’Œæ¡ˆä»¶ç«‹æ¡ˆ( acc å¼€å¤´)æ—¶é—´ã€‚

æ¡ˆä»¶ç§¯å‹é‡æ˜¯æŒ‡ç«‹æ¡ˆæ—¶é—´æ—©äºå½“å‰å¹´æœˆï¼Œåˆ¤å†³å¹´é¥­æ™šäºå½“å‰å¹´ä»½çš„æ‰€æœ‰æ¡ˆä»¶ã€‚è¿™ç§æ¶‰åŠè·¨è¡Œå˜é‡è¯†åˆ«å¤„ç†çš„æ“ä½œå°±æ˜¯ stata çš„å¼±ç‚¹ã€‚

```python
import pandas as pd

import numpy as np

from tqdm import tqdm

import time

  
  

def calculate_backlog(df: pd.DataFrame) -> pd.DataFrame:

Â  Â  # æ„é€ æ—¶é—´å­—æ®µï¼ˆä»¥æœˆä»½ä¸ºå•ä½ï¼‰

Â  Â  df['acc_time'] = df['acc_year'] * 12 + df['acc_month']

Â  Â  df['jud_time'] = df['jud_year'] * 12 + df['jud_month']

  

Â  Â  # åˆå§‹åŒ– backlog åˆ—

Â  Â  df['backlog'] = 0

  

Â  Â  # æŒ‰æ³•é™¢+æ³•å®˜åˆ†ç»„å¤„ç†

Â  Â  grouped = df.groupby(['court_name', 'judge_fin'])

  

Â  Â  print(f"å…±æœ‰åˆ†ç»„æ•°é‡ï¼š{len(grouped)}")

Â  Â  start_time = time.time()

  

Â  Â  # éå†æ¯ç»„

Â  Â  for (court, judge), group_df in tqdm(grouped, desc="Processing court-judge groups"):

Â  Â  Â  Â  idx = group_df.index

Â  Â  Â  Â  jud_times = group_df['jud_time'].to_numpy()

Â  Â  Â  Â  acc_times = group_df['acc_time'].to_numpy()

  

Â  Â  Â  Â  # åˆå§‹åŒ– backlog ç»“æœ

Â  Â  Â  Â  backlog_counts = np.zeros(len(group_df), dtype=int)

  

Â  Â  Â  Â  for i in range(len(group_df)):

Â  Â  Â  Â  Â  Â  jud_time_i = jud_times[i]

Â  Â  Â  Â  Â  Â  # ç§¯å‹æ¡ä»¶ï¼šç«‹æ¡ˆæ—¶é—´ < å½“å‰åˆ¤å†³æ—¶é—´ï¼Œä¸” å…¶ä»–æ¡ˆä»¶çš„åˆ¤å†³æ—¶é—´ > å½“å‰åˆ¤å†³æ—¶é—´

Â  Â  Â  Â  Â  Â  backlog_mask = (acc_times < jud_time_i) & (jud_times > jud_time_i)

Â  Â  Â  Â  Â  Â  backlog_counts[i] = backlog_mask.sum()

  

Â  Â  Â  Â  # æ›´æ–°ç»“æœ

Â  Â  Â  Â  df.loc[idx, 'backlog'] = backlog_counts

  

Â  Â  elapsed = time.time() - start_time

Â  Â  print(f"\nå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{elapsed:.2f} ç§’")

Â  Â  return df

  
  

def main(input_path, output_path):

Â  Â  print("è¯»å–æ•°æ®ä¸­...")

Â  Â  df = pd.read_csv(input_path, encoding='utf-8', encoding_errors='replace')

Â  Â  print(f"æ•°æ®è¯»å–å®Œæˆï¼Œå…±æœ‰ {len(df)} æ¡è®°å½•ã€‚")

  

Â  Â  df_result = calculate_backlog(df)

  

Â  Â  print(f"ä¿å­˜ç»“æœåˆ°ï¼š{output_path}")

Â  Â  df_result.to_csv(output_path, index=False, encoding='utf-8-sig')

Â  Â  print("ä¿å­˜å®Œæˆã€‚")

  
  

if __name__ == "__main__":

Â  Â  # è¯·æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹è¾“å…¥è¾“å‡ºè·¯å¾„

Â  Â  input_path = r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æ¡ˆä»¶ç§¯å‹\æ¡ˆä»¶ç§¯å‹è®¡ç®—.csv'

Â  Â  output_path = r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æ¡ˆä»¶ç§¯å‹\è®ºæ–‡_æ¡ˆä»¶ç§¯å‹è®¡ç®—.csv'

  

Â  Â  main(input_path, output_path)
```

## Stata ä¾‹å­ ï¼šæ³•é™¢-å¹´ä»½æ¡ˆä»¶å…¬å¼€ç‡

```sql
**********æ€è·¯*************
/*
ç”±äºæ¯å¹´æ¡ˆä»¶ç¼–å·ä¸º
æ°‘åˆï¼ˆ2020ï¼‰**å·ï¼Œ**å·æŒ‰ç…§é¡ºåºç¼–å·ã€‚
æœ‰å¯èƒ½æ ·æœ¬åªæœ‰1ã€3ã€5ã€7å·ï¼Œ
é€šè¿‡æå¤§ä¼¼ç„¶æ³•ä¼°è®¡äº§ç”Ÿçš„â€œå¦å…‹å…¬å¼â€ä¼°è®¡å…¬å¼€ç‡ï¼ˆäºŒæˆ˜å¦å…‹ç¼´è·ç‡ï¼‰
æœ€ç»ˆè®¡ç®—æ¯å¹´æ¯ä¸ªæ³•é™¢å¯¹åº”çš„å…¬å¼€ç‡
è€ƒè™‘å»æ‰å…¬å¼€ç‡ä½çš„æ³•é™¢ï¼ˆç­‰åŒäºå»æ‰å…¬å¼€ç‡ä½çš„åœ°åŒºï¼‰
*/
**********å˜é‡è¯´æ˜*************
/*
case_num_strï¼šè£åˆ¤æ–‡ä¹¦æ¡ˆå·
court_nameï¼šæ³•é™¢åç§°
jud_yearï¼šåˆ¤å†³å¹´ä»½
public_rateï¼šå…¬å¼€ç‡
*/
**********å‚è€ƒæ–‡çŒ®*************
/*
1å…¬å¼çš„æå‡ºï¼Œè¿˜æœ‰å…¶ä»–ä¼°è®¡å…¬å¼€ç‡çš„æ–¹æ³•
Augmenting serialized bureaucratic data: the case of Chinese courtsï¼ˆ2022ï¼Œå·¥ä½œè®ºæ–‡ï¼‰

2åº”ç”¨ä¾‹å­
Suing the government under weak rule of law: Evidence from administrative litigation reform in Chinaï¼ˆ2023ï¼Œjpubeï¼‰
*/

gen case_num_str = ustrregexs(1) if ustrregexm(case_wid_str, "æ°‘[^0-9]*([0-9]+)")
replace case_num_str = ustrregexra(case_num_str, "^0+", "")
destring case_num_str, gen(case_num)
drop case_num_str
order case_wid_str case_num


bysort court_name jud_year: egen max_case_num = max(case_num)

bysort court_name jud_year: gen case_count = _N

order max_case_N 


bysort court_name jud_year: egen max_case_N = max(case_count)
gen case_count = max_case_num
gen public_rate = max_case_N / (max_case_num * (1 + 1/max_case_N) - 1)
sum public_rate

keep court_name jud_year public_rate

duplicates drop court_name jud_year, force

duplicates report  court_name jud_year
```

## Python ä¾‹å­ï¼šå¤§å‹æ•°æ®çš„åŒ¹é…

ç°åœ¨æ¯”è¾ƒæµè¡Œå·¥å•†æ³¨å†Œæ•°æ®å’Œå…¶ä»–ä¼ä¸šæ•°æ®åº“çš„åŒ¹é…ï¼Œå‡ ä¹å°±æ˜¯ 5000 ä¸‡æ¡æœ‰æ•ˆæ•°æ®èµ·æ­¥ã€‚

Stata çš„ `merge` å‘½ä»¤åœ¨åŒ¹é…æ—¶éå¸¸æ¶ˆè€—å†…å­˜ï¼Œå› ä¸ºå…¶ä¸´æ—¶å‰¯æœ¬çš„åŸå› ä¼šè½»æ¾åœ¨å†…å­˜æŸè€—ä¸Šç¿»å€ã€‚å› æ­¤ä¸ªäººæ¨èéƒ½å¯¼å‡º `.csv` æ–‡ä»¶ä½¿ç”¨ python çš„ `duckdb`ï¼ˆ`pandas` åŒ¹é…åªæ˜¯æ¯” stata `merge` ç•¥å¾®å¥½ä¸€ç‚¹è€Œå·²ã€‚ï¼‰

ä¸‹é¢è¿™ä¸ªä¾‹å­å°±æ˜¯ `duckdb` çš„ä¾‹å­ï¼Œä»£ç ä½œç”¨å’Œ stata çš„ `merge m:1` æ•ˆæœç›¸åŒï¼Œä½†æ›´ä¼˜åŒ–å†…å­˜ï¼š

```python
import duckdb
import os

con.execute("DROP TABLE IF EXISTS using_tbl")
con.execute("DROP TABLE IF EXISTS main_tbl")
con.execute("DROP TABLE IF EXISTS merged")

# =====================
# è·¯å¾„ï¼ˆæŒ‰ä½ è‡ªå·±å®é™…è·¯å¾„æ”¹ï¼‰
# =====================

MAIN_CSV = r"C:\Users\PC\Desktop\ä¸­å›½æ‹›æŠ•æ ‡\è®ºæ–‡ä»£ç \å›å½’æ•°æ®.csv"
USING_CSV = r"C:\Users\PC\Desktop\ä¸­å›½æ‹›æŠ•æ ‡\è®ºæ–‡ä»£ç \æ³¨å†Œæ•°æ®.csv"
OUT_CSV Â = r"C:\Users\PC\Desktop\ä¸­å›½æ‹›æŠ•æ ‡\è®ºæ–‡ä»£ç \merged.csv"
DB_FILE Â = r"C:\Users\PC\Desktop\ä¸­å›½æ‹›æŠ•æ ‡\è®ºæ–‡ä»£ç \merge.duckdb"

# =====================
# è¿æ¥ DuckDBï¼ˆè½ç›˜ï¼Œé˜² OOMï¼‰
# =====================

con = duckdb.connect(DB_FILE)

# ï¼ˆå¯é€‰ï¼‰é™åˆ¶å†…å­˜ï¼Œæ›´ç¨³
con.execute("PRAGMA memory_limit='16GB'")
con.execute("PRAGMA threads=8")
print("1ï¸âƒ£ è¯»å– using è¡¨ï¼ˆ1.8 äº¿ï¼‰")
con.execute(f"""
CREATE TABLE using_tbl AS
SELECT *
FROM read_csv_auto(
Â  Â  '{USING_CSV}',
Â  Â  SAMPLE_SIZE=1000000,
Â  Â  parallel=true
)
""")

print("2ï¸âƒ£ è¯»å– main è¡¨ï¼ˆ2000 ä¸‡ï¼‰")
con.execute(f"""
CREATE TABLE main_tbl AS
SELECT *
FROM read_csv_auto(
Â  Â  '{MAIN_CSV}',
Â  Â  SAMPLE_SIZE=500000,
Â  Â  parallel=true
)
""")
  
print("3ï¸âƒ£ LEFT JOINï¼ˆm:1ï¼‰")
con.execute("""
CREATE TABLE merged AS
SELECT
Â  Â  m.*,
Â  Â  u.*
FROM main_tbl m
LEFT JOIN using_tbl u
ON m.credit_code = u.credit_code
""")

print("4ï¸âƒ£ å¯¼å‡ºç»“æœ")
con.execute(f"""
COPY merged
TO '{OUT_CSV}'
(HEADER, DELIMITER ',')
""")

con.close()
print("âœ… å®Œæˆ")
```

## python ä¾‹å­ ï¼šæ¡ˆä»¶ç±»å‹çš„åˆ†ç±»

å‚è€ƒäº†è®ºæ–‡ã€Š[Women in the Courtroom: Technology and Justice](https://academic.oup.com/restud/advance-article/doi/10.1093/restud/rdaf066/8220859)ã€‹ã€‚è£åˆ¤æ–‡ä¹¦æ¡ˆä»¶ç±»å‹å¤ªå¤šæ€ä¹ˆåŠï¼Ÿé€šè¿‡LDAæ¨¡å‹èšç±»åˆ°50ä¸ªæ¡ˆä»¶ç±»å‹ã€‚

æ­¤å¤„ä»£ç æ˜¯å‡è®¾ä½ å·²ç»æœ‰äº†ä¸€ä¸ªæ¡ˆä»¶é¢æ¿ï¼ˆ`.csv`ï¼‰ æ¡ˆä»¶å·-æ¡ˆä»¶ç±»å‹åç§°ã€‚æ¥ä¸‹æ¥å¯ä»¥é€šè¿‡æœºå™¨å­¦ä¹ åˆ†ç±»ç®€åŒ–ä¸º 50 ä¸ªæ¡ˆä»¶ç±»å‹ã€‚

åº”ç”¨ä¾‹å­å¯ä»¥å‚è€ƒ [LDA ä¸»é¢˜æ¨¡å‹ä¸ Gephi å¯è§†åŒ–](https://blog.huaxiangshan.com/zh-cn/posts/lda_gephi/)

```python
import ast
import pandas as pd
import jieba
import logging
from gensim import corpora, models
from tqdm.auto import tqdm
import warnings
import os 
import re 

# -------------------------------
# 0. å…¨å±€è®¾ç½®
# -------------------------------
# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")
jieba.setLogLevel(logging.INFO)
# åˆå§‹åŒ– jiebaï¼Œé¿å…åœ¨è¿›åº¦æ¡ä¸­æ‰“å°æ—¥å¿—
jieba.initialize()

# -------------------------------
# ç”¨æˆ·å¯é…ç½®å‚æ•° (User Configurable Parameters)
# -------------------------------
# è®¾ç½®è¦ç”Ÿæˆçš„ä¸»é¢˜æ•°é‡ã€‚è¯·åœ¨æ­¤å¤„ä¿®æ”¹ä¸»é¢˜æ•°ï¼Œä¾‹å¦‚ 20, 50, 100 ç­‰ã€‚
NUM_TOPICS = 50 
# è®¾ç½®ç”¨äºè¿‡æ»¤ä½é¢‘è¯çš„æœ€å°å‡ºç°æ¬¡æ•°ã€‚ä¾‹å¦‚ï¼Œno_below=100 è¡¨ç¤ºåªä¿ç•™å‡ºç°æ¬¡æ•° >= 100 çš„è¯æ±‡ã€‚
MIN_WORD_COUNT = 100 
# LDA è®­ç»ƒçš„è¿­ä»£æ¬¡æ•° (Passes)
NUM_PASSES = 10

# -------------------------------
# æ–‡ä»¶è·¯å¾„è®¾ç½® (æ–‡ä»¶åä¸ºåŠ¨æ€ç”Ÿæˆï¼ŒåŒ…å«ä¸»é¢˜æ•°é‡)
# -------------------------------
input_path = r"/root/æ¡ˆä»¶ç±»å‹.csv"
output_path = r"/root/æ¡ˆä»¶ç±»å‹_{}ç±»åˆ«ç»“æœ.csv".format(NUM_TOPICS) 
topic_words_path = r"/root/ä¸»é¢˜{}è¯æ±‡.csv".format(NUM_TOPICS)       
doc_topic_words_path = r"/root/æ–‡æ¡£ä¸»é¢˜è¯_{}ä¸»é¢˜.csv".format(NUM_TOPICS) 

# å®šä¹‰å¸¸ç”¨ä¸”ä½åŒºåˆ†åº¦çš„åœç”¨è¯
# ç›®æ ‡æ˜¯å¼ºåˆ¶æ¨¡å‹å…³æ³¨æ¡ˆä»¶çš„ä¸»ä½“åè¯ï¼ˆå¦‚ï¼šä¹°å–ã€é‡‘èã€å•†æ ‡ã€ç»§æ‰¿ï¼‰
CHINESE_STOPWORDS = {
    'çº çº·', 'è´£ä»»', 'æ¡ˆä»¶', 'äº‰è®®', 'çº çº·æ¡ˆä»¶', 'è¿½å¿', 'çº è‘›', 'è¯‰è®¼', 'æ³•å¾‹', 'ä¸', 'åŠ', 'çš„', 'æ˜¯',
    'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'ç­‰', 'å…¶ä»–', 'ä¹‹', 'å› ', 'ä¸­',
    # æ–°å¢åŠ çš„é€šç”¨æ³•å¾‹è¯æ±‡ï¼Œè¿™äº›è¯æ±‡åœ¨å„ç±»æ¡ˆä»¶ä¸­éƒ½å¯èƒ½å‡ºç°ï¼Œé™ä½äº†ä¸»é¢˜åŒºåˆ†åº¦
    'æƒ', 'æŸå®³', 'èµ”å¿', 'ç‰©', 'è¡Œä¸º', 'æƒåˆ©', 'æ°‘äº‹', 'ç¡®è®¤' 
}

print(f"æ­£åœ¨è¯»å–æ•°æ®: {input_path} ...")
# -------------------------------
# 1. è¯»å–æ•°æ®
# -------------------------------
data = pd.read_csv(input_path, dtype=str)
data = data.fillna("")

# -------------------------------
# 2. æ¸…æ´— type å­—æ®µ
# -------------------------------
def clean_type(x):
    x = str(x).strip()
    try:
        val = ast.literal_eval(x)
        if isinstance(val, list):
            return " ".join(val)
    except:
        pass
    # ç§»é™¤æ‰€æœ‰éä¸­æ–‡å­—ç¬¦ï¼Œç¡®ä¿åªä¿ç•™ä¸­æ–‡æ¡ˆç”±åç§°
    x = re.sub(r'[^\u4e00-\u9fa5\s]', '', x) 
    return x.replace("[", "").replace("]", "").replace("'", "")

tqdm.pandas(desc="æ¸…æ´—æ•°æ®", mininterval=0.5)
data["type_clean"] = data["type"].progress_apply(clean_type)

# -------------------------------
# 3. ä¸­æ–‡åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
# -------------------------------
print("æ­£åœ¨è¿›è¡Œåˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤...")
def cut_words_and_filter(text):
    # åˆ†è¯
    words = jieba.lcut(text)
    # è¿‡æ»¤ç©ºè¯å’Œåœç”¨è¯
    filtered_words = [w for w in words if w.strip() and w not in CHINESE_STOPWORDS]
    return filtered_words

tqdm.pandas(desc="åˆ†è¯å¤„ç†", mininterval=0.5)
data["words"] = data["type_clean"].progress_apply(cut_words_and_filter)

# è¿‡æ»¤ç©ºæ–‡æœ¬
original_len = len(data)
data = data[data["words"].map(len) > 0].reset_index(drop=True)
print(f"æœ‰æ•ˆæ–‡æœ¬æ•°é‡: {len(data)} / {original_len}")

texts = data["words"].tolist()

# -------------------------------
# 4. æ„å»ºå­—å…¸å’Œè¯­æ–™
# -------------------------------
print("æ„å»ºå­—å…¸å’Œè¯­æ–™...")
dictionary = corpora.Dictionary(texts)
# è¿‡æ»¤æ‰å‡ºç°æ¬¡æ•°å°‘äº MIN_WORD_COUNT æ¬¡çš„è¯æ±‡
dictionary.filter_extremes(no_below=MIN_WORD_COUNT) 
corpus = [dictionary.doc2bow(t) for t in tqdm(texts, desc="ç”Ÿæˆè¯­æ–™", mininterval=0.5)]

# -------------------------------
# 5. LDAè®­ç»ƒ (ä½¿ç”¨ LdaMulticore åŠ é€Ÿ)
# -------------------------------
num_topics = NUM_TOPICS # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„ä¸»é¢˜å‚æ•°
passes = NUM_PASSES    # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„è¿­ä»£æ¬¡æ•°å‚æ•°
# è‡ªåŠ¨è®¾ç½®æ ¸å¿ƒæ•°ï¼Œä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ - 1
workers = max(1, os.cpu_count() - 1) 

print(f"å¼€å§‹ LDA Multicore è®­ç»ƒ (ä¸»é¢˜æ•°={num_topics}, è½®æ•°={passes}, æ ¸å¿ƒæ•°={workers})...")

# åˆ‡æ¢åˆ° LdaMulticoreï¼Œä½¿ç”¨å¤šæ ¸å¹¶è¡Œè®¡ç®—
lda = models.LdaMulticore(
    corpus=corpus,
    id2word=dictionary,
    num_topics=num_topics,
    passes=passes,
    random_state=42,
    workers=workers, # ä½¿ç”¨å¤šæ ¸å¿ƒå¹¶è¡Œè®¡ç®—
    chunksize=5000   # å¢å¤§å—å¤§å°ä»¥ä¼˜åŒ–æ€§èƒ½
)

# -------------------------------
# 6. è¾“å‡ºä¸»é¢˜é«˜é¢‘è¯
# -------------------------------
topn = 10
topic_words = {}
for t in range(num_topics):
    topic_words[t] = [word for word, _ in lda.show_topic(t, topn=topn)]

df_topic_words = pd.DataFrame({
    "topic": list(topic_words.keys()),
    "words": [" ".join(wlist) for wlist in topic_words.values()]
})
df_topic_words.to_csv(topic_words_path, index=False, encoding="utf-8-sig")
print(f"å·²è¾“å‡ºä¸»é¢˜é«˜é¢‘è¯åˆ°ï¼š{topic_words_path}")

# -------------------------------
# 7. æ–‡æœ¬ â†’ ä¸»é¢˜æ˜ å°„ï¼ˆä¿è¯ç›¸åŒæ–‡æœ¬åŒä¸»é¢˜ï¼Œå¹¶è¾“å‡ºä¸»é¢˜è¯ï¼‰
# -------------------------------
text2topic = {}
topic_list = [] 
doc_topic_words_list = [] # å­˜æ”¾æ¯æ¡æ–‡æ¡£å¯¹åº”ä¸»é¢˜çš„å‰ N ä¸ªè¯

print("\næ­£åœ¨è¿›è¡Œæ–‡æ¡£ä¸»é¢˜é¢„æµ‹...")
# total=len(corpus) ç¡®ä¿ tqdm æ­£ç¡®è®¡ç®—è¿›åº¦
for t_clean, doc in tqdm(zip(data["type_clean"], corpus), total=len(corpus), desc="ä¸»é¢˜é¢„æµ‹", mininterval=0.5):
    if t_clean in text2topic:
        best_topic = text2topic[t_clean]
    else:
        tp = lda.get_document_topics(doc)
        if tp:
            # å–æ¦‚ç‡æœ€å¤§çš„ä¸»é¢˜ID
            best_topic = max(tp, key=lambda x: x[1])[0]
        else:
            best_topic = -1
        # ç¼“å­˜ç»“æœ
        text2topic[t_clean] = best_topic

    topic_list.append(best_topic)
    if best_topic != -1:
        doc_topic_words_list.append(" ".join(topic_words[best_topic]))
    else:
        doc_topic_words_list.append("")

# ä½¿ç”¨åŠ¨æ€åˆ—åï¼Œä¾‹å¦‚ topic50
data[f"topic{NUM_TOPICS}"] = topic_list
data["topic_words"] = doc_topic_words_list

# -------------------------------
# 8. ä¿å­˜ç»“æœ
# -------------------------------
output_columns = [c for c in data.columns if c not in ["words"]]
data[output_columns].to_csv(output_path, index=False, encoding="utf-8-sig")
print(f"å®Œæˆï¼å·²è¾“å‡ºæœ€ç»ˆç»“æœåˆ°ï¼š{output_path}")

# -------------------------------
# 9. ä¿å­˜æ–‡æ¡£å¯¹åº”ä¸»é¢˜è¯ CSV
# -------------------------------
# è°ƒæ•´åˆ—åä»¥ä¿æŒæ¸…æ™°
data_doc_topic = data[["type_clean", f"topic{NUM_TOPICS}", "topic_words"]]
data_doc_topic.columns = ["type_clean", "topic_id", "topic_words"] 
data_doc_topic.to_csv(doc_topic_words_path, index=False, encoding="utf-8-sig")
print(f"æ–‡æ¡£ä¸»é¢˜è¯å·²è¾“å‡ºåˆ°ï¼š{doc_topic_words_path}")
```

## Python ä¾‹å­ ï¼šåŸºäºå§“åé¢„æµ‹æ€§åˆ«

ç”±äºæ³•å®˜æ²¡æœ‰å…¬å¸ƒæ€§åˆ«ï¼Œå¯ä»¥é€šè¿‡å§“åé¢„æµ‹æ€§åˆ«ã€‚Python åŒ…è°ƒç”¨å³å¯ã€‚å‚è€ƒè®ºæ–‡ã€Š[Women in the Courtroom: Technology and Justice](https://academic.oup.com/restud/advance-article/doi/10.1093/restud/rdaf066/8220859)ã€‹ï¼Œä¸‹é¢ä½¿ç”¨çš„æ˜¯ `ngender` åŒ…ã€‚

è¿™ä¸ªåŒ…ä¹Ÿå­˜åœ¨ç¼ºé™·ï¼Œä½¿ç”¨çš„æ˜¯è´å¶æ–¯ä¼°è®¡è®­ç»ƒçš„å‚æ•°ï¼Œå› æ­¤å‡è®¾åå­—å­—ç¬¦æ˜¯ç‹¬ç«‹çš„ã€‚ä¾‹å¦‚ç‹èƒœç”·ï¼Œæ¯ä¸ªå­—å•ç‹¬çœ‹èµ·æ¥åç”·æ€§ï¼Œä½†æ˜¯ä¸ªå…¸å‹å¥³æ€§åå­—ï¼Œä½†æ˜¯åŸºäºè´å¶æ–¯ä¼°è®¡ï¼Œè¿™ä¸ªåŒ…ä¼šå°†å…¶è¯†åˆ«ä¸ºç”·æ€§ã€‚

å¦‚æœæƒ³è¦è¿›ä¸€æ­¥åšä¸¥è°¨ï¼Œä¸ªäººæ¨èäº†è§£ä¸€ä¸ª github ä»“åº“[é¢„æµ‹ä¸­æ–‡å§“åçš„æ€§åˆ«](https://github.com/jaaack-wang/gender-predictor) 

```python
import pandas as pd
import ngender

# Load the CSV file
df = pd.read_csv('F:\æ¡Œé¢\æ€§åˆ«é¢„æµ‹æµ‹è¯•\æµ‹è¯•.csv')  # Replace with your actual CSV file name

# Function to predict gender
def predict_gender(name):
    try:
        result = ngender.guess(str(name))  # Returns (gender, probability)
        return result[0]  # Extract gender ('male', 'female', or 'neutral')
    except:
        return 'unknown'  # Handle invalid names

# Add predicted gender column
df['é¢„æµ‹æ€§åˆ«'] = df['name'].apply(predict_gender)

# Save the updated CSV
df.to_csv('output.csv', index=False, encoding='utf-8-sig')

print("Gender predictions added to 'output.csv'")
```


## æè¿°ç»Ÿè®¡

### ç¯çŠ¶å›¾

```python
import pandas as pd
import matplotlib.pyplot as plt
from aquarel import load_theme

# åŠ è½½å¹¶åº”ç”¨ä¸»é¢˜
theme = load_theme("boxy_light")
theme.apply()

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['STZhongsong']
plt.rcParams['axes.unicode_minus'] = False

# æ•°æ®å¤„ç†
types = df['type'].explode().value_counts()
top_types = types.head(10)

# é¢œè‰²é…ç½®
colors = ['#FF6347', '#4682B4', '#008000', '#FFA500', '#8B0000',
          '#FFFF00', '#9400D3', '#FF1493', '#00FA9A', '#1E90FF']

# åˆ›å»ºç”»å¸ƒ
fig, ax = plt.subplots(figsize=(10, 8))

# ç»˜åˆ¶ç¯çŠ¶å›¾
wedges, texts, autotexts = ax.pie(
    top_types.values,
    autopct='%1.1f%%',
    startangle=90,
    pctdistance=0.85,
    wedgeprops=dict(width=0.3),
    colors=colors
)

# åˆ›å»ºè‡ªå®šä¹‰å›¾ä¾‹
legend_labels = [f"{label} ({value})" for label, value in zip(top_types.index, top_types.values)]
legend = ax.legend(
    wedges,
    legend_labels,
    title="æ¡ˆä»¶ç±»å‹",
    loc="center left",
    bbox_to_anchor=(1, 0, 0.5, 1),
    fontsize=9,
    title_fontsize=10,
    frameon=False
)

# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
total = len(df['type'])
plt.text(1.35, -1.35, 
        f"æ€»æ ·æœ¬æ•°: {total}\nå‰10ç±»å‹è¦†ç›–ç‡: {top_types.sum()/total:.1%}",
        bbox=dict(facecolor='white', alpha=0.9))

# è°ƒæ•´å¸ƒå±€
plt.title('æ•°é‡å‰10æ¡ˆä»¶ç±»å‹åˆ†å¸ƒ', pad=20)
plt.tight_layout()
plt.axis('equal')
plt.show()
```


![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250329235548472.webp)
### æ¡çŠ¶å›¾

ä¼˜åŒ–äº†æ•°å­—æ’åˆ—æ–¹å¼ä¸é‡å 

è§„å®šäº†æ°‘åˆã€å†ã€ç»ˆæ¡ˆå·çš„æ’åˆ—é¡ºåº

ä½¿ç”¨äº† python åŒ… `pip install aquarel` ä¼˜åŒ–äº†ç»˜å›¾é£æ ¼ã€‚

[Aquarel](https://github.com/lgienapp/aquarel)

```python
import pandas as pd
import matplotlib.pyplot as plt
from pylab import mpl

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œè§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
mpl.rcParams['font.sans-serif'] = ['STZhongsong']    # æŒ‡å®šé»˜è®¤å­—ä½“ï¼šè§£å†³plotä¸èƒ½æ˜¾ç¤ºä¸­æ–‡é—®é¢˜
mpl.rcParams['axes.unicode_minus'] = False           # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

# åŠ è½½æ•°æ®
df = pd.read_csv(r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æœ€ç»ˆé¢æ¿æ•°æ®\æ°‘äº‹ä¸Šè¯‰æ ·æœ¬.csv', encoding='utf-8', low_memory=False)

# ä»case_wid_strä¸­æå–æ¡ˆä»¶ç±»å‹
df['case_type'] = df['case_wid_str'].str.extract(r'(æ°‘åˆ|æ°‘ç»ˆ|æ°‘å†)')

# ç»Ÿè®¡æ¯ç§æ¡ˆä»¶ç±»å‹åœ¨ä¸åŒæ³•é™¢çº§åˆ«çš„åˆ†å¸ƒæƒ…å†µ
distribution = df.groupby(['court', 'case_type']).size().unstack(fill_value=0)

# åªä¿ç•™ç‰¹å®šçš„æ³•é™¢çº§åˆ«ï¼Œä½†åªåŒ…æ‹¬å®é™…å­˜åœ¨çš„æ³•é™¢çº§åˆ«
desired_courts = [1, 3, 4, 5, 6]
existing_courts = [court for court in desired_courts if court in distribution.index]
distribution = distribution.loc[existing_courts]

# è®¡ç®—æ¯ä¸ªæ³•é™¢çº§åˆ«çš„æ€»æ¡ˆä»¶æ•°
total_cases_per_court = distribution.sum(axis=1)

# è®¡ç®—æ¯”ä¾‹
distribution_ratio = distribution.div(total_cases_per_court, axis=0) * 100

# è®¾ç½®å›¾è¡¨å¤§å°
plt.figure(figsize=(12, 8))

# å®šä¹‰ä¸€ä¸ªç©ºåˆ—è¡¨ç”¨äºåº•éƒ¨é«˜åº¦
bottoms = [0] * len(distribution_ratio)
# æŒ‰ç…§æ°‘åˆã€æ°‘å†ã€æ°‘ç»ˆçš„é¡ºåºè¿›è¡Œå †å 
case_types_order = ['æ°‘åˆ', 'æ°‘å†', 'æ°‘ç»ˆ']
for case_type in case_types_order:  # ä½¿ç”¨æ­£åºç¡®ä¿æ°‘åˆåœ¨æœ€ä¸‹é¢
    if case_type in distribution.columns:
        plt.bar(range(len(distribution_ratio.index)), distribution_ratio[case_type], bottom=bottoms, label=case_type)
        # åœ¨æ¯ä¸ªæ¡å½¢å›¾ä¸Šæ–¹æ˜¾ç¤ºå…·ä½“æ•°é‡
        for index, value in enumerate(distribution[case_type]):
            text_y_position = bottoms[index] + (distribution_ratio.loc[existing_courts[index], case_type] / 2)
            # è°ƒæ•´æ–‡æœ¬ä½ç½®ä»¥é¿å…é‡å 
            if case_type == 'æ°‘å†':
                if index % 2 == 0:
                    text_y_position += 2  # å‘ä¸Šç§»åŠ¨
                else:
                    text_y_position -= 2  # å‘ä¸‹ç§»åŠ¨
            plt.text(index, text_y_position,
                     str(int(value)), ha='center', va='center', color='black', fontsize=9)
        # æ›´æ–°åº•éƒ¨é«˜åº¦
        bottoms = [sum(x) for x in zip(bottoms, distribution_ratio[case_type])]

# ä¿®æ”¹xè½´æ ‡ç­¾
court_labels = {i: label for i, label in enumerate(["æœ€é«˜", "é«˜çº§", "ä¸­çº§", "åŸºå±‚", "ä¸“é—¨"][:len(existing_courts)])}
plt.xticks(range(len(existing_courts)), list(court_labels.values()))

# æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
plt.title('æ¡ˆä»¶ç±»å‹åœ¨ä¸åŒæ³•é™¢çº§åˆ«çš„æ¯”ä¾‹åˆ†å¸ƒ', fontsize=14, pad=20)
plt.xlabel('æ³•é™¢çº§åˆ«', fontsize=12, labelpad=15)
plt.ylabel('æ¯”ä¾‹ (%)', fontsize=12, labelpad=15)
plt.legend(title="æ¡ˆä»¶ç±»å‹", bbox_to_anchor=(1.05, 1), loc='upper left')

# æ˜¾ç¤ºå›¾è¡¨
plt.tight_layout()
plt.show()
```

![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250329235531039.webp)

### å‡å€¼ä¸­ä½æ•°åˆ†å¸ƒå›¾

åŒ…å«å‡å€¼ã€ä¸­ä½æ•°ã€ç´¯è®¡æ¯”ä¾‹

ä½¿ç”¨äº†å‚æ•°åç§°æ˜ å°„

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pylab import mpl

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œè§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
mpl.rcParams['font.sans-serif'] = ['STZhongsong']    # æŒ‡å®šé»˜è®¤å­—ä½“ï¼šè§£å†³plotä¸èƒ½æ˜¾ç¤ºä¸­æ–‡é—®é¢˜
mpl.rcParams['axes.unicode_minus'] = False           # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

# å‡è®¾è¿™æ˜¯ä½ çš„æ•°æ®åˆ—ä¸å…¶ä¸­æ–‡åç§°çš„æ˜ å°„
var_names_zh = {
    'work_experience_years': 'å·¥ä½œå¹´é™ï¼ˆå¹´ï¼‰',
    'work_experience_months': 'å·¥ä½œç»å†ï¼ˆæœˆï¼‰',
    'average_cases_per_month': 'å¹³å‡å¤„ç†æ¡ˆå­æ•°ï¼ˆæœˆï¼‰'
}

# åŠ è½½æ•°æ®
df = pd.read_csv(r'C:\Users\PC\Desktop\hzp_project_re\re_è®ºæ–‡å¼€å§‹\æœ€ç»ˆé¢æ¿æ•°æ®\æ°‘äº‹ä¸Šè¯‰æ ·æœ¬.csv', encoding='utf-8', low_memory=False)

# è½¬æ¢å·¥ä½œç»å†ä¸ºå¹´ä»½
df['work_experience_years'] = df['work_experience_months'] / 12

# è®¡ç®—å¹³å‡ä¸€ä¸ªæœˆå¤„ç†å¤šå°‘æ¡ˆå­
df['average_cases_per_month'] = df['cumulative_count'] / (df['work_experience_months'] + 1)

def add_statistics(ax, data):
    """åœ¨ç»™å®šçš„è½´ä¸Šæ·»åŠ å¹³å‡æ•°å’Œä¸­ä½æ•°çš„æ ‡æ³¨"""
    mean_val = data.mean()
    median_val = data.median()
    
    ax.axvline(mean_val, color='red', linestyle='--')
    ax.axvline(median_val, color='blue', linestyle='--')
    # æ·»åŠ å›¾ä¾‹äºå³ä¸‹è§’
    ax.legend([f'å‡å€¼: {mean_val:.2f}', f'ä¸­ä½æ•°: {median_val:.2f}'], loc='lower right')

def add_percentage_axis(ax, data):
    """åœ¨ç»™å®šçš„è½´ä¸Šæ·»åŠ å³ä¾§çš„ç™¾åˆ†æ¯”è½´"""
    values, base = np.histogram(data.dropna(), bins=30)
    cumulative = np.cumsum(values)
    percentage = cumulative / cumulative[-1] * 100  # å°†ç´¯è®¡å€¼è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    ax_percent = ax.twinx()
    ax_percent.plot(base[1:], percentage, c='green', label='ç´¯è®¡ç™¾åˆ†æ¯”')
    ax_percent.set_ylabel('ç´¯è®¡ç™¾åˆ†æ¯” (%)')
    ax_percent.legend(loc='upper right')

def trim_data(data, lower=0.5, upper=99.5):
    """è¿”å›ç»™å®šç™¾åˆ†æ¯”èŒƒå›´å†…çš„æ•°æ®"""
    lower_quantile = np.percentile(data, lower)
    upper_quantile = np.percentile(data, upper)
    return data[(data >= lower_quantile) & (data <= upper_quantile)]

variables = ['work_experience_years', 'work_experience_months', 'average_cases_per_month']
fig, axes = plt.subplots(len(variables), 1, figsize=(15, len(variables)*5))

for idx, var in enumerate(variables):
    # ä½¿ç”¨ä¸­æ–‡åç§°
    var_zh = var_names_zh.get(var, var)
    if var == 'average_cases_per_month':
        data_to_plot = trim_data(df[var])
    else:
        data_to_plot = df[var]
    
    # åŸå§‹æ•°æ®åˆ†å¸ƒ
    sns.histplot(data_to_plot, kde=True, color="skyblue", edgecolor="black", ax=axes[idx])
    axes[idx].set_title(f'{var_zh} åˆ†å¸ƒå›¾', fontsize=14)
    add_statistics(axes[idx], data_to_plot)  # æ·»åŠ å¹³å‡æ•°å’Œä¸­ä½æ•°çš„æ ‡æ³¨
    
    # æ·»åŠ ç´¯è®¡ç™¾åˆ†æ¯”è½´
    add_percentage_axis(axes[idx], data_to_plot)

plt.tight_layout()
plt.show()
```



![å¦‚å›¾](/img/è£åˆ¤æ–‡ä¹¦æ¸…æ´—æŒ‡å—.zh-cn-20250329235518600.webp)
