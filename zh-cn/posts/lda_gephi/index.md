# LDA ä¸»é¢˜æ¨¡å‹ä¸ Gephi å¯è§†åŒ–


æœ€è¿‘å¾ˆå¤šç»æµè®ºæ–‡ä½¿ç”¨äº† LDA ä¸»é¢˜å»ºæ¨¡[^1]å’Œç¤¾ä¼šç½‘ç»œåˆ†æï¼Œæ‰€ä»¥åœ¨è¿™é‡Œå°è¯•ç”¨è¿™å¥—æ–¹æ³•å¯è§†åŒ–ä¸‹åšå®¢ä¸»é¢˜è¯ã€‚æµç¨‹å°±æ˜¯ï¼š

- Python è¯»å– markdown æ–‡æ¡£ã€‚
- æ¸…æ´—ã€åˆ†è¯ã€‚
- LDA ä¸»é¢˜æ„å»ºã€‚
- æ„å»ºç½‘ç»œæ•°æ®ã€‚
- åˆ©ç”¨ Gephi å¯è§†åŒ–ã€‚

## LDA ä¸»é¢˜å»ºæ¨¡

LDA å¯¹äºè®¡ç®—æœºæ–‡æœ¬åˆ†ææ¥è¯´æ˜¯ä¸€ä¸ªè€ä¸œè¥¿äº†ï¼Œä½†ç®€å•å¥½ç”¨ï¼Œç®—æ˜¯ç§æ— ç›‘ç£æœºå™¨å­¦ä¹ ã€‚å¯¹äºç»æµå­¦æ¥è¯´ï¼Œåªéœ€è¦çŸ¥é“å…¶ç”¨äº**æ¦‚æ‹¬æ–‡æœ¬æœ‰å¤šå°‘ä¸ªä¸»é¢˜è¯**å°±è¡Œäº†ã€‚åœ¨é¡¶åˆŠæ–‡ç« ä¸­ï¼Œå…¶å¾€å¾€ç”¨äºæµ‹åº¦æ•°æ®ç±»å‹[^7]ã€‚

è®ºæ–‡åº”ç”¨ä¸¾ä¾‹ï¼š

- JPE 2025 å¹´çš„ã€Š[More Laws, More Growth? Evidence from US States](https://www.journals.uchicago.edu/doi/full/10.1086/734874)ã€‹é‡åŒ–äº†ç¾å›½æ¯å·æ–°ç«‹æ³•ä¸»é¢˜çš„åˆ†å¸ƒï¼Œè®¨è®ºç«‹æ³•å’Œç»æµå¢é•¿çš„å…³ç³»ã€‚å…³äºè¥å•†ç¯å¢ƒç±»ä¸»é¢˜çš„ç«‹æ³•æ›´æœ‰æ•ˆï¼Œæœºåˆ¶æ˜¯å¸å¼•æŠ•èµ„ï¼ŒåŒæ—¶é™ä½ç»æµå‘å±•çš„ä¸ç¡®å®šæ€§ã€‚
- RES 2025 å¹´çš„ã€Š[Women in the Courtroom: Technology and Justice](https://academic.oup.com/restud/advance-article/doi/10.1093/restud/rdaf066/8220859)ã€‹ä½¿ç”¨ç›´æ’­åº­å®¡æ¨å¹¿è¯•ç‚¹çš„æ¡ˆä»¶é¢æ¿ç ”ç©¶å¥³æ€§æ¡ˆä»¶èƒœè¯‰æƒ…å†µï¼Œå…¶ä¸­æ¡ˆä»¶ç±»å‹åŸºäºæ¯ä¸ªæ¡ˆä»¶çš„æ³•æ¡å†…å®¹æ€»ç»“ä¸º 50 ä¸ªä¸»é¢˜ç±»å‹ã€‚
- å…¶ä»–æ–‡æœ¬åˆ†æå¯ä»¥å‚è€ƒã€Š[ é¦™æ¨Ÿæ¨æ–‡3057ï¼šæ€ä¹ˆåœ¨ç»æµå­¦ä¸­ç”¨æ–‡æœ¬åˆ†æç®—æ³•ï¼Ÿ](https://mp.weixin.qq.com/s/9SzkCP0kkG3gw-ca-wNqAA)ã€‹

å…³äº LDA çš„ç†è§£ï¼Œæ¨èä¸€ä¸ªç®€æ´å½¢è±¡çš„[æ²¹ç®¡è§†é¢‘](https://www.youtube.com/watch?v=3mHy4OSyRf0)ï¼ˆæŒ‚äº†æ¢¯å­æ‰èƒ½çœ‹è§ï¼‰ï¼š

{{< youtube 3mHy4OSyRf0>}}

## Gephi

### ä¾‹å­

Gephi æ˜¯ä¸€ä¸ªç½‘ç»œå…³ç³»å¯è§†åŒ–è½¯ä»¶ã€‚è¿™ä¸ªè½¯ä»¶æœ€æ—©è¿˜æ˜¯åšä¸» {{< person "https://www.sungyinieh.com/" æ¾æ˜“æ¶… "åšå‹" >}} æ¨èç»™æˆ‘çš„ã€‚ä»–åœ¨æ’°å†™æ¯•ä¸šè®ºæ–‡æ—¶ä½¿ç”¨è¿™ä¸ªè½¯ä»¶ç ”ç©¶äº†[å…¨çƒé”‚äº§å“è´¸æ˜“ä¾èµ–ç½‘ç»œ](https://mp.weixin.qq.com/s/M2Y3_Cre5vl7Wnk6o6J1fg)ã€‚

>å…³äºç½‘ç»œåˆ†æçš„åŸºç¡€çŸ¥è¯†ï¼Œæˆ‘ååˆ†æ¨èå­¦ä¹ å¼€æºçš„é¸¢å°¾èŠ±ç»Ÿè®¡å­¦æ•™æã€Š[æ•°æ®æœ‰é“](https://github.com/Visualize-ML/Book6_First-Course-in-Data-Science)ã€‹ã€‚

ç¤¾ä¼šç½‘ç»œæœ€ç»å…¸çš„ä¾‹å­æ˜¯**å…­åº¦ç†è®º**â€”â€”**ç»ç”±å…­ä¸ªå…³é”®äººå£«å°±å¯ä»¥è®¤è¯†ä¸–ç•Œä»»ä½•ä¸€ä¸ªäºº**ã€‚å…­åº¦ç†è®ºä½“ç°åœ¨ç½‘ç»œåˆ†æä¸­å°±æ˜¯ï¼ˆæœ€çŸ­ï¼‰è·¯å¾„å’Œç½‘ç»œè¿é€šæ€§ã€‚

ç¤¾ä¼šç½‘ç»œåˆ†ææœ€ç»å…¸çš„æ•°æ®é›†ä¸º**ç©ºæ‰‹é“ä¿±ä¹éƒ¨**ã€‚ç©ºæ‰‹é“ä¿±ä¹éƒ¨æœ‰ä¸¤ä¸ªæ•™ç»ƒï¼Œæœ‰ä¸€å¤©ä»–ä»¬åˆ†é“æ‰¬é•³äº†ã€‚æ­¤å‰æœ‰ç»Ÿè®¡å­¦å®¶è®°å½•äº†ä¿±ä¹éƒ¨çš„äººé™…å…³ç³»ï¼Œç„¶åè¿›è¡Œç¤¾ä¼šç½‘ç»œå¯è§†åŒ–ç„¶åèšç±»ï¼Œæœ€ç»ˆæˆå‘˜åˆ†å®¶çš„åˆ†å¸ƒåŸºæœ¬å»åˆã€‚


![ç©ºæ‰‹é“ä¿±ä¹éƒ¨æ•°æ®é›†å¯è§†åŒ–ï¼Œæ¥æº wiki ç™¾ç§‘](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-20250824235924254.webp)

ç¤¾ä¼šç½‘ç»œå¯è§†åŒ–ä¸»è¦æ¶‰åŠç½‘ç»œå…³ç³»çš„æ¼”åŒ–ï¼ˆèšé›†ã€åˆ†è£‚ï¼‰ã€åˆ†å¸ƒ[^6]ï¼ˆå…³é”®èŠ‚ç‚¹ã€æƒé‡ã€å¯†åº¦ã€ç»“æ„[^2]ï¼‰ã€é¢„æµ‹ã€‚

è®ºæ–‡ã€Š[Ownership Networks and Firm Growth: What Do Forty Million Companies Tell Us About the Chinese Economy? ](https://cfrc.pbcsf.tsinghua.edu.cn/__local/C/E3/F1/034E2CB105C230C8A028BDF18AD_5B83ADAD_18F58A.pdf?e=.pdf)ã€‹ç ”ç©¶äº†ä¸­å›½è‚¡æƒç½‘ç»œçš„åˆ†å¸ƒå’Œæ¼”åŒ–ã€‚

![å¦‚å›¾](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-20250824230329625.webp)

è®ºæ–‡ã€Š[Community Interaction and Conflict on the Web](https://arxiv.org/abs/1803.03697)ã€‹ç ”ç©¶äº† raddit çš„ç¤¾ç¾¤ç½‘ç»œï¼š**ä¸åˆ° 1% çš„ç¤¾åŒºå¼•å‘äº† 74% çš„è´Ÿé¢åŠ¨å‘˜è¡Œä¸º**ã€‚è¿™äº›å†²çªä¸€èˆ¬ç”±æ´»è·ƒç¤¾åŒºçš„æ ¸å¿ƒæˆå‘˜å‘èµ·ï¼Œè€ŒçœŸæ­£å‚ä¸å†²çªçš„å´æ˜¯è¾ƒä¸æ´»è·ƒçš„å¤–å›´æˆå‘˜ã€‚



![raddit çš„ç¤¾ç¾¤ç½‘ç»œ](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-20250824231228354.webp)

Citespace è¿™ç§æ–‡çŒ®ç»¼è¿°è½¯ä»¶ä¹Ÿæ˜¯ä¸€ç§ç½‘ç»œåˆ†æï¼Œä¹Ÿå°±æ˜¯çŸ¥è¯†å›¾è°±ã€‚å¯å‚è§å¦å¤–ä¸€ç¯‡åšæ–‡ã€Š[ Citespace æ–‡çŒ®å¯è§†åŒ–](https://blog.huaxiangshan.com/zh-cn/posts/citespace/)ã€‹ã€‚

ä»¥å‰æµ‹è¯•çš„å›¾ç‰‡ã€‚

![äºŒæ¬¡å…ƒä¸»é¢˜ï¼Œå¯ä»¥å‘ç°Bç«™ã€åˆéŸ³æœªæ¥ä¹Ÿæ˜¯ä¸‰çº§å…³é”®è¯ï¼ˆhomoæ˜¯è¿™æ ·çš„ï¼‰](/img/Citespaceæ–‡çŒ®å¯è§†åŒ–.zh-cn-20240523120837358.webp)

### åˆ†æè½¯ä»¶

> è®© gpt æ€»ç»“äº†ä¸‹å¯ä»¥é€‰æ‹©å“ªäº›ç¤¾ä¼šç½‘ç»œåˆ†æè½¯ä»¶ã€‚

| å·¥å…·            | å®šä½                     | ä¼˜ç‚¹                     | ç¼ºç‚¹              | é€‚åˆåœºæ™¯         |
| ------------- | ---------------------- | ---------------------- | --------------- | ------------ |
| **Gephi**     | ç½‘ç»œå¯è§†åŒ– + æ¢ç´¢             | ç•Œé¢å‹å¥½ï¼›ä¸°å¯Œçš„å¸ƒå±€ç®—æ³•ï¼›ç¤¾åŒºæ£€æµ‹ç­‰åŠŸèƒ½   | åé‡å¯è§†åŒ–ï¼›ä¸é€‚åˆè¶…å¤§è§„æ¨¡ç½‘ç»œ | å¿«é€Ÿå¯è§†åŒ–ã€æ¼”ç¤ºã€æ¢ç´¢  |
| **Cytoscape** | ç”Ÿç‰©ç½‘ç»œåˆ†æèµ·å®¶ â†’ æ³›ç”¨          | æ’ä»¶ç”Ÿæ€ä¸°å¯Œï¼›ç§‘ç ”å¸¸ç”¨ï¼›åˆ†æ+å¯è§†åŒ–ç»“åˆç´§å¯† | å¤§è§„æ¨¡æ€§èƒ½æœ‰é™ï¼›ç•Œé¢ç¨å¤æ‚   | ç”Ÿç‰©ä¿¡æ¯å­¦ã€ç§‘ç ”ç½‘ç»œåˆ†æ |
| **Pajek**     | ç»å…¸ç¤¾ä¼šç½‘ç»œåˆ†æè½¯ä»¶             | å¯å¤„ç†ç™¾ä¸‡çº§å¤§ç½‘ç»œï¼›å­¦æœ¯ç•Œå¸¸è§        | ç•Œé¢è€æ—§ï¼›å­¦ä¹ æ›²çº¿é™¡      | å¤§è§„æ¨¡ç¤¾ä¼šç½‘ç»œç ”ç©¶    |
| **NodeXL**    | Excel æ’ä»¶ï¼ˆå¾®è½¯ç³»ï¼‰          | ä¸Šæ‰‹å¿«ï¼›ç¤¾äº¤åª’ä½“æ•°æ®æŠ“å–æ–¹ä¾¿         | åŠŸèƒ½æœ‰é™ï¼›å•†ä¸šç‰ˆæ‰å®Œæ•´     | ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æå…¥é—¨   |
| **Python**    | NetworkX / igraph ç¼–ç¨‹å·¥å…· | çµæ´»ï¼›å¯å®šåˆ¶ï¼›èƒ½ç»“åˆæœºå™¨å­¦ä¹ å’Œæ–‡æœ¬æŒ–æ˜    | æ— å¯è§†åŒ–ç•Œé¢ï¼›éœ€å†™ä»£ç      | é«˜åº¦çµæ´»ã€ç§‘ç ”å®éªŒ    |
## ä»£ç 

ä¸‰ä¸ªæ–‡ä»¶å¤¹ï¼š

1. åšå®¢ markdown æ–‡ä»¶å¤¹ï¼ˆåšå®¢åŸæ–‡`. md`ï¼‰
2. åˆ†è¯æ–‡ä»¶å¤¹ï¼ˆtxt æ–‡æ¡£ä¸€ä¸ªè¯å•ç‹¬ä¸€è¡Œå³å¯ï¼Œä¾‹å¦‚äº†ã€çš„ã€å¾—ã€åœ°ã€ä»–ã€å¥¹ã€æ»‘ç¿”é—ª......ï¼‰
3. ç»“æœæ–‡ä»¶å¤¹

æˆ‘ä½¿ç”¨çš„ `jieba` åˆ†è¯ã€`gensim.models` åº“çš„æ¨¡å‹è®­ç»ƒæ–¹å¼ï¼Œå¦‚æœæƒ³è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯ä»¥ä»è¿™å‡ ä¸ªç¯èŠ‚è¿›è¡Œä¿®æ”¹[^4]ã€‚

> æ³¨æ„ `gensim` å’Œ `numps` çš„ç‰ˆæœ¬å…¼å®¹é—®é¢˜ã€‚

```python
import os
import re
import glob
import itertools
import jieba
import pandas as pd
import networkx as nx
from collections import defaultdict
from gensim import corpora
from gensim.models import LdaModel

# å…¨å±€é…ç½®å‚æ•°
DOCS_GLOB = r'D:\\PyTorch_practice\\åšå®¢åˆ†æ\\æ•°æ®\\åŸå§‹åšå®¢\\*.md'   # æ–‡æ¡£è·¯å¾„é€šé…ç¬¦
STOPWORDS_PATH = r'D:\\PyTorch_practice\\åšå®¢åˆ†æ\\æ•°æ®\\è®¾ç½®\\åˆ†è¯stop.txt'  # å¯é€‰åœç”¨è¯æ–‡ä»¶
OUTPUT_DIR = r'D:\\PyTorch_practice\\åšå®¢åˆ†æ\\æ•°æ®\\ç»“æœè¾“å‡º\\'       # ç»Ÿä¸€è¾“å‡ºæ–‡ä»¶å¤¹
NO_BELOW = 5  # è¯é¢‘ä½äºæ­¤å€¼çš„è¯å°†è¢«è¿‡æ»¤
NO_ABOVE = 0.5  # è¯é¢‘é«˜äºæ­¤æ¯”ä¾‹çš„è¯å°†è¢«è¿‡æ»¤
KEEP_N = 100000  # ä¿ç•™çš„æœ€é«˜é¢‘è¯æ•°é‡
NUM_TOPICS = 10  # ä¸»é¢˜æ•°é‡
TOPN = 20  # æ¯ä¸ªä¸»é¢˜æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡
MAX_EDGES = 200  # è¯å…±ç°å›¾ä¸­ä¿ç•™çš„æœ€å¤§è¾¹æ•°

# é»˜è®¤åœç”¨è¯é›†åˆ
DEFAULT_STOPWORDS = set([
    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'ä¸€ä¸ª', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬',
    'ä»–ä»¬', 'å¥¹ä»¬', 'è¿™', 'é‚£', 'å…¶', 'åˆ', 'è¢«', 'ä¸Š', 'ä¸­', 'å¯¹', 'æ‰€', 'ä¸º', 'äº'
])

def load_stopwords(path):
    """åŠ è½½åœç”¨è¯åˆ—è¡¨"""
    sw = set()
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    w = line.strip()
                    if w:
                        sw.add(w)
        except Exception as e:
            print(f'è¯»å–åœç”¨è¯æ–‡ä»¶å¤±è´¥ {path}: {e}')
    
    # å¦‚æœè‡ªå®šä¹‰åœç”¨è¯ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤åœç”¨è¯
    return sw if sw else DEFAULT_STOPWORDS

def load_documents(glob_pattern):
    """åŠ è½½æ–‡æ¡£é›†åˆ"""
    docs = []
    filenames = []
    
    # è·å–åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨
    file_list = sorted(glob.glob(glob_pattern))
    if not file_list:
        print(f'æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {glob_pattern}')
        return filenames, docs
        
    print(f'æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡ä»¶')
    
    # è¯»å–æ¯ä¸ªæ–‡ä»¶å†…å®¹
    for fp in file_list:
        try:
            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read().strip()
                if text:
                    docs.append(text)
                    filenames.append(os.path.basename(fp))
        except Exception as e:
            print(f'è¯»å–æ–‡ä»¶å¤±è´¥ {fp}: {e}')
    
    return filenames, docs

# æ–‡æœ¬æ¸…æ´—ä¸åˆ†è¯
RE_CLEAN = re.compile(r"[\s\d\u0000-\u007F]+")  # å»æ‰ ascii/æ•°å­—/å¤šä½™ç©ºç™½ï¼Œä¿ç•™ä¸­æ–‡æ±‰å­—å’Œä¸­æ–‡æ ‡ç‚¹

def preprocess(text, stopwords):
    """æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…æ´—ã€åˆ†è¯ã€è¿‡æ»¤"""
    text = RE_CLEAN.sub(' ', text)
    tokens = jieba.lcut(text)
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—è¯
    tokens = [t for t in tokens if t.strip() and t not in stopwords and len(t) > 1]
    return tokens

def build_cooccurrence(tokens_list):
    """æ„å»ºè¯å…±ç°çŸ©é˜µ"""
    cooc = defaultdict(int)
    freq = defaultdict(int)
    
    for tokens in tokens_list:
        # å¯¹æ¯ç¯‡æ–‡æ¡£ä¸­çš„è¯å»é‡
        unique_tokens = list(dict.fromkeys(tokens))
        
        # æ›´æ–°è¯é¢‘
        for w in unique_tokens:
            freq[w] += 1
            
        # æ„å»ºå…±ç°å…³ç³»
        for a, b in itertools.combinations(unique_tokens, 2):
            if a != b:
                # ç¡®ä¿æœ‰åºï¼Œé¿å…é‡å¤è®¡æ•°
                key = (a, b) if a < b else (b, a)
                cooc[key] += 1
                
    return freq, cooc

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print('åŠ è½½åœç”¨è¯...')
    stopwords = load_stopwords(STOPWORDS_PATH)
    print(f'åŠ è½½äº† {len(stopwords)} ä¸ªåœç”¨è¯')
    
    print('åŠ è½½æ–‡æ¡£...')
    filenames, docs = load_documents(DOCS_GLOB)
    print(f'æ‰¾åˆ° {len(docs)} ç¯‡æ–‡æ¡£')
    
    if len(docs) == 0:
        print('æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œæ£€æŸ¥ DOCS_GLOB è®¾ç½®')
        return
    
    print('åˆ†è¯å¹¶é¢„å¤„ç†...ï¼ˆè¿™ä¸€æ­¥å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰')
    texts = [preprocess(d, stopwords) for d in docs]
    
    # ç»Ÿè®¡å¤„ç†åçš„è¯æ±‡ä¿¡æ¯
    total_tokens = sum(len(text) for text in texts)
    print(f'é¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_tokens} ä¸ªè¯æ±‡')
    
    # æ„å»ºå­—å…¸ä¸è¯­æ–™
    print('æ„å»ºè¯å…¸ä¸è¯­æ–™...')
    dictionary = corpora.Dictionary(texts)
    original_size = len(dictionary)
    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)
    filtered_size = len(dictionary)
    print(f'è¯å…¸è¿‡æ»¤: {original_size} -> {filtered_size} ä¸ªè¯')
    
    corpus = [dictionary.doc2bow(text) for text in texts]
    
    # è®­ç»ƒLDAæ¨¡å‹
    print('è®­ç»ƒ LDA æ¨¡å‹...')
    lda = LdaModel(
        corpus=corpus, 
        id2word=dictionary, 
        num_topics=NUM_TOPICS, 
        passes=15, 
        random_state=42,
        per_word_topics=True
    )
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(OUTPUT_DIR, 'lda_model.model')
    lda.save(model_path)
    print(f'LDA æ¨¡å‹å·²ä¿å­˜: {model_path}')
    
    # è¾“å‡ºä¸»é¢˜-è¯åˆ†å¸ƒ
    topics = lda.show_topics(num_topics=NUM_TOPICS, num_words=TOPN, formatted=False)
    print('\nä¸»é¢˜å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªä¸»é¢˜ï¼‰:')
    
    rows_kw = []
    for tid, terms in topics:
        topic_words = ', '.join([f'{w}({p:.4f})' for w, p in terms])
        print(f'Topic {tid}: {topic_words}')
        
        for rank, (word, prob) in enumerate(terms, start=1):
            rows_kw.append({
                'topic': tid, 
                'rank': rank, 
                'word': word, 
                'weight': float(prob)
            })
    
    # ä¿å­˜ä¸»é¢˜å…³é”®è¯
    kw_path = os.path.join(OUTPUT_DIR, 'topic_keywords.csv')
    df_kw = pd.DataFrame(rows_kw)
    df_kw.to_csv(kw_path, index=False, encoding='utf-8-sig')
    print(f'ä¸»é¢˜å…³é”®è¯å·²ä¿å­˜: {kw_path}')
    
    # å¯¼å‡ºæ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
    print('å¯¼å‡ºæ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ...')
    rows = []
    for doc_id, bow in enumerate(corpus):
        doc_topics = lda.get_document_topics(bow, minimum_probability=0.0)
        for tid, weight in doc_topics:
            rows.append({
                'doc': filenames[doc_id], 
                'doc_id': doc_id, 
                'topic': int(tid), 
                'weight': float(weight)
            })
    
    dt_path = os.path.join(OUTPUT_DIR, 'doc_topic.csv')
    df_dt = pd.DataFrame(rows)
    df_dt.to_csv(dt_path, index=False, encoding='utf-8-sig')
    print(f'æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒå·²ä¿å­˜: {dt_path}')
    
    # æ„å»º topic-term äºŒéƒ¨å›¾
    print('æ„å»º topic-term äºŒéƒ¨å›¾...')
    G = nx.Graph()
    
    for tid, terms in topics:
        # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯ä½œä¸ºä¸»é¢˜åç§°
        top_words = [w for w, _ in terms[:3]]
        topic_label = f"Topic_{tid}_" + "_".join(top_words)
        G.add_node(f'topic_{tid}', label=topic_label, type='topic', topic_id=tid)
    
    for tid, terms in topics:
        for term, prob in terms:
            if not G.has_node(term):
                G.add_node(term, label=term, type='term')
            weight = float(prob)
            G.add_edge(f'topic_{tid}', term, weight=weight)
    
    # ä¿å­˜äºŒéƒ¨å›¾
    bipartite_path = os.path.join(OUTPUT_DIR, 'gephi_topic_term.gexf')
    nx.write_gexf(G, bipartite_path)
    print(f'Topic-Term äºŒéƒ¨å›¾å·²ä¿å­˜: {bipartite_path}')
    
    # æ„å»ºè¯å…±ç°å›¾
    print('æ„å»ºè¯å…±ç°å›¾...')
    freq, cooc = build_cooccurrence(texts)
    
    # æŒ‰æƒé‡æ’åºè¾¹ï¼Œåªä¿ç•™æƒé‡æœ€é«˜çš„MAX_EDGESæ¡è¾¹
    sorted_edges = sorted(cooc.items(), key=lambda x: x[1], reverse=True)
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦ä¿ç•™çš„èŠ‚ç‚¹ï¼ˆå‡ºç°åœ¨å‰MAX_EDGESæ¡è¾¹ä¸­çš„èŠ‚ç‚¹ï¼‰
    nodes_to_keep = set()
    edges_to_keep = []
    
    for (a, b), w in sorted_edges:
        if len(edges_to_keep) >= MAX_EDGES:
            break
        nodes_to_keep.add(a)
        nodes_to_keep.add(b)
        edges_to_keep.append(((a, b), w))
    
    # åˆ›å»ºå›¾ï¼Œåªæ·»åŠ éœ€è¦ä¿ç•™çš„èŠ‚ç‚¹å’Œè¾¹
    H = nx.Graph()
    
    # æ·»åŠ èŠ‚ç‚¹ï¼ˆåªä¿ç•™å‡ºç°åœ¨è¾¹ä¸­çš„èŠ‚ç‚¹ï¼‰
    for node in nodes_to_keep:
        if node in freq:
            H.add_node(node, label=node, frequency=int(freq[node]), type='term')
    
    # æ·»åŠ è¾¹
    for (a, b), w in edges_to_keep:
        if a in H and b in H:  # ç¡®ä¿ä¸¤ä¸ªèŠ‚ç‚¹éƒ½å­˜åœ¨
            H.add_edge(a, b, weight=int(w))
    
    # ç§»é™¤å­¤ç«‹èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    H.remove_nodes_from(list(nx.isolates(H)))
    
    print(f'è¯å…±ç°å›¾åŒ…å« {H.number_of_nodes()} ä¸ªèŠ‚ç‚¹å’Œ {H.number_of_edges()} æ¡è¾¹')
    
    # ä¿å­˜å…±ç°å›¾
    cooccurrence_path = os.path.join(OUTPUT_DIR, 'gephi_term_cooccurrence.gexf')
    nx.write_gexf(H, cooccurrence_path)
    print(f'è¯å…±ç°å›¾å·²ä¿å­˜: {cooccurrence_path}')
    
    # è¾“å‡ºæ€»ç»“ä¿¡æ¯
    print('\nå…¨éƒ¨å®Œæˆã€‚ç”Ÿæˆçš„æ–‡ä»¶ï¼š')
    output_files = [
        'lda_model.model', 
        'topic_keywords.csv', 
        'doc_topic.csv', 
        'gephi_topic_term.gexf', 
        'gephi_term_cooccurrence.gexf'
    ]
    
    for fn in output_files:
        file_path = os.path.join(OUTPUT_DIR, fn)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            print(f' - {file_path} ({file_size/1024:.1f} KB)')
        else:
            print(f' - {file_path} (æ–‡ä»¶æœªç”Ÿæˆ)')
    
    print('\nGephi å¯è§†åŒ–å»ºè®®:')
    print('1. æ‰“å¼€ Gephiï¼šFile â†’ Open â†’ é€‰æ‹© .gexf æ–‡ä»¶')
    print('2. ä½¿ç”¨ Layoutï¼ˆä¾‹å¦‚ ForceAtlas2ï¼‰è¿›è¡Œå¸ƒå±€')
    print('3. æ ¹æ® node attribute çš„ type ä¸Šè‰²/ç­›é€‰')
    print('4. æ ¹æ® degree/weight è°ƒæ•´èŠ‚ç‚¹å¤§å°')

if __name__ == '__main__':
    main()
```

ä½¿ç”¨ä»£ç ç”Ÿæˆå›¾æ–‡ä»¶ `gephi_topic_term.gexf` åï¼Œ

1. æ‰“å¼€ Gephiï¼šFile â†’ Open â†’ é€‰æ‹© .gexf æ–‡ä»¶
2. ä½¿ç”¨ Layoutï¼ˆä¾‹å¦‚ ForceAtlas2ï¼‰è¿›è¡Œå¸ƒå±€
3. æ ¹æ® node attribute çš„ type ä¸Šè‰²/ç­›é€‰
4. æ ¹æ® degree/weight è°ƒæ•´èŠ‚ç‚¹å¤§å°

å¦‚æœä½¿ç”¨è¿‡ citespace å°±ä¸éš¾ç†è§£ gephi çš„ä¸€äº›ç¾åŒ–æ“ä½œ[^3]ã€‚ä¸ªäººæ„Ÿè§‰è¿™ç§å¯è§†åŒ–ä¸»è¦æ˜¯ä¸ºäº†æ›´ç›´è§‚å‘ˆç°èšç±»ç»“æ„ï¼Œä½†æ˜¯å®é™…ä¸Šå¤§æ•°æ®å¾ˆéš¾è·‘é‡åŠ›å›¾å¯è§†åŒ–ã€‚**ç¤¾ä¼šç½‘ç»œåˆ†æçš„é‡ç‚¹åœ¨äºå¯¹äºç‰¹æ®Šç»“æ„ã€é‡è¦èŠ‚ç‚¹çš„æ•æ‰å’Œæµ‹åº¦**ã€‚

ä½¿ç”¨ LDA ç”Ÿæˆä¸»é¢˜å’Œè¯è¯­åï¼Œå¯è§†åŒ–ç»“æœå¦‚ä¸‹ï¼š

çœ‹æ¥æˆ‘çš„åšå®¢å›´ç»•**ç»æµå­¦**ï¼Œç„¶åå‘è½¯ä»¶ã€å­å­¦ç§‘æ•£å¼€ã€‚ç”µå½±è§‚åæ„Ÿã€å…¶ä»–è½¯ä»¶ç¡®å®è¢«å­¤ç«‹åˆ°æœ€è¿œï¼Œ

![LDA ç”Ÿæˆä¸»é¢˜|444x221](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-20250824180041018.webp)

ä¸‹é¢è¿™ä¸ªå›¾æ˜¯ä¼˜åŒ–äº†ä»£ç ï¼Œä½¿ç”¨ `jieba` è¿›ä¸€æ­¥åˆ å‡å‰¯è¯å½¢å®¹è¯å¾—åˆ°çš„å›¾ç‰‡ï¼Œä¸ªäººæ„Ÿè§‰è¿™ç§å›¾ç‰‡æ€»ç»“å°±ç›¸å½“åˆ°ä½äº†ã€‚

![å¦‚å›¾](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-1759046125538.webp)
![15ä¸ªä¸»é¢˜](/img/ç¤¾ä¼šç½‘ç»œ.zh-cn-1759046196231.webp)
ä¼˜åŒ–ç‰ˆä»£ç ï¼ˆä¸ªäººç¬”è®°æœ¬è·‘ä¸åŠ¨äº†ä¹°äº†ä¸ªä¾¿å®œäº‘æœåŠ¡å™¨è·‘çš„ï¼‰ã€‚
```python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ– LDA ç®¡é“ï¼šæ”¯æŒ Markdown (.md) æ–‡æ¡£ã€è‡ªåŠ¨åœç”¨è¯ã€è¯æ€§è¿‡æ»¤
- ä½¿ç”¨ LdaMulticore å¤šçº¿ç¨‹è®­ç»ƒ
- æ”¯æŒè±å…è¯å¼ºåˆ¶ä¿ç•™ä¸ºä¸»é¢˜å…³é”®è¯
- å¯ç›´æ¥ç”Ÿæˆ Gephi äºŒéƒ¨å›¾
"""

import os
import glob
import re
import jieba
import jieba.posseg as pseg
import pandas as pd
import networkx as nx
from collections import defaultdict
from gensim import corpora
from gensim.models.ldamulticore import LdaMulticore

# Markdown è§£æ
try:
    import markdown
    from bs4 import BeautifulSoup
except ImportError:
    print("é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åº“ï¼Œè¯·æ‰§è¡Œ 'pip install markdown beautifulsoup4'")
    exit()

# -------------------- é…ç½® --------------------
DOCS_GLOB = "/root/åšå®¢åˆ†æ/æ•°æ®/åŸå§‹åšå®¢/*.md"
STOPWORDS_PATH = "/root/åšå®¢åˆ†æ/æ•°æ®/è®¾ç½®/åˆ†è¯stop.txt"
OUTPUT_DIR = "/root/åšå®¢åˆ†æ/æ•°æ®/ç»“æœè¾“å‡º"

# è¯å…¸å’Œè¯­æ–™å‚æ•°
NO_BELOW = 3            # è¯è¯­è‡³å°‘å‡ºç°çš„æ–‡æ¡£æ•°
NO_ABOVE = 0.8          # è¯è¯­åœ¨æ–‡æ¡£ä¸­å‡ºç°æ¯”ä¾‹ä¸Šé™
KEEP_N = 100000              # è¯å…¸æœ€å¤§è¯æ•°

# LDA æ¨¡å‹å‚æ•°
NUM_TOPICS = 10         # ä¸»é¢˜æ•°
TOPN = 15               # æ¯ä¸ªä¸»é¢˜æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡
PASSES = 20             # LDAè®­ç»ƒè½®æ•°
WORKERS = 28             # å¤šçº¿ç¨‹æ•°ï¼ˆå»ºè®® = CPUæ ¸å¿ƒæ•° - 1ï¼‰
CHUNKSIZE = 5000        # æ¯æ‰¹å¤„ç†æ–‡æ¡£æ•°é‡
BATCH = True            # æ˜¯å¦ä½¿ç”¨å°æ‰¹é‡è®­ç»ƒ

# åˆ†è¯ä¸åœç”¨è¯
USE_POS_FILTER = True
POS_BLACKLIST = set(['x', 'c', 'u', 'd', 'p', 't', 'm', 'q', 'r'])
USE_AUTO_DF_STOPWORDS = True
AUTO_DF_THRESHOLD = 0.8
AUTO_DF_TOPK = 200

# é¢å¤–åœç”¨è¯
ADDITIONAL_STOPWORDS = set([
    'ä»€ä¹ˆ', 'æ¨è', 'è·å–', 'ç‚¹å‡»', 'ç”Ÿæˆ', 'ç»˜åˆ¶',
    'å­˜åœ¨', 'é—®é¢˜', 'ä½¿ç”¨', 'å¯ä»¥', 'æˆ‘ä»¬', 'ç°åœ¨', 'é€šè¿‡', 'æ–‡ç« ', 'å†…å®¹', 'ç ”ç©¶æ³•', 'äººä¸º', 'ä¿ç•™', 'æ¥è®²', 'å«é­‚', 'å­¦å®¶'
])

DEFAULT_STOPWORDS = set([
    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'ä¸€ä¸ª',
    'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'è¿™', 'é‚£', 'å…¶', 'åˆ', 'è¢«', 'ä¸Š', 'ä¸­', 'å¯¹', 'æ‰€', 'ä¸º', 'äº'
])

# è±å…è¯ï¼ˆå¼ºåˆ¶ä¿ç•™ä¸ºä¸»é¢˜å…³é”®è¯ï¼‰
WHITELIST_WORDS = set(['ç»æµ', 'å›½å®¶', 'æ”¿åºœ', 'æ”¿ç­–', 'æ–‡å­¦'])

# æ–‡æœ¬æ¸…ç†æ­£åˆ™
RE_CLEAN = re.compile(r"[\s\d\u0000-\u007F]+")


# -------------------- å‡½æ•° --------------------
def extract_text_from_markdown(md_content):
    html = markdown.markdown(md_content)
    soup = BeautifulSoup(html, 'html.parser')
    for tag in soup(['pre', 'code']):
        tag.decompose()
    return soup.get_text(separator=' ')


def load_stopwords(path):
    sw = set()
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                w = line.strip()
                if w:
                    sw.add(w)
    combined = set(DEFAULT_STOPWORDS) | set(ADDITIONAL_STOPWORDS) | sw
    return combined


def load_documents(glob_pattern):
    docs, filenames = [], []
    file_list = sorted(glob.glob(glob_pattern, recursive=True))
    if not file_list:
        print(f'æœªæ‰¾åˆ°æ–‡ä»¶: {glob_pattern}')
        return filenames, docs
    print(f'æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡ä»¶')
    for fp in file_list:
        try:
            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
                raw_text = f.read().strip()
                if raw_text:
                    clean_text = extract_text_from_markdown(raw_text)
                    docs.append(clean_text)
                    filenames.append(os.path.basename(fp))
        except Exception as e:
            print(f'è¯»å–å¤±è´¥ {fp}: {e}')
    return filenames, docs


def auto_find_generic_terms(tokens_list, threshold=AUTO_DF_THRESHOLD, top_k=AUTO_DF_TOPK):
    n_docs = len(tokens_list)
    if n_docs == 0:
        return set()
    df = defaultdict(int)
    for tokens in tokens_list:
        for t in set(tokens):
            df[t] += 1
    df_ratio = {t: cnt / n_docs for t, cnt in df.items()}
    high_df = {t for t, r in df_ratio.items() if r >= threshold}
    if len(high_df) < top_k:
        sorted_by_df = sorted(df.items(), key=lambda x: x[1], reverse=True)
        high_df |= {t for t, _ in sorted_by_df[:top_k]}
    return high_df


def preprocess(text, stopwords, use_pos_filter=USE_POS_FILTER, pos_blacklist=POS_BLACKLIST):
    if not isinstance(text, str):
        return []
    text = RE_CLEAN.sub(' ', text)
    if use_pos_filter:
        words = []
        for w, flag in pseg.cut(text):
            short_flag = flag.split()[0] if flag else flag
            if (short_flag in pos_blacklist or w in stopwords or len(w) <= 1) and w not in WHITELIST_WORDS:
                continue
            words.append(w)
        return words
    else:
        tokens = [t for t in jieba.lcut(text) if (t not in stopwords or t in WHITELIST_WORDS) and len(t) > 1]
        return tokens


# -------------------- ä¸»æµç¨‹ --------------------
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print('åŠ è½½åœç”¨è¯...')
    stopwords = load_stopwords(STOPWORDS_PATH)
    print(f'åŠ è½½äº† {len(stopwords)} ä¸ªåœç”¨è¯')

    print('åŠ è½½ Markdown æ–‡æ¡£...')
    filenames, docs = load_documents(DOCS_GLOB)
    print(f'æˆåŠŸå¤„ç† {len(docs)} ç¯‡æ–‡æ¡£')
    if len(docs) == 0:
        return

    print('æ–‡æœ¬é¢„å¤„ç†...')
    texts = [preprocess(d, stopwords) for d in docs]

    if USE_AUTO_DF_STOPWORDS:
        print('è‡ªåŠ¨æ£€æµ‹é«˜é¢‘æ³›ç”¨è¯...')
        detected = auto_find_generic_terms(texts)
        detected -= WHITELIST_WORDS  # è±å…è¯ä¸åŠ å…¥åœç”¨è¯
        if detected:
            print(f'æ£€æµ‹åˆ° {len(detected)} ä¸ªæ³›ç”¨è¯ï¼ŒåŠ å…¥åœç”¨è¯é‡æ–°å¤„ç†')
            stopwords |= detected
            texts = [preprocess(d, stopwords) for d in docs]

    total_tokens = sum(len(text) for text in texts)
    print(f'å…± {total_tokens} ä¸ªæœ‰æ•ˆè¯æ±‡')

    print('æ„å»ºè¯å…¸ä¸è¯­æ–™...')
    dictionary = corpora.Dictionary(texts)
    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)
    corpus = [dictionary.doc2bow(text) for text in texts]

    print(f'æ–‡æ¡£æ•°é‡: {len(corpus)}, è¯å…¸å¤§å°: {len(dictionary)}')
    print('è®­ç»ƒ LDA æ¨¡å‹ (å¤šçº¿ç¨‹)...')
    lda = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=NUM_TOPICS,
        passes=PASSES,
        workers=WORKERS,
        chunksize=CHUNKSIZE,
        batch=BATCH,
        random_state=42,
        per_word_topics=True
    )
    model_path = os.path.join(OUTPUT_DIR, 'lda_model.model')
    lda.save(model_path)
    print(f'LDA æ¨¡å‹å·²ä¿å­˜: {model_path}')

    # ä¸»é¢˜å…³é”®è¯
    topics = lda.show_topics(num_topics=NUM_TOPICS, num_words=TOPN, formatted=False)
    rows_kw = []
    print('\nä¸»é¢˜å…³é”®è¯:')
    for tid, terms in topics:
        topic_words = ', '.join([f'{w}({p:.4f})' for w, p in terms])
        print(f'Topic {tid}: {topic_words}')
        for rank, (word, prob) in enumerate(terms, start=1):
            rows_kw.append({'topic': tid, 'rank': rank, 'word': word, 'weight': float(prob)})
    pd.DataFrame(rows_kw).to_csv(os.path.join(OUTPUT_DIR, 'topic_keywords.csv'),
                                 index=False, encoding='utf-8-sig')

    # æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
    rows_dt = []
    for doc_id, bow in enumerate(corpus):
        doc_topics = lda.get_document_topics(bow, minimum_probability=0.0)
        for tid, weight in doc_topics:
            rows_dt.append({'doc': filenames[doc_id], 'doc_id': doc_id, 'topic': int(tid), 'weight': float(weight)})
    pd.DataFrame(rows_dt).to_csv(os.path.join(OUTPUT_DIR, 'doc_topic.csv'),
                                 index=False, encoding='utf-8-sig')
    print('æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒå·²ä¿å­˜')

    # æ„å»º Topic-Term äºŒéƒ¨å›¾
    print('æ„å»º Topic-Term äºŒéƒ¨å›¾...')
    G = nx.Graph()
    for tid, terms in topics:
        top_words = [w for w, _ in terms[:3]]
        topic_label = f"Topic_{tid}_" + "_".join(top_words)
        G.add_node(f'topic_{tid}', label=topic_label, type='topic', topic_id=tid)
        for term, prob in terms:
            if not G.has_node(term):
                G.add_node(term, label=term, type='term')
            G.add_edge(f'topic_{tid}', term, weight=float(prob))
    nx.write_gexf(G, os.path.join(OUTPUT_DIR, 'gephi_topic_term.gexf'))
    print('Topic-Term äºŒéƒ¨å›¾å·²ä¿å­˜')

    # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
    print('\nå…¨éƒ¨å®Œæˆ! ç”Ÿæˆæ–‡ä»¶:')
    for fn in ['lda_model.model', 'topic_keywords.csv', 'doc_topic.csv', 'gephi_topic_term.gexf']:
        path = os.path.join(OUTPUT_DIR, fn)
        if os.path.exists(path):
            print(f' - {path} ({os.path.getsize(path)/1024:.1f} KB)')


if __name__ == '__main__':
    main()

```

## ä¸€äº›æ„Ÿå¹

æ€»æ˜¯åæ§½æœºå™¨å­¦ä¹ æ˜¯é»‘ç®±å­ï¼Œä½†æ˜¯å¯¹äºä½¿ç”¨å„ç§åˆ†æè½¯ä»¶çš„æˆ‘æ¥è¯´ï¼Œå¾ˆå¤šè½¯ä»¶æ–¹æ³•å’Œé»‘ç®±å­åˆæœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿå‡è®¾ stata å¤–éƒ¨å‘½ä»¤é”™è¯¯ï¼Œå¦‚æœä¸çŸ¥æ ¹çŸ¥åº•ï¼Œæˆ–è€…ä»ç†è®ºä¸Šå¤šè§’åº¦æ¯”å¯¹ç»“æœå°±éš¾ä»¥å‘ç°é”™è¯¯ã€‚

ä¾‹å¦‚ [ã€ŒStataã€bdiff+reghdfeçš„æ­£ç¡®ç”¨æ³•ï¼Œå¾ˆå¤šè®ºæ–‡éƒ½è¢«å‘äº†](https://mp.weixin.qq.com/s/IsuB02q6Lek07Oxknvx6Vw)ã€‚ä¸ªäººè®°å¾—ä»¥å‰æœ‰ä¸ª did å¤–éƒ¨å‘½ä»¤ä¹Ÿå‡ºç°è¿‡é”™è¯¯ã€‚

æˆ‘ç›¸ä¿¡ ai ä¼šåŠ å¼ºä»£ç ç±»å­¦ä¹ çš„æ­£åé¦ˆï¼Œä½†ä¸æ­¤åŒæ—¶ ai ä¹Ÿä¼šå¸¦æ¥â€œæˆ‘æ‡‚äº†â€çš„å¹»è§‰ã€‚æ­£å¦‚è´¹æ›¼ä¸€å‘å¼ºè°ƒçš„é‚£æ ·â€”â€”**çŸ¥é“å’Œç†è§£å¹¶ä¸ç›¸åŒ**[^5]ã€‚è·‘é€šä»£ç å’Œç†è§£ä»£ç è¿œè¿œä¸æ˜¯ä¸€ä¸ªå±‚æ¬¡çš„å­¦ä¹ ã€‚Ai å­¦ä¹ çš„çŸ¥è¯†å¹»è§‰æ—¶ä»£ï¼Œä»€ä¹ˆæ‰æ˜¯æ‡‚äº†ï¼Ÿæˆ–è®¸åº”è¯¥è¢«é‡æ–°æ€è€ƒã€‚


[^1]: è™½ç„¶ LDA æ–¹æ³•å·²ç»å¾ˆè€äº†, ä½†æ˜¯ç»æµå­¦å‘æ¥æ“…é•¿å†·é¥­æ–°ç‚’ã€‚
[^2]: æŠŠå®‡å®™çœ‹ä½œä¸€ä¸ªç½‘ç»œç©ºé—´ï¼Œæœ€æ–°ç ”ç©¶å‘ç°åœ°çƒä¹Ÿå¯èƒ½å¤„äºä¸€ä¸ªç©ºæ´ä¹‹ä¸­ã€‚è‡ªä»çœ‹äº†çŸ¥ä¹ä¸Šå…³äºæ˜Ÿé™…èˆªè¡Œçš„ä¸€ç¯‡å›ç­”[ï¼ˆäººç±»çœŸçš„æ°¸è¿œæ— æ³•ç¦»å¼€æ‹‰å°¼äºšå‡¯äºšè¶…æ˜Ÿç³»å›¢å—ï¼‰](
https://www.zhihu.com/question/329908379/answer/1943305925965374152 )ï¼Œæˆ‘å·²ç»ä¸ç›¸ä¿¡åœ°çƒä¸Šæœ‰å¤–æ˜Ÿäººäº†ğŸ˜¢ã€‚
[^3]: ä½†æ˜¯å¤§æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåˆ†æ•£æ€§åˆ†å¸ƒç¾åŒ–è®¡ç®—å¤ªè¿‡å¤æ‚ï¼Œå› æ­¤ä½¿ç”¨å¾ˆå°‘ã€‚
[^4]: æœ‰äº† aiï¼Œæ•°æ®åˆ†æéƒ½ä¸éš¾ï¼Œéš¾çš„æ˜¯æ‹¿åˆ°ç¬¦åˆè¦æ±‚çš„ç´ æã€‚
[^5]: æ‰€ä»¥è´¹æ›¼åæ§½ç¤¾ä¼šç§‘å­¦å¯èƒ½æ˜¯ä¼ªç§‘å­¦ã€‚
[^6]: å…‰æ˜¯æè¿°æ€§ç»Ÿè®¡å°±å·²ç»è¶³å¤Ÿèµšäººçœ¼çƒã€‚
[^7]: è¿‘å¹´å¼‚è´¨æ€§ä¸»ä½“çš„æŒ–æ˜è¶Šæ¥è¶Šé‡è¦


